# Multiple predictors and interactions in linear regression

## Objectives

- Interpret regression models with a categorical predictor that has more than two levels.
- Learn to interpret regression models with more than one predictor.
- Introduce statistical interactions in regression models.
- Cover strategies for visualizing multivariate regression models.



```{r load_packages_05, message = F, results = 'hide'}

library(pacman)

p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot,
       modelr, forcats, knitr, kableExtra)

theme_set(theme_bw())

```

We're going to be using the PanTHERIA dataset like we did in the [previous lesson](#reg-continue). Just like last time, here is a quick clean up of the dataset before we do anything "principled."

```{r pantheria_05}

pantheria <- read_tsv(here('data', 'PanTHERIA_1-0_WR05_Aug2008.txt'), 
                      na = '-999.00') %>%
  clean_names() %>% 
  mutate(mass_log = log(x5_1_adult_body_mass_g),
         range_group_log = log(x22_1_home_range_km2),
         range_indiv_log = log(x22_2_home_range_indiv_km2),
         density_log = log(x21_1_population_density_n_km2),
         activity_cycle = case_when(x1_1_activity_cycle == 1 ~ 'nocturnal',
                                    x1_1_activity_cycle == 2 ~ 'mixed',
                                    x1_1_activity_cycle == 3 ~ 'diurnal'),
         trophic_level = case_when(x6_2_trophic_level == 1 ~ 'herbivore',
                                   x6_2_trophic_level == 2 ~ 'omnivore',
                                   x6_2_trophic_level == 3 ~ 'carnivore'))

```




## More than one predictor


So far all of the linear regression models we have encountered have either been intercept-only or with a single predictor. For this lesson we're going to continue our focus on three variables from the PanTHERIA dataset: population density, body size, and trophic level. Population density, measured as the number of individuals per square-kilometer, is our variable of interest -- we want to define a model which describes how population density varies between mammal species. We have previously investigated body size as a predictor of population density, but the posterior predictive analysis of our model of population density with only body mass as a predictor demonstrates that this model does not adequately describe the data. We were able to do this by asking if our model is able to describe differences in the data that we know about but that our model does not (e.g. trophic level).

### Categorical predictor

Trophic level is a categorical predictor with three levels. The previous categorical predictor we dealt with (Bivalve versus Brachiopod) was binary which made it very easy to interpret. In that lesson I briefly introduced the idea of dummy coding and demonstrated how that would work with a three level categorical variable. I'm going to reiterate and expand on that demonstration here.

`brm`, and R in general, will automatically translate categorical predictors like our `trophic_level` variable into what's called *dummy coding*. 

When we dummy code a categorical variable what we are doing is turning one variable with $k$ states into $k - 1$ binary variables. One state of the categorical variable is considered the "baseline" or the default condition for any observation. The other $k - 1$ binary variables then describe if the observation is different from the default -- these are called contrasts. The standard behavior for this in R is that the first state, alphabetically, is made the baseline.  Here is this in action:
```{r dummy_trophic}

pantheria %>%
  drop_na(trophic_level) %>%
  model_matrix(~ trophic_level) %>%
  slice(1:10) %>%
  kable() %>%
  kableExtra::kable_styling()

```

The `model_matrix()` function returns a tibble with $k$ columns. Take a closer look at the column names. First, focus on the second and third columns. Each title is the variable name (`trophic_level`) combined with one of the levels (e.g. `trophic_levelherbivore`). One of the factor's levels, carnivore, is not named as one of the columns, instead that level is subsumed in the `(Intercept)` column. 

The default state of any observation is this "carnivore." A 0 in the `trophic_levelherbivore` or `trophic_levelomnivore` columns means that the observation is not an herbivore or omnivore, respectively. If there is a 1 in the herbivore or omnivore column that means that observation is an herbivore or omnivore and not a carnivore -- hence why the $k - 1$ binary variables are called contrasts (with respect to the default condition/intercept). Importantly, an observation cannot/should not have a 1 in more than one of the $k - 1$ binary variables associated with a single categorical predictor.

Let's see what happens when we suppress the intercept using R's [formula syntax](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html).

```{r dummy_nointercept}

pantheria %>%
  drop_na(trophic_level) %>%
  model_matrix(~ -1 + trophic_level) %>%
  slice(1:10) %>%
  kable() %>%
  kableExtra::kable_styling()

```

By suppressing the intercept in our model matrix, we've dramatically changed the first column of our tibble. Instead of being labeled `(Intercept)` and being a column of only 1s, we have a column named `trophic_levelcarnivora` which is a binary variable indicating if an observation is a carnivore or not. 

I do not recommend suppressing the intercept when developing a model, especially if you are generating your contrasts independent of the model formula (e.g. when using `brm()`), and extra especially if you have more predictors in your model than just the categorical variable being coded out. If you are not careful, you might end up with the equivalent of two intercepts which causes a model to be completely undefined and useless.

As as been discussed before, R tacitly converts our categorical variables (e.g. variables where our observations are character data) into dummy coding. While this is convenient, it can lead to confusion if you aren't super familiar with categorical variables. For example, I once had a colleague ask, "How are there 5 parameters when we only have 1 predictor in our linear regression (they expected three)?" 

Something I like to do is make the most common or most general class be the intercept. This adds a bit of logic to the contrasts, as they are now "in contrast" to the most common state. The most common class in `trophic_level` is "herbivore". To change which class becomes the intercept, we need to manipulate the R structure of `trophic_level`. I'm choosing to do this using functions from the `forcats` package.

```{r dummy_recode}

pantheria %>%
  drop_na(trophic_level) %>%
  mutate(trophic_level = fct_infreq(trophic_level)) %>% # reorder by frequency
  model_matrix(~ trophic_level) %>%
  slice(1:10) %>%
  kable() %>%
  kableExtra::kable_styling()

```

By controlling which of the classes from our categorical variable is considered the intercept, we can make inference easier. Carnivores are relatively rare in our dataset, so having them as the "default" condition for our model is a bit confusing -- most of our observations would be "exceptions" or "in contrast to" our default. By having herbivores act as are default, we inherently are describing more parts of our data before looking at the contrasts.

There are [many other strategies for encoding categorical variables](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/), but dummy coding is by far the most common -- it helps that it is the R default. 



### Defining out model




FOR LATER


$$
\begin{align}
y_{i} &\sim \text{Normal}(\mu_{i}, \sigma) \\
\mu_{i} &= \alpha + \beta_{1} m_{i} + \beta_{2} o_{i} + \beta_{3} h_{i} \\
\alpha &\sim \text{Normal}(0, 10) \\
\beta &\sim \text{Normal}(0, 5) \\
\sigma &\sim \text{Cauchy}^{+}(0, 5) \\
\end{align}
$$





### Fitting model in `brms`

First, like we discussed and demonstrated in last lesson, let's center our continuous predictor `mass_log`. Then we can fit our model with a more interpretable intercept.

```{r fit_multi, cache = TRUE, message = FALSE, results = 'hide'}

pantheria <- 
  pantheria %>%
  drop_na(density_log, mass_log, trophic_level) %>%
  mutate(mass_log_center = mass_log - mean(mass_log))

m_1 <- 
  pantheria %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log_center + trophic_level),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(normal(-1, 5), class = b, coef = mass_log_center),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r multi_print}

print(m_1)

```


### Aside: Standardizing 


```{r stan_pantheria}

pantheria <- 
  pantheria %>%
  drop_na(density_log, mass_log, trophic_level) %>%
  mutate(mass_log_stan = scale(mass_log, center = TRUE, scale = TRUE))

```


```{r fit_multistan, cache = TRUE, message = FALSE, results = 'hide'}

m_2 <- 
  pantheria %>%
  brm(data = .,
      family = gaussian(),
      formula = bf(density_log ~ 1 + mass_log_stan+ trophic_level),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 5), class = b),
                prior(normal(-1, 5), class = b, coef = mass_log_stan),
                prior(cauchy(0, 5), class = sigma)),
      iter = 2000,
      warmup = 1000,
      chains = 4,
      cores = 4,
      refresh = 0)

```

```{r multistan_print}

print(m_2)

```

I recommend always rescaling your (continuous) predictors to have the same scale.


### Checking model fit

```{r multi_fitted}

pantheria %>%
  add_fitted_draws(model = m_2,
                   n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log_stan, y = density_log)) +
  geom_line(mapping = aes(y = .value, group = .draw),
            alpha = 1 / 20,
            colour = 'blue') +
  geom_point(data = pantheria, size = 2) +
  scale_fill_brewer() +
  facet_grid(~ trophic_level) +
  NULL

```

```{r multi_predicted}

pantheria %>%
  data_grid(mass_log_stan = seq_range(mass_log_stan, n = 1000),
            trophic_level) %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = mass_log_stan, y = density_log)) +
  stat_lineribbon(mapping = aes(y = .prediction),
                  .width = c(0.9, 0.5, 0.1),
                  colour = 'blue') +
  geom_point(data = pantheria, size = 2) +
  scale_fill_brewer() +
  facet_grid(~ trophic_level) +
  NULL

```

```{r multi_dens_ppc}

pantheria %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  ggplot(aes(x = .prediction, group = .draw)) +
  geom_line(stat = 'density',
            alpha = 0.1,
            colour = 'blue') +
  geom_line(stat = 'density',
            data = pantheria,
            mapping = aes(x = density_log,
                          group = NULL),
            colour = 'black',
            size = 1.5) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = expression(paste('Population density ', log(n / km^2))),
       title = 'Population density, actual versus predicted') +
  NULL


```

```{r median_ppc}

pan_trophic_summary <-
  pantheria %>%
  group_by(trophic_level) %>%
  dplyr::summarize(median = median(density_log))

pantheria_summary_ppc <- 
  pantheria %>%
  add_predicted_draws(model = m_2,
                      n = 100) %>%
  ungroup() %>%
  group_by(.draw, trophic_level) %>%
  dplyr::summarize(density_median_ppc = median(.prediction))

pantheria_summary_ppc %>%
  ggplot(aes(x = density_median_ppc)) +
  geom_histogram(fill = 'blue') +
  geom_vline(data = pan_trophic_summary,
             mapping = aes(xintercept = median),
             size = 2) +
  facet_grid(~ trophic_level) +
  labs(x = 'Median population density', y = '') +
  NULL

```




## Interactions
