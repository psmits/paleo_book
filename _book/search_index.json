[
["index.html", "Analytical Paleobiology Preface", " Analytical Paleobiology Peter D Smits 2019-02-26 Preface This This book uses the tidyverse collection of R packages with a particular emphasis on dplyr, ggplot2, and purrr. Other tidyverse packages are used as necessary (e.g. modelr). I emphasize Bayesian data analysis approaches throughout this text. To this end, model inference is done using the brms package which is a flexible tool for implementing Stan-based models in R. Manipulation of those results, and some aspects of visualization, is done using the tidybayes package. I attempt to stick to the tidyverse style guide The pacman package is used throughout to ensure that all packages are both installed and loaded into namespace. "],
["managing-and-processing-paleobiology-database-data.html", "1 Managing and Processing Paleobiology Database data 1.1 Objectives 1.2 Reading 1.3 Introduction 1.4 Getting 1.5 Processing 1.6 Sharing 1.7 Summary", " 1 Managing and Processing Paleobiology Database data 1.1 Objectives Introduce the data stored in the Paleobiology Database. Learn how to programatically download PBDB data. Introduce tidy data and some good practices when managing data. Learn how to make PBDB cleaner and tidier library(pacman) p_load(tidyverse, janitor, knitr, kableExtra) theme_set(theme_bw()) 1.2 Reading The following material are recommended pre-readings before starting this tutorial. You do not have to read all of them, just pick at least one. Wickham 2014 “Tidy Data”. Wilson et al. 2017 “Good enough practices in scientific computing” PLoS Computational Biology. Verde Arregotia et al. 2018 “Good practices for sharing analysis-ready data in mammalogy and biodiversity research” Hystrix, the Italian Journal of Mammalogy. Bryan “Project oriented workflow” tidyverse.org. Bryan “Zen and aRt of Workflow Maintenance” talk. Bryan “Code Smells and Feels” talk. Bryan 2017 “Excuse me, do you have a moment to talk about version control?” PeerJ. 1.3 Introduction Any project you work on as a scientist has multiple parts: data, documentation, reports, code, etc. Managing and keeping track of these parts is not a simple task. Today we will discuss a small part of this process: data wrangling and sharing using the tidyverse set of packages and syntax. This lesson is in three parts: getting data, processing data, and sharing data. This tutorial assumes a fair amount of familiarity with the tidyverse, in particular dplyr. For a tutorial on using dplyr and purrr I recommend - R for Data Science - https://dplyr.tidyverse.org/articles/dplyr.html. 1.4 Getting One of the greatest resources in paleobiology is the aptly named Paleobiology Database, or PBDB for short. The PBDB is a freely available internet repository of fossil occurrences, collections, taxonomic opinions, and lots of other information. The standard way to access information in the PBDB is through the class Download Generator. In the past it was very difficult to replicate previous PBDB downloads because of how difficult they were to communicate – with so many manual options, it is hard to easily transmit this information to another author. The modern Download Generator (at time of this writing) has one major improvement for increasing the reproducibility of downloads – a URL. Every option updates a URL that calls our data from the PBDB. Play around with the download options and see how the URL changes. That URL is a call to the PBDB’s API, which is the data service for interfacing with the material stored in the underlying database. This means we can share the URL along with our study so that other researchers can make the same data call. The API documentation leaves something to be desired, but as you interact with the docs and start writing your own API calls, it should become easier. A fossil occurrence is the core data type of the PBDB and probably the most important data type in all of paleobiology – the unique recording of an organism at a particular location in time and space. Normally we want a list of fossil occurrences that correspond to our study system or period of time. For data output from the PBDB for occurrences, each row is an observation and each column is a property of that fossil or metadata corresponding to its collection and identification. We are going to focus on downloading information about fossil occurrences. Here are a few example URLs which make calls to the PBDB API. Use the API documentation to discern and describe the differences between the different calls. https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;show=all https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;taxon_status=valid&amp;show=all https://paleobiodb.org/data1.2/occs/list.txt?base_name=Cetacea&amp;interval=Miocene&amp;idreso=genus&amp;show=all https://paleobiodb.org/data1.2/occs/taxa.txt?base_name=Cetacea&amp;interval=Miocene&amp;show=attr The best part of using a URL based call is that we can embed them in our R scripts. Here is a simple example (note I’m suppressing warnings here, so don’t get scared when you see a lot of them): url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) The carnivora object represents the basic PBDB list response with all information for every observation as a data.frame. Because the URL points directly to a CSV (or JSON) file, we never have to actually save a copy of our data to our local machine and it instead just lives in memory – though you might want to download and store the data every so often (e.g. write_csv()). Also, by using a direct API call to the PBDB instead of relying on a downloaded file our analyses can instaly be updated when new data is added to the PBDB. I find tibbles easier to process than data.frame-s, so my scripts tend to look like this: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() If you play around with the carnivora object you’ll notice it has TONS of columns – 118! Each of these columns records some bit of information about that fossil – taxonomic identity, location, source, enterer, etc. You can check the API documentation for a description of each column. Frustrating many of these fields might be empty or inconsistently entered – I’m looking at you lithology1 and environment. Additionally, a lot of our fossils might not be identified to the species or even genus level, or are not identified with confidence. This serves as an important point about the PBDB: the data isn’t perfect. This means that the next step of our analysis is “cleaning” or “tidying” our data until we can actually analyze it! 1.5 Processing This section focuses on using the dplyr package from the tidyverse to process our data until it is actually usable for our purposes. Luckily, our data is already considered “tidy” because each row is an observation and each column is a variable. This does not mean our data is ready for analysis yet. Our job as analysts is to then process and filter our data till it is ready to be analyzed. Is every observation up to snuff? Or are there errors encoded in our data? Example filters we might consider - identified precisely – no ambiguous or imperfect “calls” - identified to genus or better - paleocoordinates (latitude AND longitude) - body fossils - collected after a certain date We might also want to standardize the form of our column names. Capital letters, spaces, and punctuation are all really frustrating to code around. Luckily, these operations are very easy to do with with dplyr and the janitor package. For example, let’s filter out the imprecise fossils and those not identified to at least the genus level, and have paleocoordinates. Let’s also make sure all the variable names have the same logic (they are all already fine, but this is a good habit to get into!) carnivora_filter &lt;- carnivora %&gt;% janitor::clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) This new tibble, carnivora_filter, is a subset of the ordinal data that should follow the rules we’ve laid out in the URL-based API call and the few lines of R code. If we gave this document to someone else, they could reproduce our dataset. The accepted_* variables in PBDB data correspond to the accepted, or best, identification of a fossil. Differences between identified_* and accepted_* variables are commonly due to re-identification or changes in taxonomy. While this is really convenient on its face, sometimes the accepted species names assumes too much confidence in the identification. For example, let’s take a close look at a few records. carnivora_table &lt;- carnivora_filter %&gt;% select(identified_name, identified_rank, accepted_name, accepted_rank) %&gt;% slice(50:60) knitr::kable(carnivora_table) %&gt;% kableExtra::kable_styling() identified_name identified_rank accepted_name accepted_rank Phlaocyon minor species Phlaocyon minor species Desmocyon thomsoni species Desmocyon thomsoni species Promartes lepidus species Promartes lepidus species Promartes lepidus species Promartes lepidus species Daphoenodon notionastes species Daphoenodon notionastes species Daphoenodon n. sp. notionastes species Daphoenodon notionastes species Phlaocyon achoros species Phlaocyon achoros species Cynarctoides lemur species Cynarctoides lemur species Cormocyon cf. copei species Cormocyon copei species Megalictis ? sp. genus Megalictis genus Phlaocyon ? cf. annectens species Phlaocyon annectens species In most cases there is very good correspondence between the identified name and the accepted name, but not always. For example, the ninth of this table corresponds to a fossil identified as “Cormocyon cv. copei” but is given the accepted name of “Cormocyon copei” – an identification that is arguably overconfident! But does it matter? That’s up to you and your research, but let’s assume it does for now. How do we resolve this and downgrade these overconfident identifications? The simplest way might be to downgrade any identified names that include punctuation and non-character symbols to just there genus. After all, “cf.”, “sp.” and “n. sp.” all involve punctuation. But how do we deal with text information? Turns out there is a whole special language for dealing with text: regular expressions. RegEx are sequences of characters that help us match specific patterns in text. In this example, I’m using a RegEx to identify all cases where there is punctuation present in the identified name – I don’t not care where the punctuation is, just that there is punctuation. To do this, I’m going to be using functions from the stringr package which provide for easier interaction with text and regular expressions than the functions in base R. I always spend a lot of time on Google figuring our RegEx before I use them as they are not intuitive, so don’t worry too much about understanding regular expressions early on – I’m using some special, easy to understand, forms that will be generally useful to you. carnivora_clean &lt;- carnivora_filter %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) carnivora_clean %&gt;% select(identified_name, accepted_name, improve_name) %&gt;% slice(50:60) %&gt;% knitr::kable() %&gt;% kable_styling() identified_name accepted_name improve_name Phlaocyon minor Phlaocyon minor Phlaocyon minor Desmocyon thomsoni Desmocyon thomsoni Desmocyon thomsoni Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Daphoenodon notionastes Daphoenodon notionastes Daphoenodon notionastes Daphoenodon n. sp. notionastes Daphoenodon notionastes Daphoenodon Phlaocyon achoros Phlaocyon achoros Phlaocyon achoros Cynarctoides lemur Cynarctoides lemur Cynarctoides lemur Cormocyon cf. copei Cormocyon copei Cormocyon Megalictis ? sp. Megalictis Megalictis Phlaocyon ? cf. annectens Phlaocyon annectens Phlaocyon If we really wanted to be slick, we could combine all of the above into a single block. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) 1.5.1 Binning Fossil occurrences in the PBDB have temporal information such as geologic stage, or a rough numerical age (max_ma, min_ma). The numerical age of a fossil occurrence is bounded between a max and a min. The nature of the fossil record means that we do not have exact ages for anything, instead we have ranges. This uncertainty in age presents a lot of problems that we have to deal with in our analyses, especially if the temporal order of our fossils matters to our question! An extremely common way to overcome this uncertainty is to coarsen the resolution of our fossils by binning them – assigning similarly aged fossils to a shared temporal unit. Each temporal bin can be said to have a “width” – the length of time covered by that bin. In our example, we may want to track diversity over time. We are going to do this by counting the number of unique genera present in our time bins. To do this, we have to determine how many bins there are and to which bin each fossil belongs. The age of each fossil, however, is a range and not a single value. We could use the midpoint of this range to assign each fossil to a bin, but what if the age range of some fossils is much larger than our bin width? First, let’s take a look at the amount of uncertainty there is in the age estimates of our fossil occurrences. carnivora_clean %&gt;% mutate(age_range = abs(max_ma - min_ma)) %&gt;% ggplot(aes(x = age_range)) + geom_histogram() + labs(x = &#39;Age range (My)&#39;, y = &#39;Count&#39;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can see that a lot of our fossils have age uncertainty of 5 million years or less, and a few of them have large ranges of 10 million years or more. Fossils with age ranges greater than 10 million years are potentially suspect or at least are not high quality – certainly trying to assign them to a single 2 million year bin isn’t going to be ideal as it adds confidence where there is none. In your own analyses you might consider these situations on a case-by-case basis or try and find more information from other sources, but for purposes of this tutorial we will exclude those occurrences who’s age ranges are 10 million years or greater. We can dither over an “optimal” bin width for our data, but for the purpose of this exercise let’s just assign all our fossil occurrences to 2 million year bins. Because binning data is so common, I’ve written a function, bin_ages() to do this. I present it here and will use it to bin our data. The bin_ages() function follows that convention that the youngest bin is bin 1. I’ve written documentation for this function using roxygen formatting. Feel free to modify it or write your own. #&#39; Break time data up into bins #&#39; #&#39; Have fun with this. basic rules. greater than equal to base, less than top. #&#39; #&#39; @param x vector of ages #&#39; @param by bin width #&#39; @param age logical bin age returned, not number (default FALSE, return bin number) #&#39; @return vector of bin memberships #&#39; @author Peter D Smits &lt;peterdavidsmits@gmail.com&gt; bin_ages &lt;- function(x, by = NULL, number = NULL, age = FALSE) { if(is.null(by) &amp; is.null(number)) { return(&#39;no scheme given. specify either bin width or number of bins.&#39;) } if(!is.null(by) &amp; !is.null(number)) { return(&#39;too much information. specify either bin width OR number of bins, not both.&#39;) } # range to bin top &lt;- ceiling(max(x)) bot &lt;- floor(min(x)) # create bins if(!is.null(by)) { unt &lt;- seq(from = bot, to = top, by = by) } else if(!is.null(number)) { unt &lt;- seq(from = bot, to = top, length.out = number + 1) } # bin top and bottom unt1 &lt;- unt[-length(unt)] unt2 &lt;- unt[-1] # assign memberships uu &lt;- map2(unt1, unt2, ~ which(between(x, left = .x, right = .y))) # what if we want the &quot;age&quot; of the bin, not just number? if(age == TRUE) { unt_age &lt;- map2_dbl(unt1, unt2, ~ median(c(.x, .y))) } # create output vector y &lt;- x for(ii in seq(length(uu))) { if(age == FALSE) { y[uu[[ii]]] &lt;- ii } else if(age == TRUE) { y[uu[[ii]]] &lt;- unt_age[ii] } } y } Let’s use this function to bin our data. carnivora_bin &lt;- carnivora_clean %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2)) carnivora_bin %&gt;% summarize(bin_number = n_distinct(bin)) ## # A tibble: 1 x 1 ## bin_number ## &lt;int&gt; ## 1 9 Ok, so now we have a column bin that identifies the temporal bin that each occurrence belongs to. The quick summary at the bottom demonstrates that we have broken our data into 9 bins of equal length. The limit here is that our bins are identified by their number and not their “age”. Luckily, the age parameter of the bin_ages() function that changes the output from bin number to bin age. Here it is in use. carnivora_bin &lt;- carnivora_bin %&gt;% mutate(bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) # take a look carnivora_bin %&gt;% select(mid_ma, bin, bin_age) %&gt;% slice(1:10) %&gt;% knitr::kable(.) %&gt;% kableExtra::kable_styling() mid_ma bin bin_age 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.6040 2 8 14.8950 5 14 13.7890 5 14 13.7890 5 14 As before, we can combine all of these operations into a set of piped statements. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) ## Warning: Duplicated column names deduplicated: &#39;cc&#39; =&gt; &#39;cc_1&#39; [47] ## Parsed with column specification: ## cols( ## .default = col_character(), ## occurrence_no = col_double(), ## reid_no = col_double(), ## flags = col_logical(), ## collection_no = col_double(), ## identified_no = col_double(), ## accepted_attr = col_logical(), ## accepted_no = col_double(), ## max_ma = col_double(), ## min_ma = col_double(), ## ref_pubyr = col_double(), ## reference_no = col_double(), ## plant_organ = col_logical(), ## plant_organ2 = col_logical(), ## abund_value = col_double(), ## lng = col_double(), ## lat = col_double(), ## collection_subset = col_logical(), ## paleolng = col_double(), ## paleolat = col_double(), ## zone = col_logical() ## # ... with 25 more columns ## ) ## See spec(...) for full column specifications. ## Warning: 1048 parsing failures. ## row col expected actual file ## 1071 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1071 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1073 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## .... ............... .................. ...... ............................................................................................. ## See problems(...) for more details. 1.6 Sharing So far we have basically two data “files”: the raw data output from the PBDB via our URL-based API call, and the cleaned data we’ve crafted with a bit of code. Both of these datasets are extremely important and should be shared with the audience. There are two ways to share the raw data associated with our study: the URL-based API call, and a spreadsheet of the downloaded information. Because the information on the PBDB updates over time, an API call made today might not yield an identical dataset. Earlier I hailed this as fantastic, which it is, but it is also limiting – someone might not be able completely reproduce your analysis using just this information. The API call is useful for improving and expanding on previous analyses, but by itself is not enough to reproduce your analysis. You also need to share the raw data download so that your complete analysis, including filtering and cleaning, is reproducible. You probably also want to save a local copy of your filtered and cleaned dataset so you don’t have to re-run your cleaning scripts all the time. It also means you can separate your cleaning from the rest of your analysis. You’ll end up with multiple R scripts and multiple datasets – that’s good. For example, my projects tend to have multiple subfolders: R/, data/, and results/. In the R directory, I’ll have multiple scripts – one for loading and cleaning data, one for visualizing this cleaned data, and at least one for analyzing the cleaned data. I save the raw data to the data/ directory, and the cleaned data and figured in the results/ directory. Here is a quick example of what I mean without referencing subdirectories: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) carnivora_clean &lt;- carnivora %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) write_csv(carnivora, &#39;carnivora_raw.csv&#39;) write_csv(carnivora_clean, &#39;carnivora_clean.csv&#39;) 1.7 Summary In this lesson we introduced the PBDB API and how to include URL-based API calls in our R code. We also covered multiple aspects of cleaning PBDB data including species identifications and temporal binning. Finally, we briefly covered exporting our data, both raw and cleaned, so that our analyses are more reproducible. All of these steps are fundamental in any analysis and form a great deal of the work associated with any analysis problem. "],
["introduction-to-bayesian-data-analysis.html", "2 Introduction to Bayesian data analysis 2.1 Objectives 2.2 Preface 2.3 Learning from data 2.4 Terms and theory 2.5 Summary", " 2 Introduction to Bayesian data analysis Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed. – Gelman et al BDA3: 4. Bayesian data analysis is just a logical procedure for processing information. – McElreath Statistical Rethinking: 12. 2.1 Objectives Learn what a (Bayesian) model actually means. Introduce the logic behind Bayesian updating. Dip our toes into numerical methods using brms. Cover the basics of summarizing a posterior distribution using tidybayes. 2.2 Preface We’re going to be using the tidyverse for data wrangling, and brms for modeling. Discussion and activities are derived from Statistical Rethinking by Richard McElreath, Bayesian Data Analysis 3 by Gelman et al., and Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. I’m also using code examples from this rewriting of Rethinking. library(pacman) p_load(tidyverse, brms, tidybayes) theme_set(theme_bw()) 2.3 Learning from data Bayesian inference is a fancy way of counting and comparing possibilities. As we collect and analyze our data, we learn which possibilities are more plausible than others. The logical strategy is “When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible.” 2.3.1 Counting and plausibility We’re going to use a simple example from Statistical Rethinking to start our thinking about Bayesian analysis. Setup: We’ve got a bag with four marbles. Each marble can be either blue (B) or white (W). We don’t know the composition of the bag. Our goal is to figure out which of possible configuration is most plausible, given whatever evidence we learn about the bag. A sequence of three marbles have pulled from the bag, one at a time, returned to the bag, and then the bag is shaken before drawing another. We draw the sequence [B W B]. How many ways are there to produce this draw? Try first this possible bag composition: [B W W W]. Here is the full table of ways to produce [B W B] given each possible bag composition. conjecture ways to produce [B W B] [W W W W] 0 x 4 x 0 = 0 [B W W W] 1 x 3 x 1 = 3 [B B W W] 2 x 2 x 2 = 8 [B B B W] 3 x 1 x 3 = 9 [B B B B] 4 x 0 x 4 = 0 What happens when we draw another marble from the bag? We update the counts! How do we update the counts? We multiply the prior counts by the new count, with the old counts are acting as our prior counts. Here is an example how we would update our counts if we were to draw an additional [B]. conjecture ways to produce [B] previous counts new count [W W W W] 0 0 0 x 0 = 0 [B W W W] 1 3 3 x 1 = 3 [B B W W] 2 8 8 x 2 = 16 [B B B W] 3 9 9 x 3 = 27 [B B B B] 4 0 0 x 4 = 0 Logically, what we’ve just done can be expressed as plausibility of [B W W W] after seeing [B W B] \\(\\propto\\) ways [B W W W] can produce [B W B] X prior plausibility of [B W W W]. But these are just counts and plausibilities; we want probabilities! First, to make explaining ourselves simpler, let’s define \\(p\\) as the proportion of blue marbles in the bag and \\(D_{new}\\) as our data. We now have: plausibility of \\(p\\) after \\(D_{new}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility of \\(p\\). We want to standardize the plausibility so that the sum of the plausibilities for all conjectures sums to 1. To standardize, we add up all the products, one for each \\(p\\), then divide each product by the sum of the products. possible combinations \\(p\\) ways to produce data plausibility [W W W W] 0 0 0 [B W W W] 0.25 3 0.15 [B B W W] 0.5 8 0.40 [B B B W] 0.75 9 0.45 [B B B B] 1 0 0 This process is equivalent to plausibility of \\(p\\) after \\(D_{new}\\) = (ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility \\(p\\)) / sum of products Each part of the calculations we’ve done so far correspond directly to quantities in applied probability theory. The conjectured proportion of blue marbles, \\(p\\), is usually called a parameter. The relative number of ways that \\(p\\) can produce the data is usually called the likelihood. The prior plausibility of a specific \\(p\\) is called the prior probability. The updated plausibility of any specific \\(p\\) is called the posterior probability. 2.3.2 Building a model Bayesian inference is made easier by working with probabilities instead of counts, but this makes everything look a lot harder. Setup: We’ve a globe with land and water sections. We want to know how much of the globe is water. Our strategy is to throw the globe up, catch it, and note the surface under the right index finger. Let’s do this 9 times. d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;)) # then rephrase in terms of trials and count of water (d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;))) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 To get the logic machinery working, we need to make some assumptions. These assumptions constitute the our model. An ideal design loop for designing a Bayesian model has three steps Data story Describe aspects of the underlying reality and the sampling process. Translate this description into a formal probability model. Acts as a framework for interpretation, but still just a story. Helps with realizing additional questions that must be answered as hypotheses are frequently vague. Updating Bayesian models begin with a set of plausibilities assigned to each possibility (Prior). Update those plausibilities based on the data to give posterior plausibility. Evaluate Certainty is no guarantee that the model is good or accurate. Supervise and critique your model! Check model’s adequacy for some purpose, or in light of stuff we don’t know. 2.3.2.1 A data story How did the data come to be? Describe aspects of the underlying reality as well as the sampling process, sufficient enough for specifying an algorithm to simulate new data. Write out the data story for this activity. 2.3.2.2 Bayesian updating A Bayesian model begins with one set of plausibilities assigned to each possible result: the prior plausibilities. These values are updated in light of data to produce our posterior plausibilities. This process is called Bayesian updating. sequence_length &lt;- 50 # how many points to calculate prob for d %&gt;% expand(n_trials, # for each value of ... p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% left_join(d, by = &quot;n_trials&quot;) %&gt;% group_by(p_water) %&gt;% # lag is the *previous* value mutate(lagged_n_success = lag(n_success, k = 1), lagged_n_trials = lag(n_trials, k = 1)) %&gt;% ungroup() %&gt;% # if first time, flat prior. # otherwise use previous posterior as new prior. mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), strip = str_c(&quot;n = &quot;, n_trials), # which draw is it? likelihood = dbinom(x = n_success, size = n_trials, prob = p_water)) %&gt;% # calculate likelihood for current draw # normalize the prior and the likelihood, making them probabilities group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + facet_wrap(~strip, scales = &quot;free_y&quot;) Side note on sample size: Bayesian estimates are valid and interpretable at any sample size. This fact is very much in contrast to the folk-wisdom around a minimum number of samples needed that you’ll hear in non-Bayesian contexts. In non-Bayesian contests, statistical inference is justified by behaviour at large samples sizes, called asymptotic behaviour. The reason Bayesian estimates are always valid and interpretable comes down to the prior. If the prior is bad, the resulting posterior could be misleading. Regardless, all estimates in either context are based on assumptions. 2.3.2.3 Evaluate Our model is learning from a “small world” of data. If there are important differences between the model and reality, then there is no guarantee of “large world” performance. Certainty is no guarantee that the model is good. As data increases, our model will become increasingly sure of the proportion of water. This sureness happens even if the model is seriously misleading because our estimates are conditional on our model. What is happening is that, given a specific model, we can be sure that plausible values are within a narrow range. It is important that you supervise and critique your model, and not just assume it worked or is correct because your code did not return an error. Anything that is not included in our model might not affect our inference directly, but might affect it indirectly because of that unmodeled dependence. For example, we currently are assuming that the order the data was collected in doesn’t matter (exchangeable), but what if the order of the observations actually did matter? Check the model’s inferences in light of aspects of the data that you know but the model doesn’t know about. This part of data analysis is an inherently creative endeavor that is up to you (the analyst) and your scientific community. Robots can’t do this step for you. The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right and are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions. Instead, our objective should be to test the model’s adequacy for some purpose. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you know as a domain expert and compare it to your model; if there is a conflict you should update your model (likelihood and/or prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently scientific questions, not statistical questions. As I said earlier, robot’s can’t do this step for you. 2.4 Terms and theory Common notation \\(y\\) observed data. \\(\\tilde{y}\\) unobserved data. \\(X\\) explanatory variables, covariates, etc. \\(\\Theta\\) parameters of interest. \\(p(\\cdot|\\cdot)\\) conditional probability distribution. \\(p(\\cdot)\\) marginal probability distribution. \\(p(\\cdot,\\cdot)\\) joint probability distribution. \\(Pr(\\cdot)\\) probability of an event. Likelihood Specifies the plausibility of data mathematically. Maps each conjecture onto the relative number of ways the data could occur, given that possibility. Sometimes written \\(p(y | \\Theta)\\) or \\(L(\\Theta | y)\\). Parameters Adjustable inputs. 1+ quantities we want to know about. Represent the different conjectures for causes or explanations of the data Difference between data and parameters is fuzzy and exploitable in Bayesian analysis –&gt; advanced topic Prior Every parameter you are trying to estimate must be provided a prior. The “initial conditions” of the plausibility of each possibility. Which parameters values do we think are more plausible than others? Constrain parameters to reasonable ranges. Express any knowledge we have about that parameter before any data is observed. An engineering assumption; helps us learn from our data. Regularizing or weakly informative priors are conservative in that they tend to guard against inferring strong associations between variables –&gt; advanced topics. Sometimes written \\(p(\\Theta)\\). Posterior Logical consequence of likelihood, the set of parameters to estimate, and priors for each parameter. The relative plausibility of different parameter values, conditional on the data. Sometimes written \\(p(\\Theta | y)\\). 2.4.1 Bayes’ Theorem The logic defining the posterior distribution is called Bayes’ Theorem. The theorem itself is an intuitive result from probability theory. First, describe the model and data as a joint probability. \\[ \\begin{align} p(y, \\Theta) &amp;= p(\\Theta | y) p(y) \\\\ p(y, \\Theta) &amp;= p(y | \\Theta) p(\\Theta) \\\\ \\end{align} \\] Then set equal to each other and solve for \\(p(\\Theta | y)\\): \\[ \\begin{align} p(\\Theta | y) p(y) &amp;= p(y | \\Theta) p(\\Theta) \\\\ p(\\Theta | y) &amp;= \\frac{p(y | \\Theta) p(\\Theta)}{p(y)} \\\\ \\end{align} \\] Et voilà, Bayes’ Theorem. The probability of any particular value of \\(\\Theta\\), given the data, is equal to the product of the likelihood and the prior, divided by \\(p(y)\\). “But what’s \\(p(y)\\)?” you ask. The term \\(p(y)\\) is a confusing one – it can be called the “average likelihood,” “evidence,” or “probability of the data.” The average likelihood here means it is averaged over the prior and its job is to standardize the posterior so it integrates to 1. \\(p(y)\\) is expressed mathematically as: \\[ p(y) = E(p(y | \\Theta)) = \\int p(y | \\Theta) p(\\Theta) d\\Theta \\] \\(E(\\cdot)\\) means to take the expectation, a (weighted) average. Notice also that \\(p(y)\\) is a type of marginal probability distribution; the process of integrating out a term (\\(\\Theta\\)) is called marginalization – we are averaging \\(y\\) over all values of \\(\\Theta\\). Remember also that an integral is like an average over a continuous distribution of values. 2.4.2 But how does it work? Our model has three parts: likelihood, parameters, and the prior. These values get put into a “motor” that gives us a posterior distribution. The motor goes through the process of conditioning the prior on the data. Turns out that knowing all the rules doesn’t necessarily help us with the calculations. For most interesting models we will ever consider, the necessary integrals in the Bayesian conditioning machinery have no closed form and can’t be calculated no matter how talented you are. Each new parameter effectively adds a new level to the integral used to calculate \\(p(y)\\). Instead, we need to rely on numerical techniques to approximate the mathematics defined in Bayes’ theorem. All of the numerical techniques we use produce only samples from the posterior distribution, not the distribution itself. Luckily, samples from the distribution are easier to work with than the actual distribution – this way we don’t have to do integrals. 2.4.2.1 Grid approximation One of the simplest conditioning techniques is grid approximation. While most parameters are continuous, we can get a decent approximation of them by considering only a finite grid of parameter values. For any particular value \\(p&#39;\\), compute the posterior probability by multiplying the prior probability of \\(p&#39;\\) by the likelihood of \\(p&#39;\\). Grid approximation is a teaching tool that forces you to really understand the nature of Bayesian updating. You will probably never use it in your actual work. Grid approximation scales very poorly as the number of parameters increases. How to do grid approximation: Define a grid (range of values to look at). Calculate the value of the prior at each parameter value of the grid. Compute the likelihood at each parameter value. Compute the unstandardized posterior at each parameter, but multiplying the prior by the likelihood. Standardize the posterior by dividing each value by the sum of all unstandardized values. The number of points you evaluate on the grid determines the precision of your estimates. More points, finer grained posterior. For example, here’s the posterior probability of the percent of water on the globe given our data evaluated at 5 points and 20 points. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # computer product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # compute product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) 2.4.2.2 Markov chain Monte Carlo For many models, grid approximation or quadratic approximation just aren’t good enough. Grid approximation takes too long as your model gets bigger. Quadratic approximation chokes on complex models. Instead, we end up having to use a technique like Markov chain Monte Carlo (MCMC). Unlike grid approximation, where we computed the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, where the frequencies of those values correspond to the posterior plausibilities. Let’s do a quick example where we fit this model using brms. This package acts as an interface with Stan probabilistic programming language which implements Hamiltonian Monte Carlo sampling, a fancy type of MCMC. The function brm() is the workhorse of the brms package, and builds and compiles a Stan model as defined in R. If you’ve used functions like lm() or glm(), some of the syntax should look familiar to you. # this can take a bit as the model compiles globe_brms &lt;- brm(data = list(w = 24), # generate data family = binomial(link = &quot;identity&quot;), # define likelihood distribution formula = w | trials(36) ~ 1, # define parameter prior = prior(uniform(0, 1), class = Intercept), # give prior to parameter control = list(adapt_delta = 0.95), # control sampling behavior --&gt; advanced topic refresh = 0, # silences a bunch of text iter = 2000, # how many draws from posterior? (default) warmup = 1000) # how many draws till we start recording (default = 1/2 iter) print(globe_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.50 0.79 1214 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # look at the posterior distribution of proportion water globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_x_continuous(&#39;proportion water&#39;, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) 2.4.2.3 Aside on Interpreting Probabilities From Math with Bad Drawings. 2.4.3 Working with samples In applied Bayesian analysis we rarely work directly with the integrals required by Bayes’ theorem. Most numerical techniques we use, including MCMC methods, produce a set of samples that are individual draws from the posterior distribution. These samples transform a problem in calculus to a problem in data summary. It is easier to count the number of samples within an interval then calculate the integral for that interval. This section also serves as a brief introduction to summarizing posterior samples using tidybayes, which we will continue using in our next lesson. Once our model produces a posterior distribution, the model’s work is done. It is now our job to summarize and interpret that posterior. Common questions we might want to ask include: How much posterior probability lies below some parameter value? How much posterior probability lies between two parameter values? Which parameter value marks the lower 5% of the posterior probability? Which range of parameter values contains 90% of the posterior probability? Which parameter value has the highest posterior probability? 2.4.3.1 Intervals of defined boundaries What is the posterior probability that the proportion of water is less than 50%? Count the number of observations that are less than 0.5, then divide by the number of samples. globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &lt; 0.5) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.0278 What if we want to know the posterior probability that the proportion of water is between 0.5 and 0.75? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &gt; 0.5 &amp; b_Intercept &lt; 0.75) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.862 2.4.3.2 Intervals of defined mass An interval of defined mass report two parameter values that contain between them the specified amount of posterior probability, a probability mass. You probably have heard of confidence intervals – an interval of posterior probability is called a credible interval, though the distinction between the terms isn’t terribly important. There are two kinds of intervals of defined mass: percentile (or quantile) interval, and highest posterior density interval. 2.4.3.2.1 Percentile interval (PI) For example, you may want to know the boundaries of the lower 80% posterior interval. This interval has to start at 0, but where does it stop? How do we calculate the 80th percentile? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q80 = quantile(b_Intercept, 0.8)) ## # A tibble: 1 x 1 ## q80 ## &lt;dbl&gt; ## 1 0.723 What about the middle 80% interval? There are lots of ways to get this information, so here are two examples. # one way (qu &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q10 = quantile(b_Intercept, 0.1), q90 = quantile(b_Intercept, 0.9))) ## # A tibble: 1 x 2 ## q10 q90 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.555 0.754 # another way globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% quantile(prob = c(0.1, 0.9)) ## 10% 90% ## 0.5549006 0.7542126 We can also plot this interval on the distribution. # plot the distribution p1 &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(limits = c(0, 1)) # get back the density line calculation p1_df &lt;- ggplot_build(p1)$data[[1]] # this is messy # shade area under the distribution p1 + geom_area(data = subset(p1_df, x &gt; qu$q10 &amp; x &lt; qu$q90), aes(x=x,y=y), fill = &quot;black&quot;, color = NA) 2.4.3.2.2 Highest posterior density interval An HPDI is defined as the narrowest interval containing the specified probability mass. There are an infinite of posterior intervals with the same mass, but what if you want that interval that best represents the parameter values most consistent with the data AND you want the densest of these intervals. Here an example of a 50% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% hdi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6137909 0.7158464 # compare to 50% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% qi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6082956 0.7113937 Why are the PI and HPDI not equal? If the choice of interval makes a big difference in your summary, then you probably shouldn’t be using intervals to summarize the posterior! The entire distribution of samples is our actual estimate, these summaries are just there to help digest this wealth of information. Do not only work with simplified versions of our posterior estimates! 2.4.3.3 Point estimates Sometimes we only want a single point from the distribution. Given an entire posterior distribution, what value should we report? Mechanically, this task is simple – pick a summary (e.g. mean) and go. Conceptually, however, this task is actually quite complex. The Bayesian parameter estimate is the entire posterior distribution, and not just a single number. In most cases, it is unnecessary to choose a single point estimate. It is always better to report more than necessary about the posterior distribution than not enough. The three most common point estimates are the mean, median, and mode – you’re probably already familiar with all three of them. A principled way of choosing among these three estimates is considering as products of different loss functions. Loss functions are an advanced topic we will not cover today but I encourage you to read up on; here’s a good blog entry by John Myles White on the subject. The mode represents the parameter value with the highest posterior probability, or the maximum a posteriori estimate (MAP). In frequentist contexts, the maximum likelihood estimate is the equivalent to mode of the likelihood function. Calculating the mode of a distribution is an optimization problem and isn’t always easy. Luckily, tidybayes has a function which gets us a mode (and other information) from the posterior samples. Here is a code snippet that gives a mode and a middle 95% percentile interval: globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mode_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.662 0.497 0.794 0.95 mode qi We could also report a mean or median. Here are some ways to do get these estimates along with some kind of interval: # mean with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.497 0.794 0.95 mean qi # mean with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.499 0.795 0.95 mean hdci # median with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.661 0.497 0.794 0.95 median qi # median with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.661 0.499 0.795 0.95 median hdci Usually it is better to communicate as much about the posterior distribution as you can. 2.5 Summary So far we’ve introduced the conceptual mechanism driving Bayesian data analysis. This framework emphasizes the posterior probability distribution, which is the logical compromise between our previous information and whatever new information we’ve gained (given our model). Posterior probabilities state the relative plausibility of each conjectured possibility that could have produced the data, given our model. These plausibilities are updated in light of observations, a process known as Bayesian updating. We’ve defined the components of a Bayesian model: a likelihood, one or more parameters, and a prior for every parameter. The likelihood defines the plausibility of the data, given a fixed value for the parameters. The prior provides the initial plausibility of each parameter value, before accounting for the data. These components, when processed through Bayes’ Theorem, yield the posterior distribution. Many of the actual calculations necessary to yield the posterior distribution have no closed-form solution and must instead be approximated using numerical methods. We covered grid approximation as a gentle introduction to sampling. We also fit our basic model using brms, which uses Stan’s HMC engine to condition our estimates on the data. Given the posterior samples from our model fit, we also covered basic methods for summarizing a posterior distribution such as intervals and point estimates. "],
["introduction-to-linear-regression.html", "3 Introduction to linear regression 3.1 Objectives 3.2 Linear regression 3.3 Summary", " 3 Introduction to linear regression 3.1 Objectives Set up a linear regression model. Interpret the parameters of a regression model. Communicate a model descriptions to others. Fit regression models in brms. Basics of summarizing and visualizing a model fit. library(pacman) p_load(tidyverse, modelr, brms, tidybayes, here) theme_set(theme_bw()) 3.2 Linear regression Linear regression refers to a large family of statistical models which attempt to learn about the mean and variance of some measurement, using an additive combination of other measures. Linear regression is a descriptive model that corresponds to many different processes. Normally regression is presented as a way of representing the relationship between two or more variables. I believe this description obscures the interpretation and meaning of regression results. Instead, we’re going to think of linear regression as a method that summarizes how the average values of a numerical outcome vary as a linear functions of predictors. Regression can be used to predict an outcome given a linear function of those predictors, and regression coefficients can be thought of as comparisons across predicted values or as comparisons among averages in the data. A regression coefficient describes the expected change in the response per unit change in its predictor. Linear regression uses a Gaussian/Normal distribution to describe the distribution of our measurement of interest. Like any model, linear regression is not universally applicable. But linear regression is pretty foundational in statistics because once you can build and interpret a linear regression model, it is easy to move on to other types of regression for when things aren’t Normal. Models of normally distributed data are very common: t-test, single regression, multiple regression, ANOVA, ANCOVA, MANOVA, MANCOVA, etc. All of these models are functionally equivalent. Learning and understanding each of these special cases is a lot of unnecessary work; instead, we’re going to focus on a general modeling strategy that subsumes all of these special cases. Additionally, we’re going to cover a single means of communicating a model that encodes all of our assumptions and presents them clearly. 3.2.1 Talking about models Here are the choices encoded in a model description: Outcome variable or variables that we hope to predict or understand (\\(y\\)). Likelihood distribution that defines the plausibility of the individual observations. Predictors or covariates – a set of other measurements that we hope to use to predict or understand the outcome (\\(X\\)). Relation between the shape of the likelihood distribution (e.g. location and scale) to the predictor variables. The nature of this relation forces us to define all the parameters of the model. Priors for all of the parameters in the model. Here’s the globe tossing model from last week \\[ \\begin{align} w &amp;\\sim \\text{Binomial}(n, p) \\\\ p &amp;\\sim \\text{Uniform}(0, 1). \\\\ \\end{align} \\] Identify the parts of this model description. The \\(\\sim\\) symbol indicates a stochastic relationship. A stochastic relationship means that the variable or parameter is mapped onto a distribution of values. It is stochastic because no single instance of the variable on the left is known with certainty. This relationship is probabilistic as some values are more plausible than others, though there are many plausible values under any model. It is ok if you do not immediately understand this notation; that’s a normal part of learning. This notation allows us to specify and communicate our model clearly so that other people can understand what we’re doing. This language is general, and can be used to describe all model types. By continuing to use this notation, we will build familiarity with this syntax. 3.2.2 Growing a regression model We’re going to use a real dataset of invertebrate valve sizes and slowly build up a regression model to describe differences between taxonomic gorups. We will begin with a proto-regression model and then add predictors. The dataset we’re going to have fun with as part of today’s activities is the Bivalve and Brachiopod body size data from Payne et al. 2014 ProcB. Their data includes location, age, type, and valve length. Load the “occurrence” tab-delimited file and start exploring the distribution of body sizes. # `here` allows us to grab from subdirectory without formally specifying path # allows portability r &lt;- read_tsv(here(&#39;data&#39;, &#39;payne_bodysize&#39;, &#39;Occurrence_PaleoDB.txt&#39;)) ## Parsed with column specification: ## cols( ## taxon_name = col_character(), ## pbdb_collection_no = col_double(), ## p_lat = col_double(), ## p_lng = col_double(), ## int_midpoint = col_double(), ## taxon = col_character(), ## sub = col_character(), ## size = col_double() ## ) # modify raw data for use # genus can occur 1+ times (d &lt;- r %&gt;% group_by(taxon_name, taxon) %&gt;% dplyr::summarize(size = mean(size)) %&gt;% # is this always a good idea? mutate(size_log = log(size)) %&gt;% # log transform ungroup()) ## # A tibble: 3,980 x 4 ## taxon_name taxon size size_log ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abra Biv 38 3.64 ## 2 Abrekia Bra 11.6 2.45 ## 3 Abruptolopha Biv 98.8 4.59 ## 4 Acambona Bra 39.9 3.69 ## 5 Acanthalosia Bra 37.5 3.62 ## 6 Acanthambonia Bra 1.74 0.556 ## 7 Acanthatia Bra 23.5 3.16 ## 8 Acanthocardia Biv 102. 4.63 ## 9 Acanthocosta Bra 24.5 3.20 ## 10 Acanthopecten Biv 12.1 2.50 ## # … with 3,970 more rows # look at the data d %&gt;% gather(key, value, size, size_log) %&gt;% ggplot(aes(x = value)) + stat_bin() + facet_wrap(~ key, scales = &#39;free_x&#39;, switch = &#39;x&#39;) + labs(x = &#39;valve length&#39;) ## Warning: &#39;switch&#39; is deprecated. ## Use &#39;strip.position&#39; instead. ## See help(&quot;Deprecated&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We want a measurement variable to model as a Gaussian distribution. The Gaussian distribution has two parameters describing the shape of the distribution – mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Here’s a quick picture of a Gaussian distribution, play around with the mean and standard deviation parameters. ggplot(data = tibble(x = seq(from = -25, to = 25, by = 0.1)), aes(x = x, y = dnorm(x, mean = 5, sd = 1000))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) The way Bayesian updating works means we want to consider all possible combinations of \\(\\mu\\) and \\(\\sigma\\) and rank them by posterior plausibility. The posterior is in effect a distribution of plausible Gaussian distributions. To define the log valve length as Gaussian distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we write \\[ s_{i} \\sim \\text{Normal}(\\mu, \\sigma) \\] What do the different parts of this statement mean? To complete this model we need to define the priors for the parameters \\(\\mu\\) and \\(\\sigma\\). We need a joint prior \\(Pr(\\mu, \\sigma)\\), but most of the time we’ll just define independent priors for each parameter – this is equivalent to saying \\(Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)\\). Here’s a more complete look at the Gaussian model of log valve length: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(3, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] What do each of these three lines mean? Let’s discuss the choice of priors for \\(\\mu\\) and \\(\\sigma\\). It is a good idea to plot your priors so that you can better understand your assumptions. # prior for mean of log valve length ggplot(data = tibble(x = seq(from = -50, to = 50, by = .1)), aes(x = x, y = dnorm(x, mean = 3, sd = 10))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu)) # prior for standard deviation of log valve length ggplot(data = tibble(x = seq(from = 0, to = 30, by = .1)), aes(x = x, y = dunif(x, min = 0, max = 20))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(sigma)) But what do these distributions mean for the distribution of log valve lengths? These individual priors imply the full distribution of lengths, so let’s simulate them together and plot those results: n &lt;- 1e4 # number of samples tibble(sample_mu = rnorm(n, mean = 5, sd = 5), sample_sigma = runif(n, min = 0, max = 10)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% # joint distribution ggplot(aes(x = x)) + stat_density() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&quot;Prior predictive distribution for &quot;, italic(s[i])))) The resulting distribution describes the relative prior plausibilities we’ve defined for the distribution of log valve lengths. Play around with the numbers in the priors and see its effect on the prior probability density of log valve lengths. 3.2.2.1 Sampling from the model Now that we’ve completely defined out model (likelihood, data to condition on, parameters, and priors for all parameters) we can estimate the posterior plausibilities of our parameter values. We’re going to use the brms package to fit our model. This package was briefly introduced at the end of last lesson, and we’re going to get more experience with it today. After we fit the model, we’ll use functions from tidybayes that will help us extract and visualize our posterior distribution. m_1 &lt;- brm(data = d, family = gaussian(), formula = bf(size_log ~ 1), prior = c(prior(normal(3, 10), class = Intercept), prior(uniform(0, 20), class = sigma)), iter = 2000, # default warmup = 1000, # 1/2 iter chains = 4, # each chain is a set of samples cores = 4, # parallel processing; this might not work on your computer refresh = 0) # less text output # what does the brm object look like? print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: size_log ~ 1 ## Data: d (Number of observations: 3980) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.20 0.01 3.17 3.23 2991 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.92 0.01 0.90 0.94 3196 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # what do the estimates and chains look like? plot(m_1) # what are the parameter named? get_variables(m_1) ## [1] &quot;b_Intercept&quot; &quot;sigma&quot; &quot;lp__&quot; &quot;accept_stat__&quot; ## [5] &quot;stepsize__&quot; &quot;treedepth__&quot; &quot;n_leapfrog__&quot; &quot;divergent__&quot; ## [9] &quot;energy__&quot; # extract the samples m_1 %&gt;% spread_draws(b_Intercept, sigma) ## # A tibble: 4,000 x 5 ## .chain .iteration .draw b_Intercept sigma ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 3.20 0.919 ## 2 1 2 2 3.20 0.913 ## 3 1 3 3 3.19 0.919 ## 4 1 4 4 3.19 0.931 ## 5 1 5 5 3.21 0.903 ## 6 1 6 6 3.21 0.907 ## 7 1 7 7 3.21 0.916 ## 8 1 8 8 3.20 0.932 ## 9 1 9 9 3.21 0.910 ## 10 1 10 10 3.21 0.909 ## # … with 3,990 more rows We now have 4000 samples from the joint posterior. How do we want to summarize them? Here are some examples: # median and 95% quantile interval m_1 %&gt;% gather_draws(b_Intercept, sigma) %&gt;% median_qi() # ?brms::point_interval for documentation ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 3.20 3.17 3.23 0.95 median qi ## 2 sigma 0.916 0.896 0.936 0.95 median qi # posterior predictive distribution m_1 %&gt;% add_predicted_draws(newdata = d, # original data gives to simulate from model = ., # the model we want the PPD from n = 100) %&gt;% # how many draws from PPD per observation ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = d, # compare to original data distribution mapping = aes(x = size_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &#39;log valve length&#39;) We will be discussing the posterior predictive distribution further later in the lesson. For now, think of this as the distribution of outcomes implied by our model – the distribution of plausible distributions. 3.2.3 Adding a predictor to the mix Currently, our model doesn’t really resemble what we think of as “regression.” Typically, we want to understand how the mean of our outcome variable is related to one or more predictor variables. Let’s start with a basic analysis questions: do Bivalves and Brachiopods differ in valve length and how? Start by looking at the data. d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) To build a linear model the strategy is to make the parameter for the mean of the Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters we invent. A linear model means that we assume that the predictors have a perfectly constant and additive relationship to the mean of the outcome. Currently, our model looks like this: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(3, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] How do we add “Bivalve vs Brachiopod” to our model of valve size? First, let the mathematical name of the column “taxon” from the tibble d be called \\(x\\). This variable \\(x\\) is a predictor variable that can take one of two values: 0 for Biv(alve), and 1 for Bra(chiopod). Additionally, this vector has the same length as \\(s\\). How do we then express how \\(x\\) describes or predicts the values of \\(s\\)? To get taxonomic group into the model, we need to define \\(\\mu\\) as a function of the values in \\(x\\). Here’s how we could do this: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta x_{i}\\\\ \\alpha &amp;\\sim \\text{Normal}(3, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] What do each of the lines in the model represent? What has changed and why? But where did \\(\\alpha\\) and \\(\\beta\\) come from? We made them up! \\(\\mu\\) and \\(\\sigma\\) are necessary and sufficient for describing the Gaussian distribution. \\(\\alpha\\) and \\(\\beta\\) are devices we invent for manipulating \\(\\mu\\), allowing it vary across cases in our data. Making up new parameters like \\(\\alpha\\) and \\(\\beta\\) is a common strategy for expanding the amount of information in our model. These parameters are targets of our learning – each must be described in the posterior density. When we want to learn something, we invent a parameter (or parameters) describing it. In the case of the linear model, \\(\\mu_{i} = \\alpha + \\beta x_{i}\\), we are now asking two questions about the mean of \\(s\\) instead of just one. What is the expected log valve length when \\(x_{i} = 0\\) (e.g. species is a Bivalve)? The parameter \\(\\alpha\\) answers this question. For this reason, \\(\\alpha\\) is called the intercept. What is the change in expected log valve length, when \\(x_{i}\\) changes by 1 unit? The parameter \\(\\beta\\) answers this questions, and is often called a slope. These two parameters along with \\(x\\) describe a line that passes through \\(\\alpha\\) when \\(x_{i} = 0\\) and has slope \\(\\beta\\). Remember that \\(x\\) is a binary vector – it only takes on one of two values: 0 or 1. For a binary predictor, the regression coefficient is interepreted as the difference between the averages of the two groups. Let’s think more about this choice of prior. Do you think there is an equal chance that average brachiopod valves are larger or smaller than bivalves? In this context, we have so much data that this is harmless. In other contexts, our sampler might need more information to finds its target. So let’s fit the model: m_2 &lt;- brm(data = d, family = gaussian(), formula = size_log ~ 1 + taxon, prior = c(prior(normal(3, 10), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 20), class = sigma)), iter = 2000, # default warmup = 1000, # 1/2 iter chains = 4, # each chain is a set of samples cores = 4, # parallel processing; this might not work on your computer! refresh = 0) # less text output # look at the brm summary print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: size_log ~ 1 + taxon ## Data: d (Number of observations: 3980) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.69 0.02 3.65 3.74 3739 1.00 ## taxonBra -0.77 0.03 -0.82 -0.71 3535 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.84 0.01 0.82 0.86 3978 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # look at the brm plot plot(m_2) 3.2.3.1 Aside: Dummy coding brm, and R in general, will automatically translate categorical predictors like our taxon variable into what’s called dummy coding. Here’s an illustration of what that means using the subtype variable “sub” from our raw data: r %&gt;% distinct(sub) ## # A tibble: 3 x 1 ## sub ## &lt;chr&gt; ## 1 Het ## 2 nonHet ## 3 inart r %&gt;% model_matrix(~ sub) %&gt;% # from modelr distinct() ## # A tibble: 3 x 3 ## `(Intercept)` subinart subnonHet ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 0 1 ## 3 1 1 0 r %&gt;% model_matrix(~ sub) ## # A tibble: 164,402 x 3 ## `(Intercept)` subinart subnonHet ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 ## 7 1 0 0 ## 8 1 0 0 ## 9 1 0 0 ## 10 1 0 0 ## # … with 164,392 more rows One of the values of the column is designated to be the intercept. In R, the default intercept is the first value of the vector, alphabetically. The other variables are called contrasts or dummy variables – they describe the difference in mean value when compared to the intercept. By adding the intercept and the regression coefficient, you get the estimated mean for that group. If the categorical variable has \\(k\\) states, then there are \\(k - 1\\) contrasts or dummy variables. There are many other strategies for encoding categorical variables, but dummy coding is by far the most common – it helps that it is the R default. Don’t worry too much about understanding dummy coding yet – just understand that it exists and that we will return to it in a later lesson. 3.2.4 Interpreting the model fit There are two broad categories of how we process model fit: tables, and plotting. Tables are fine, but plotting is key. It is easy to feel like you understand a table while still getting it wrong. Plotting the implications of your estimates will allow you to inquire about several things that are sometimes hard to read from tables: Whether or not the model fitting procedure worked correctly. The absolute magnitude, rather than merely relative magnitude, or a relationship between outcome and predictor. The uncertainty surrounding an average relationship. The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty. With practice extracting estimates from your model and plotting them, you can ask any question you can think of, for any model. Let’s start by getting a basic summary of our posterior: m_2 %&gt;% gather_draws(b_Intercept, b_taxonBra, sigma) %&gt;% median_qi() ## # A tibble: 3 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 3.69 3.65 3.74 0.95 median qi ## 2 b_taxonBra -0.766 -0.823 -0.714 0.95 median qi ## 3 sigma 0.840 0.822 0.859 0.95 median qi The first row corresponds to the model intercept, which we called \\(\\alpha\\). This parameter corresponds to the expected log valve length when \\(x = 0\\). In the case of this model, this is easily interpreted as the expected log valve length of a Bivalve species. The second row is the slope term \\(\\beta\\). This parameter describes the expected change in log valve size associated with unit change in \\(x\\). In this case of this model, this parameter is easier to describe: this is an estimate of the expected difference in log valve length between Bivalves (\\(x = 0\\)) and Brachiopods (\\(x = 1\\)). By adding \\(\\beta\\) and \\(\\alpha\\), we get the estimate for Brachiopod expected log valve length. The third line is the standard deviation term \\(\\sigma\\), which describes the with of the distribution of log valve lengths. A useful trick for interpreting \\(\\sigma\\) is that about 95% of the probability of a Gaussian distribution lies between plus/minus two standard deviations from the mean. In this case, the estimate tells us that 95% of plausible log valve lengths lie within 1.68 log millimeters (\\(2\\sigma\\)) of the mean log valve length. 3.2.4.1 Linear predictor Our regression model describes a line with an intercept and a slope. As demonstrated above, this is true even in the case of a binary predictor – even though our predictor can only take one of two values, the formula still describes a line. The function brms::add_fitted_draws() estimates the expected log valve length from the linear model part of our model. Remember that the linear model describes only the mean log valve length, and not the spread of log valve length. This function is a convenient way to help us visualize this part of our model. Here’s an illustration of the linear relationship between taxonomic group and log valve size as described by the median estimates for each taxonomic group: # parameters of the line d_fitted &lt;- d %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% # 100 posterior estimates ungroup() %&gt;% group_by(taxon) %&gt;% # want to know taxon summary dplyr::summarize(value = median(.value)) %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) # make text clearer d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + geom_line(data = d_fitted, mapping = aes(x = taxon, y = value, group = 1), size = 1, colour = &#39;blue&#39;) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) The above plot uses the median point estimates and includes none of our uncertainty about the relationship between the taxonomic groups and log valve length. There are a few ways we can demonstrate our uncertainty about the linear relationship. We can plot multiple lines at the same time: d_fitted &lt;- d %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) # make text clearer d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + geom_line(data = d_fitted, mapping = aes(x = taxon, y = .value, group = .draw), size = 1, colour = &#39;blue&#39;, alpha = 0.1) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) 3.2.4.2 Posterior prediction An aspect of Bayesian models we’ve yet to discuss at length is the posterior predictive distribution \\(p(\\tilde{y} | y)\\). Prediction is when, given our model and parameter estimates, we want to estimate the outcome for some combination of covariates. The posterior predictive distribution is the distribution of outcomes defined by the plausible parameter values and data – instead of a single prediction, we have a distribution of predictions. For example, we might want to predict the log valve length of some species given our model and parameter estimates. For each possible value of a parameter, there is an implied distribution of outcomes. If we compute the distribution of outcomes for each value, this gives us a posterior predictive distribution. Our full model describes log valve length as a Gaussian distribution with a mean (as a linear model) and a standard deviation. Our previous plots only considered the linear model aspect which describes the mean of the distribution, but there is still more information in our model that we’ve yet to consider: the estimated standard deviation \\(\\sigma\\). The brms::add_predicted_draws() function does this exactly. For each “new” observation, we obtain a series of predictions about that observations log valve length, not just the expected value of log valve length. This function is similar to the brms::add_fitted_draws() function we used above, but instead of predicting from the linear model we are predicting from the entire distribution. Let’s illustrate this by comparing our log valve length data to our posterior predictive distribution for that data: d_predicted &lt;- d %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + stat_lineribbon(data = d_predicted, mapping = aes(y = .prediction), alpha = 0.5) + scale_fill_brewer() 3.2.4.2.1 Posterior predictive tests What if we want to test how well our model fits specific parts of our data, and not just compare the similarities of their distributions? For example, how well does our model estimate the mean log valve size of each taxon? Here’s the logic of posterior predictive tests: if data simulated from our posterior predictive distribution is able to reproduce a specific aspect of the original data, then the model might be doing something right. And if there are systematic failures in our model’s ability to predict the data, then the model must be doing something wrong. Remember the warnings from last lesson: The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right and are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions. Instead, our objective should be to test the model’s adequacy for some purpose. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you know as a domain expert and compare it to your model; if there is a conflict you should update your model (likelihood and/or prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently scientific questions, not statistical questions. As I said earlier, robot’s can’t do this step for you. Here’s are a few example posterior predictive tests: # summarize the original data d_sum &lt;- d %&gt;% group_by(taxon) %&gt;% dplyr::summarize(mean = mean(size_log), sd = sd(size_log), iqr = IQR(size_log)) %&gt;% # lots of options, none is best gather(key, value, mean, sd, iqr) %&gt;% mutate(key = recode(key, mean = &#39;Mean&#39;, sd = &#39;Std Dev&#39;, iqr = &#39;Inter Quart Range&#39;), taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) d %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% # calculate from n PPD draws group_by(taxon, .draw) %&gt;% dplyr::summarize(mean = mean(.prediction), sd = sd(.prediction), iqr = IQR(.prediction)) %&gt;% ungroup() %&gt;% gather(key, value, mean, sd, iqr) %&gt;% mutate(key = recode(key, mean = &#39;Mean&#39;, sd = &#39;Std Dev&#39;, iqr = &#39;Inter Quart Range&#39;), taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &#39;grey80&#39;, colour = NA) + geom_vline(data = d_sum, mapping = aes(xintercept = value), size = 1.5, colour = &#39;grey20&#39;) + facet_wrap(taxon ~ key, scales = &#39;free&#39;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &#39;&lt;stat&gt; log valve length&#39;) Where does our model do well? Where does our model do poorly? Why? How could we modify our model to overcome these failures? Even though it should be obvious and mandatory to do go through with and report an extended posterior predictive checking process, it is relatively rare in macroevolutionary biology and paleobiology. Indeed, so rare that posterior predictive checking, if done creatively and well, can get you a paper: Example 1, Example 2, Example 3. 3.3 Summary To review, this lesson was an exercise in developing, communicating, and summarizing a Bayesian model – specifically, a linear regression model. We’ve slowly developed a linear regression model by expanding a Gaussian distribution to include the effects of predictor information. We first developed our model using the symbolic representation of a statistical model, and we then implemented our model using functions from brms. We explored a number of ways of representing and visualizing posterior distributions; these included tables and figures. We briefly covered the difference between fitted predictions and the posterior predictive distribution. Finally, we discussed the concept of posterior predictive tests. "],
["reg-continue.html", "4 Continuing with regression with continuous predictors 4.1 Objectives 4.2 Our first example 4.3 A single continuous predictor 4.4 Summary", " 4 Continuing with regression with continuous predictors 4.1 Objectives In a previous lesson we introduced linear regression with a single, binary predictor. This lesson expands on that initial introduction by introducing and explaining continuous predictors. Along the way I will continue to emphasize checking the quality or adequacy of model fit as an important part of both understanding our model and improving out model. Including a continuous predictor in a regression model. Learn to interpret continuous predictors. Continue to focus on evaluating model fit as major step in modeling. library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, modelr, knitr, kableExtra) theme_set(theme_bw()) 4.2 Our first example For this lesson we will be using fitting models to data from the PanTHERIA} database, a large collection of trait data for extant mammals. For a more detailed explanation of the dataset and each variable, you can review the data dictionary. A key detail to PanTHERIA is that missing data is coded as -999.00 and not as NA or blank cells. This knowledge is something we can use when importing our data so that it translates easily into the R environment. Also, a lot of the variables have terrible names with mixed unction, capital letters, and begin with numbers – all of these are very bad to program around and need to be dealt with (using the janitor package). That all being said, let’s import the dataset, clean it up a bit, and then start visualizing it. pantheria &lt;- read_tsv(here(&#39;data&#39;, &#39;PanTHERIA_1-0_WR05_Aug2008.txt&#39;), na = &#39;-999.00&#39;) %&gt;% clean_names() %&gt;% mutate(mass_log = log(x5_1_adult_body_mass_g), range_group_log = log(x22_1_home_range_km2), range_indiv_log = log(x22_2_home_range_indiv_km2), density_log = log(x21_1_population_density_n_km2), activity_cycle = case_when(x1_1_activity_cycle == 1 ~ &#39;nocturnal&#39;, x1_1_activity_cycle == 2 ~ &#39;mixed&#39;, x1_1_activity_cycle == 3 ~ &#39;diurnal&#39;), trophic_level = case_when(x6_2_trophic_level == 1 ~ &#39;herbivore&#39;, x6_2_trophic_level == 2 ~ &#39;omnivore&#39;, x6_2_trophic_level == 3 ~ &#39;carnivore&#39;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## MSW05_Order = col_character(), ## MSW05_Family = col_character(), ## MSW05_Genus = col_character(), ## MSW05_Species = col_character(), ## MSW05_Binomial = col_character(), ## References = col_character() ## ) ## See spec(...) for full column specifications. pantheria %&gt;% drop_na(activity_cycle) %&gt;% ggplot(aes(x = trophic_level, y = range_group_log)) + geom_violin(fill = &#39;grey80&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) + geom_jitter(height = 0, alpha = 0.5, mapping = aes(colour = msw05_order)) + scale_colour_viridis(discrete = TRUE, name = &#39;Order&#39;) + labs(x = &#39;Activity Cycle&#39;, y = expression(paste(&#39;Group range size &#39;, log(km^2))), title = &#39;Group range size differences between trophic levels, \\norders highlighted&#39;) ## Warning: Removed 1079 rows containing non-finite values (stat_ydensity). ## Warning: Removed 1079 rows containing missing values (geom_point). pantheria %&gt;% ggplot(aes(x = mass_log, y = density_log, colour = msw05_order)) + geom_point() + scale_colour_viridis(discrete = TRUE, name = &#39;Order&#39;) + labs(x = expression(paste(&#39;Body mass &#39;, log(g^2))), y = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Body mass and population density, orders highlighted&#39;) ## Warning: Removed 4469 rows containing missing values (geom_point). There are tons of ways we could deconstruct this dataset, some much more logical than others. For this tutorial, we’re going to focus on trying to understand how population density varies across mammals. There are tons of factors that can influence the population density of a species, so our process will be to slowly build up a model one predictor at a time. Let’s begin our model in a similar fashion to our previous one, with an intercept-only model. We can then add our first continuous predictor from there. Our response variable is density_log is a continuous value from \\(-\\infty\\) to \\(\\infty\\) so it is probably a good place to start by assuming it could be approximated with a normal distribution, as is common with linear regression. The normal distribution as defined in brms has two parameters: mean and standard deviation. For our simple intercept-only model, we do not need to expand on these parameters – after all, the intercept describes the average value or mean of the response. Let’s write this out. Let \\(y\\) be density_log, \\(\\mu\\) be the mean of density_log, and \\(\\sigma\\) be the standard deviation of density_log. \\[ y \\sim \\text{Normal}(\\mu, \\sigma) \\] Can you recall what all the of the parts of the above statement mean? What are we missing? Priors! We can probably stick with pretty vague priors here – the mean is probably somewhere between -10 and 10 log(millimeters) and probably has at least that much range. Here’s my starting point. Could I improve it? Do we have enough data that it probably won’t matter? \\[ \\begin{align} y &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 10) \\\\ \\end{align} \\] Something is new here – what is this Cauchy\\(^{+}\\) distribution? The Cauchy distribution is a thick-tailed probability distribution closely related to the Student t distribution. The Cauchy\\(^{+}\\) distribution is the half-Cauchy distribution – this means it is only defined for all positive real values (including 0.) You can think of this distribution as a weakly regularizing prior for standard deviations – most of the mass is concentrated towards low values and 0, but the heavy tail means there is non-zero probably of large values for \\(\\sigma\\). The half-Cauchy distribution has two parameters: location and scale. The location is the middle of the full distribution and the scale describes the “width” of heaviness of the tails. For the half-Cauchy distribution the location parameter is mostly a formality as it defines the “backstop” of the distribution – that it is defined for values of 0 or greater. Here is a quick visual of the Cauchy distribution’s behavior as you vary the scale. The half-Cauchy is just this distribution reflected about 0. df &lt;- tibble(x = seq(from = -20, to = 20, by = 0.1)) %&gt;% mutate(scale_1 = dcauchy(x, location = 0, scale = 1), scale_5 = dcauchy(x, location = 0, scale = 5), scale_10 = dcauchy(x, location = 0, scale = 10), scale_20 = dcauchy(x, location = 0, scale = 20), scale_50 = dcauchy(x, location = 0, scale = 50)) df %&gt;% gather(key = &#39;key&#39;, value = &#39;value&#39;, -x) %&gt;% separate(key, c(&#39;type&#39;, &#39;scale&#39;)) %&gt;% mutate(scale = factor(scale, levels = sort(order(scale)))) %&gt;% ggplot(aes(x = x, y = value, colour = scale)) + geom_line(size = 2) + scale_y_continuous(NULL, breaks = NULL) + scale_colour_viridis(discrete = TRUE) + labs(colour = &#39;Scale&#39;) + NULL As you can see, at scales of 20 or greater the Cauchy really begins to resemble the uniform distribution but with a bump in density around 0. Let’s implement our intercept-only model in brms. We are going to need to ignore species that have missing data for density_log – something brms() can do automatically. Is getting rid of all this data ideal? Let’s assume it has no effect on our results for now, but if you are interested in learning more about handling missing values in our models, look up data imputation – this is an advanced topic we will not be covering in these introductory lessons. So, first we need to filter our data to just those observations with population density values. Then we can fit our model. pantheria &lt;- pantheria %&gt;% drop_na(density_log) m_1 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 ## Data: . (Number of observations: 956) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.86 0.10 3.67 4.05 2969 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 2.97 0.07 2.83 3.10 2701 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As with any model, we should see how well it describes our data – maybe this simple model does a good enough job? Let’s compare our observed distribution of population densities versus our posterior predictive distribution. pantheria %&gt;% add_predicted_draws(model = m_1, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted.&#39;) + NULL While we might be describing the overall mean and standard deviation of our data, I do not think our simple model is capable of capturing the a lot of the complexity in our data. Our data appears to be multimodal and has a very different spread, especially on the right-hand side. We are going to have to include more information we if want to better describe our data. 4.3 A single continuous predictor Just like in our previous lesson, to improve our model we’re going to add a single predictor. What’s new to this lesson is that that predictor is a continuous variable: average individual mass in log grams or mass_log. In linear regression, our predictors tend to describe change in mean \\(y\\) has a function of an intercept, one or more regression coefficients, and one or more predictor. To do this, we need to define \\(\\mu\\) as a function of our predictor. Do you remember from last lesson how we did this? Try writing out a model definition by hand. First, let’s define \\(x\\) as mass_log. Also, let’s define two more variables: let \\(\\alpha\\) be the intercept of our regression, and let \\(\\beta\\) be the regression coefficient for \\(x\\). Given this new information, here is how we can write out our regression model. \\[ \\begin{align} y_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta x_{i} \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(-1, 5) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 5) \\\\ \\end{align} \\] Are the choice of priors reasonable? Take a closer look at the prior for \\(\\beta\\) – this is a weakly informative prior. I’m guessing that the slope is probably negative, but allow for the possibility of a 0 or positive slope – albeit less than that of a negative slope. Is this justified? Think of the physical definition of the variables – what would you guess the relationship between body size and population density to be? You might notice this model is functionally identical to the model from our previous lesson where we had only a single binary predictor. The difference is all on the data end, and not the model, as \\(x\\) can take on any value and not just 0 and 1. How do we interpret all of these parameters? Our previous lesson gave us all the information we needed to describe each parameter, but I will reiterate them here because this is really important. If we don’t know what our parameters precisely mean, we cannot interpret them. \\(\\mu\\) average value of \\(y\\) \\(\\sigma\\) standard deviation of \\(y\\) \\(\\alpha\\) intercept, average value of \\(y\\) when \\(x\\) = 0 \\(\\beta\\) slope, expected change in \\(y\\) per unit change in \\(x\\) Let’s implement this model in brms. Like before, we are going to ignore species that have missing data for either density_log or mass_log – brms() can do this for us automatically, but let’s do it by hand here again. So, our first set is filter down the pantheria tibble again and then fit our new model. We’ve already dropped all observations missing density values, so we just need to do the same for mass values. Conveniently, there is no harm in checking for missing density_log values again so that everything is clear. pantheria &lt;- pantheria %&gt;% drop_na(density_log, mass_log) m_2 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(-1, 10), class = b), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log ## Data: . (Number of observations: 947) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 8.94 0.16 8.63 9.25 4248 1.00 ## mass_log -0.74 0.02 -0.78 -0.70 4051 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.95 0.04 1.86 2.03 4715 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3.1 Aside: Centering The intercept of a linear regression model is normally interpreted as the average value of \\(y\\) when all predictors equal 0. A consequence of this definition means that the value of the intercept is frequently uninterpretable without also studying the regression coefficients. This is also the reason that we commonly need very weak priors for intercepts. A trick for improving our interpretation of the intercept \\(\\alpha\\) is centering our (continuous) predictors. Centering is the procedure of subtracting the mean of a variable from each value. Namely: pantheria &lt;- pantheria %&gt;% mutate(mass_log_center = mass_log - mean(mass_log)) \\(\\alpha\\) is still the expected value of the outcome variable when the predictor is equal to zero. But now the mean value of the predictor is also zero. So the intercept now means: the expected value of the outcome, when the predictor is at its average value. This makes interpreting the intercept a lot easier. To illustrate this, let’s refit the model with the newly centered data. m_3 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_center), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(1, 5), class = b), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_center ## Data: . (Number of observations: 947) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.85 0.06 3.72 3.97 4372 1.00 ## mass_log_center -0.74 0.02 -0.78 -0.70 3917 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.95 0.04 1.86 2.03 3656 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Can you explain why centering changes the value of \\(\\alpha\\) but not \\(\\beta\\)? Centering will not change our models posterior predictive performance, but really improves the interpretability of our model parameters. Centering can also be beneficial for estimating parameter values by decreasing posterior correlation among the parameters. I recommend always centering your (continuous) predictors. 4.3.2 Checking model fit Now let’s see how much adding this predictor improves our ability to describe \\(y\\). We can also visualize our data as a scatter plot with the linear predictor overlain to demonstrate our model estimates of mean population density as a function of species mass. pantheria %&gt;% add_fitted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + geom_line(mapping = aes(y = .value, group = .draw), alpha = 1 / 20, colour = &#39;blue&#39;) + geom_point(data = pantheria, size = 2) + scale_fill_brewer() The previous plot only covers our model’s estimates for mean population density but this is not all our model is telling us. As with our earlier posterior predictive comparisons from the intercept-only model, we can use the full posterior predictive distribution to compare our observed data to 100 datasets drawn from the posterior predictive distribution. Because the posterior predictive distribution also takes into account the estimated scale of our data (\\(\\sigma\\)) and thus estimates individual values of \\(y\\) and not just the expected value of \\(y\\), these types of comparisons give us a fuller appreciation of how well our model is or is not representing out data. pantheria %&gt;% data_grid(mass_log_center = seq_range(mass_log_center, n = 1000)) %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), colour = &#39;blue&#39;) + geom_point(data = pantheria, size = 2) + scale_fill_brewer() + NULL We can also do explicit posterior predictive tests to see how well our model captures specific parts of our data such as the overall density, the median, or differences between unmodeled classes. For our first posterior predictive test, let’s do a comparison between the density of our data, \\(y\\), and the densities of 100 simulated datasets drawn from our posterior predictive distribution, \\(y^{\\tilde}\\). pantheria %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted.&#39;) + NULL Let’s then see how well our model reproduces the median population density even though our model is defined for the mean of population density. pantheria_summary &lt;- pantheria %&gt;% dplyr::summarize(median = median(density_log)) %&gt;% pull() pantheria_summary_ppc &lt;- pantheria %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw) %&gt;% dplyr::summarize(density_median_ppc = median(.prediction)) med_ppc &lt;- pantheria_summary_ppc %&gt;% dplyr::summarize(per = sum(density_median_ppc &gt; pantheria_summary) / n()) %&gt;% pull() pantheria_summary_ppc %&gt;% ggplot(aes(x = density_median_ppc)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(xintercept = pantheria_summary, size = 2) + labs(subtitle = paste(med_ppc, &#39;% of estimates greater than observed&#39;), x = &#39;Median population density&#39;, y = &#39;&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s see if our model can reproduce the differences in population density between trophic levels even though that information was not included in our model. This requires us dropping a bit more of our data because trophic level is not assigned for as many species as density or body mass. pan_cut &lt;- pantheria %&gt;% drop_na(density_log, mass_log_center, trophic_level) pan_trophic_summary &lt;- pan_cut %&gt;% group_by(trophic_level) %&gt;% dplyr::summarize(median = median(density_log)) pan_trophic_summary_ppc &lt;- pan_cut %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw, trophic_level) %&gt;% dplyr::summarize(density_median_ppc = median(.prediction)) pan_trophic_summary_ppc %&gt;% ggplot(aes(x = density_median_ppc)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(data = pan_trophic_summary, mapping = aes(xintercept = median), size = 2) + facet_wrap(~ trophic_level) + labs(x = &#39;Median population density&#39;, y = &#39;&#39;) + NULL ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Or we can even look at how well our posterior predictive distribution does across the different trophic levels, even though those are not encoded in the model. pan_cut &lt;- pantheria %&gt;% drop_na(density_log, mass_log_center, trophic_level) pan_cut %&gt;% data_grid(mass_log_center = seq_range(mass_log_center, n = 1000), trophic_level) %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), colour = &#39;blue&#39;) + geom_point(data = pan_cut, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL While our model does an okay job at predicting median population density of omnivores and herbivores, it is very bad at estimating the population density of carnivores. Given all of the above plots, in particular the failure of our model to capture the differences between the (unmodeled) trophic levels, it appears that while this model does slightly better job at approximating our data than our earlier model it still fails are adequately representing our data. Even though we did not model the differences between the trophic groups, by seeing if our model captures these differences we get a better idea about how we might improve our model. In this case, because we know that while there are differences between trophic groups, these differences are not captured by the model. If we want to make sure we are describing the different sources of variation, we might want to include the tropic levels as additional predictors on top of body size. But what is this about adding more predictors? What does that mean? That is the subject of our next lesson. 4.4 Summary In this lesson we introduced continuous predictors in linear regression. We also covered a lot of examples of how to inspect your model’s adequacy at describing our data. This process involved checking our model’s ability to represent unmodeled variation in the data that we know about but that our model does not. Our next step will be including these previously unmodeled factors in a new expanded model. In our next lesson we will cover multivariate models, or models with more than one predictor (e.g body mass AND trophic level). "],
["multiple-predictors-and-interactions-in-linear-regression.html", "5 Multiple predictors and interactions in linear regression 5.1 Objectives 5.2 More than one predictor 5.3 Interactions", " 5 Multiple predictors and interactions in linear regression 5.1 Objectives Interpret regression models with a categorical predictor that has more than two levels. Learn to interpret regression models with more than one predictor. Introduce statistical interactions in regression models. Cover strategies for visualizing multivariate regression models. library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, modelr, forcats, knitr, kableExtra) theme_set(theme_bw()) We’re going to be using the PanTHERIA dataset like we did in the previous lesson. Just like last time, here is a quick clean up of the dataset before we do anything “principled.” pantheria &lt;- read_tsv(here(&#39;data&#39;, &#39;PanTHERIA_1-0_WR05_Aug2008.txt&#39;), na = &#39;-999.00&#39;) %&gt;% clean_names() %&gt;% mutate(mass_log = log(x5_1_adult_body_mass_g), range_group_log = log(x22_1_home_range_km2), range_indiv_log = log(x22_2_home_range_indiv_km2), density_log = log(x21_1_population_density_n_km2), activity_cycle = case_when(x1_1_activity_cycle == 1 ~ &#39;nocturnal&#39;, x1_1_activity_cycle == 2 ~ &#39;mixed&#39;, x1_1_activity_cycle == 3 ~ &#39;diurnal&#39;), trophic_level = case_when(x6_2_trophic_level == 1 ~ &#39;herbivore&#39;, x6_2_trophic_level == 2 ~ &#39;omnivore&#39;, x6_2_trophic_level == 3 ~ &#39;carnivore&#39;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## MSW05_Order = col_character(), ## MSW05_Family = col_character(), ## MSW05_Genus = col_character(), ## MSW05_Species = col_character(), ## MSW05_Binomial = col_character(), ## References = col_character() ## ) ## See spec(...) for full column specifications. 5.2 More than one predictor So far all of the linear regression models we have encountered have either been intercept-only or with a single predictor. For this lesson we’re going to continue our focus on three variables from the PanTHERIA dataset: population density, body size, and trophic level. Population density, measured as the number of individuals per square-kilometer, is our variable of interest – we want to define a model which describes how population density varies between mammal species. We have previously investigated body size as a predictor of population density, but the posterior predictive analysis of our model of population density with only body mass as a predictor demonstrates that this model does not adequately describe the data. We were able to do this by asking if our model is able to describe differences in the data that we know about but that our model does not (e.g. trophic level). 5.2.1 Categorical predictor Trophic level is a categorical predictor with three levels. The previous categorical predictor we dealt with (Bivalve versus Brachiopod) was binary which made it very easy to interpret. In that lesson I briefly introduced the idea of dummy coding and demonstrated how that would work with a three level categorical variable. I’m going to reiterate and expand on that demonstration here. brm, and R in general, will automatically translate categorical predictors like our trophic_level variable into what’s called dummy coding. When we dummy code a categorical variable what we are doing is turning one variable with \\(k\\) states into \\(k - 1\\) binary variables. One state of the categorical variable is considered the “baseline” or the default condition for any observation. The other \\(k - 1\\) binary variables then describe if the observation is different from the default – these are called contrasts. The standard behavior for this in R is that the first state, alphabetically, is made the baseline. Here is this in action: pantheria %&gt;% drop_na(trophic_level) %&gt;% model_matrix(~ trophic_level) %&gt;% slice(1:10) %&gt;% kable() %&gt;% kableExtra::kable_styling() (Intercept) trophic_levelherbivore trophic_levelomnivore 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 The model_matrix() function returns a tibble with \\(k\\) columns. Take a closer look at the column names. First, focus on the second and third columns. Each title is the variable name (trophic_level) combined with one of the levels (e.g. trophic_levelherbivore). One of the factor’s levels, carnivore, is not named as one of the columns, instead that level is subsumed in the (Intercept) column. The default state of any observation is this “carnivore.” A 0 in the trophic_levelherbivore or trophic_levelomnivore columns means that the observation is not an herbivore or omnivore, respectively. If there is a 1 in the herbivore or omnivore column that means that observation is an herbivore or omnivore and not a carnivore – hence why the \\(k - 1\\) binary variables are called contrasts (with respect to the default condition/intercept). Importantly, an observation cannot/should not have a 1 in more than one of the \\(k - 1\\) binary variables associated with a single categorical predictor. Let’s see what happens when we suppress the intercept using R’s formula syntax. pantheria %&gt;% drop_na(trophic_level) %&gt;% model_matrix(~ -1 + trophic_level) %&gt;% slice(1:10) %&gt;% kable() %&gt;% kableExtra::kable_styling() trophic_levelcarnivore trophic_levelherbivore trophic_levelomnivore 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 By suppressing the intercept in our model matrix, we’ve dramatically changed the first column of our tibble. Instead of being labeled (Intercept) and being a column of only 1s, we have a column named trophic_levelcarnivora which is a binary variable indicating if an observation is a carnivore or not. I do not recommend suppressing the intercept when developing a model, especially if you are generating your contrasts independent of the model formula (e.g. when using brm()), and extra especially if you have more predictors in your model than just the categorical variable being coded out. If you are not careful, you might end up with the equivalent of two intercepts which causes a model to be completely undefined and useless. As as been discussed before, R tacitly converts our categorical variables (e.g. variables where our observations are character data) into dummy coding. While this is convenient, it can lead to confusion if you aren’t super familiar with categorical variables. For example, I once had a colleague ask, “How are there 5 parameters when we only have 1 predictor in our linear regression (they expected three)?” Something I like to do is make the most common or most general class be the intercept. This adds a bit of logic to the contrasts, as they are now “in contrast” to the most common state. The most common class in trophic_level is “herbivore”. To change which class becomes the intercept, we need to manipulate the R structure of trophic_level. I’m choosing to do this using functions from the forcats package. pantheria %&gt;% drop_na(trophic_level) %&gt;% mutate(trophic_level = fct_infreq(trophic_level)) %&gt;% # reorder by frequency model_matrix(~ trophic_level) %&gt;% slice(1:10) %&gt;% kable() %&gt;% kableExtra::kable_styling() (Intercept) trophic_levelomnivore trophic_levelcarnivore 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 By controlling which of the classes from our categorical variable is considered the intercept, we can make inference easier. Carnivores are relatively rare in our dataset, so having them as the “default” condition for our model is a bit confusing – most of our observations would be “exceptions” or “in contrast to” our default. By having herbivores act as are default, we inherently are describing more parts of our data before looking at the contrasts. There are many other strategies for encoding categorical variables, but dummy coding is by far the most common – it helps that it is the R default. 5.2.2 Defining our model Now that we have refreshed our understanding of how categorical covariates are handled in a regression model, let’s write out a model describing how density_log varies as a linear function of our trophic_level categorical predictor and the mass_log continuous covariate. As always, let’s write out our model in statistical notation before implementing it in brms. I’m going to be a bit more formal in defining our statistical model than before because it is good practice and because this model is more complicated than the ones we have seen before. We have \\(N\\) total observations (species) in our dataset. We can define our response variable, density_log, as \\(y_{i}\\) where \\(i\\) index which species that measure is from and \\(i = 1, 2, \\ldots, N\\). We have our two predictor variables mass_log and trophic_level which are continuous and categorical, respectively. Our first predictor is the continuous mass_log of each observation – let’s call this variable \\(x_{i}\\) where \\(i = 1, 2, \\ldots, N\\). Our second variable is the categorical trophic_level which has three classes: carnivore, herbivore, omnivore. As discussed before, we are going to rewrite this variable as a set of \\(k - 1\\) contrasts where \\(k\\) is the number of levels in the variable (i.e. three). Let’s name herbivore to be the baseline, with carnivore and omnivore being the contrasts. We can define these two binary predictors as \\(c_{i}\\) and \\(o_{i}\\), respectively. This covers all of our data. Now lets define the parameters that describe our the bits of our data are related. We need to define a regression model describing how the expected value of \\(y\\) varies as a function of \\(x\\), \\(c\\), and \\(o\\). Let’s assume a Normal distribution is a suitable descriptor of population density, the core assumption of linear regression. We can the define mean and standard deviation of this distribution as \\(\\mu_{i}\\) and \\(\\sigma\\). \\(\\mu\\) is indexed by observation \\(i\\) where \\(i = 1, 2, \\ldots, N\\) because we are going to define it as a linear function of our predictors which vary for each of our \\(N\\) observations. We then can define the linear function describing \\(mu_{i}\\) as an additive relationship of each covariate multiplied by its own regression coefficient plus an intercept. Let’s identify the intercept as \\(\\alpha\\) and the three coefficients as \\(\\beta_{1}\\), \\(\\beta_{2}\\), and \\(\\beta_{3}\\). Now that we’ve defined all of our data terms and parameters, we need to define the remaining priors for the regression coefficients and standard deviation parameter. I’m going to stick with what the priors we used from our previous lesson, with a slightly informative prior for the effect of body mass and general weakly informative priors for the other regression coefficients. \\[ \\begin{align} y_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta_{1} m_{i} + \\beta_{2} o_{i} + \\beta_{3} h_{i} \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta_{1} &amp;\\sim \\text{Normal}(-1, 5) \\\\ \\beta_{2} &amp;\\sim \\text{Normal}(0, 5) \\\\ \\beta_{3} &amp;\\sim \\text{Normal}(0, 5) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 5) \\\\ \\end{align} \\] 5.2.3 Fitting model in brms Now that we’ve been able to write out our complete statistical model, we can not implement it in brms. Our first task limiting our data to not include observations with missing values for our variables of interest. After that, we need to center our continuous predictor so that our intercept has a clean interpretation. After our data is prepared for analysis, we can then write our brm() call. pantheria_fit &lt;- pantheria %&gt;% drop_na(density_log, mass_log, trophic_level) %&gt;% mutate(mass_log_center = mass_log - mean(mass_log), trophic_level = fct_infreq(trophic_level)) m_1 &lt;- pantheria_fit %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_center + trophic_level), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 5), class = b), prior(normal(-1, 5), class = b, coef = mass_log_center), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_center + trophic_level ## Data: . (Number of observations: 746) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept 3.15 0.10 2.95 3.35 4610 ## mass_log_center -0.82 0.02 -0.87 -0.78 4434 ## trophic_levelherbivore 1.30 0.15 1.00 1.60 4493 ## trophic_levelcarnivore -1.31 0.21 -1.71 -0.92 4422 ## Rhat ## Intercept 1.00 ## mass_log_center 1.00 ## trophic_levelherbivore 1.00 ## trophic_levelcarnivore 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.80 0.05 1.72 1.90 4410 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 5.2.4 Aside: Standardizing Remember, regression coefficients (e.g. \\(\\beta_{1}\\)) are interpreted as the expected change in \\(y\\) per unit change in the predictor (e.g. \\(m\\)). If our predictors are not on the same scale, it is “unfair” to compare them directly. For example, how can we compare the effect size of body mass versus being an herbivore on population density? While the regression coeffcient for being an herbivore (instead of an omnivore) has a greater magnitude than the coefficient for mass, but this is misleading because a complete difference in category to a 1-km\\(^2\\) change in geographic range. A natural solution to this problem is to standardize our predictors so that they are all on the same scale (having the same standard deviation). You might have heard of this as z-scores or something else – we will not be using this langauge. To standardize our data, Gelman and Hill recommend scaling or predictors by dividing by 2 standard devations so that “a 1-unit change in the rescaled predcitor corresponds to a change from 1 standard deviation blow the mean, to 1 standard devaition above.” But why 2 standard deviations and not 1? By dividing by 2 standard deviations we gain comparability between our continuous covariates and the binary predictors. For example, consider a simple binary variable \\(x\\) where 0 and 1 occur with equal probability. The standard deviation of this variable is then \\(\\sqrt{0.5 \\cdot 0.5} = 0.5\\), which means the standardized variable, \\((x - \\mu_{x}) / (2 \\sigma_{x})\\), takes on the values \\(\\pm 0.5\\), and its coefficient reflects comparisons between \\(x = 0\\) and \\(x = 1\\). In contrast, dividing by 1 standard deviation means that the scaled variable takes on values \\(\\pm 1\\) and its coefficients correspond to half the difference between the two possible values of \\(x\\). Gelman and Hill also state that in complicated regression models with lots of predictors, we can leave our binary inputs as is and just transform our continuous predictors. In that case, the 2 standard deviation rule gives rough comparability in the coeffcieints. Our goal should be able to have meaningful interpretations for as many parameters as possible, especially regression coefficients. To ensure this, we should center and scale all continuous predictor variables before analysis. If these transformations aren’t done, than we can have difficulty interpreting our models intercept and can incorrectly and unfairly compare the estimates of our regression coefficients. pantheria_fit2 &lt;- pantheria %&gt;% drop_na(density_log, mass_log, trophic_level) %&gt;% mutate(mass_log_stan = (mass_log - mean(mass_log)) / (2 * sd(mass_log)), trophic_level = fct_infreq(trophic_level)) m_2 &lt;- pantheria_fit2 %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_stan + trophic_level), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 5), class = b), prior(normal(-1, 5), class = b, coef = mass_log_stan), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_stan + trophic_level ## Data: . (Number of observations: 746) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept 3.15 0.10 2.95 3.35 3486 ## mass_log_stan -5.00 0.14 -5.28 -4.73 4164 ## trophic_levelherbivore 1.29 0.15 0.99 1.59 3460 ## trophic_levelcarnivore -1.32 0.21 -1.73 -0.93 4052 ## Rhat ## Intercept 1.00 ## mass_log_stan 1.00 ## trophic_levelherbivore 1.00 ## trophic_levelcarnivore 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.80 0.05 1.71 1.90 4443 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 5.2.5 Checking model fit So we’ve been able to fit our model and have (more) interpretable parameters. Our work with this model, however, is not done. As with our previous lessons, we now need to get an idea for how well our model represents our data – can we “trust” inferences based on its parameter estimates? pantheria_fit2 %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_stan, y = density_log)) + geom_line(mapping = aes(y = .value, group = .draw), alpha = 1 / 20, colour = &#39;blue&#39;) + geom_point(data = pantheria_fit2, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL pantheria_fit2 %&gt;% data_grid(mass_log_stan = seq_range(mass_log_stan, n = 1000), trophic_level) %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_stan, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), colour = &#39;blue&#39;) + geom_point(data = pantheria_fit2, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL pantheria_fit2 %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria_fit2, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted&#39;) + NULL pantheria_fit2 %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria_fit2, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted&#39;) + facet_wrap(~ trophic_level) + NULL pan_trophic_summary &lt;- pantheria_fit2 %&gt;% group_by(trophic_level) %&gt;% dplyr::summarize(median = median(density_log)) pantheria_summary_ppc &lt;- pantheria_fit2 %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw, trophic_level) %&gt;% dplyr::summarize(density_median_ppc = median(.prediction)) pantheria_summary_ppc %&gt;% ggplot(aes(x = density_median_ppc)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(data = pan_trophic_summary, mapping = aes(xintercept = median), size = 2) + facet_wrap(~ trophic_level) + labs(x = &#39;Median population density&#39;, y = &#39;&#39;) + NULL ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.3 Interactions "]
]
