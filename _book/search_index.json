[
["index.html", "Analytical Paleobiology Preface", " Analytical Paleobiology Peter D Smits 2019-02-08 Preface This This book uses the tidyverse collection of R packages with a particular emphasis on dplyr, ggplot2, and purrr. Other tidyverse packages are used as necessary (e.g. modelr). I emphasize Bayesian data analysis approaches throughout this text. To this end, model inference is done using the brms package which is a flexible tool for implementing Stan-based models in R. Manipulation of those results, and some aspects of visualization, is done using the tidybayes package. I attempt to stick to the tidyverse style guide "],
["managing-and-processing-paleobiology-database-data.html", "1 Managing and Processing Paleobiology Database data 1.1 Objectives 1.2 Reading 1.3 Introduction 1.4 Getting 1.5 Processing 1.6 Sharing 1.7 Summary", " 1 Managing and Processing Paleobiology Database data 1.1 Objectives Introduce the data stored in the Paleobiology Database. Learn how to programatically download PBDB data. Introduce tidy data and some good practices when managing data. Learn how to make PBDB cleaner and tidier library(tidyverse) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.0 ## ✔ tibble 2.0.1 ✔ dplyr 0.7.8 ## ✔ tidyr 0.8.2 ✔ stringr 1.3.1 ## ✔ readr 1.3.1 ✔ forcats 0.3.0 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(janitor) library(knitr) library(kableExtra) 1.2 Reading The following material are recommended pre-readings before starting this tutorial. You do not have to read all of them, just pick at least one. Wickham 2014 “Tidy Data”. Wilson et al. 2017 “Good enough practices in scientific computing” PLoS Computational Biology. Verde Arregotia et al. 2018 “Good practices for sharing analysis-ready data in mammalogy and biodiversity research” Hystrix, the Italian Journal of Mammalogy. Bryan “Project oriented workflow” tidyverse.org. Bryan “Zen and aRt of Workflow Maintenance” talk. Bryan “Code Smells and Feels” talk. Bryan 2017 “Excuse me, do you have a moment to talk about version control?” PeerJ. 1.3 Introduction Any project you work on as a scientist has multiple parts: data, documentation, reports, code, etc. Managing and keeping track of these parts is not a simple task. Today we will discuss a small part of this process: data wrangling and sharing using the tidyverse set of packages and syntax. This lesson is in three parts: getting data, processing data, and sharing data. This tutorial assumes a fair amount of familiarity with the tidyverse, in particular dplyr. For a tutorial on using dplyr and purrr I recommend - R for Data Science - https://dplyr.tidyverse.org/articles/dplyr.html. 1.4 Getting One of the greatest resources in paleobiology is the aptly named Paleobiology Database, or PBDB for short. The PBDB is a freely available internet repository of fossil occurrences, collections, taxonomic opinions, and lots of other information. The standard way to access information in the PBDB is through the class Download Generator. In the past it was very difficult to replicate previous PBDB downloads because of how difficult they were to communicate – with so many manual options, it is hard to easily transmit this information to another author. The modern Download Generator (at time of this writing) has one major improvement for increasing the reproducibility of downloads – a URL. Every option updates a URL that calls our data from the PBDB. Play around with the download options and see how the URL changes. That URL is a call to the PBDB’s API, which is the data service for interfacing with the material stored in the underlying database. This means we can share the URL along with our study so that other researchers can make the same data call. The API documentation leaves something to be desired, but as you interact with the docs and start writing your own API calls, it should become easier. A fossil occurrence is the core data type of the PBDB and probably the most important data type in all of paleobiology – the unique recording of an organism at a particular location in time and space. Normally we want a list of fossil occurrences that correspond to our study system or period of time. For data output from the PBDB for occurrences, each row is an observation and each column is a property of that fossil or metadata corresponding to its collection and identification. We are going to focus on downloading information about fossil occurrences. Here are a few example URLs which make calls to the PBDB API. Use the API documentation to discern and describe the differences between the different calls. https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;show=all https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;taxon_status=valid&amp;show=all https://paleobiodb.org/data1.2/occs/list.txt?base_name=Cetacea&amp;interval=Miocene&amp;idreso=genus&amp;show=all https://paleobiodb.org/data1.2/occs/taxa.txt?base_name=Cetacea&amp;interval=Miocene&amp;show=attr The best part of using a URL based call is that we can embed them in our R scripts. Here is a simple example (note I’m suppressing warnings here, so don’t get scared when you see a lot of them): url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) The carnivora object represents the basic PBDB list response with all information for every observation as a data.frame. Because the URL points directly to a CSV (or JSON) file, we never have to actually save a copy of our data to our local machine and it instead just lives in memory – though you might want to download and store the data every so often (e.g. write_csv()). Also, by using a direct API call to the PBDB instead of relying on a downloaded file our analyses can instaly be updated when new data is added to the PBDB. I find tibbles easier to process than data.frame-s, so my scripts tend to look like this: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() If you play around with the carnivora object you’ll notice it has TONS of columns – 118! Each of these columns records some bit of information about that fossil – taxonomic identity, location, source, enterer, etc. You can check the API documentation for a description of each column. Frustrating many of these fields might be empty or inconsistently entered – I’m looking at you lithology1 and environment. Additionally, a lot of our fossils might not be identified to the species or even genus level, or are not identified with confidence. This serves as an important point about the PBDB: the data isn’t perfect. This means that the next step of our analysis is “cleaning” or “tidying” our data until we can actually analyze it! 1.5 Processing This section focuses on using the dplyr package from the tidyverse to process our data until it is actually usable for our purposes. Luckily, our data is already considered “tidy” because each row is an observation and each column is a variable. This does not mean our data is ready for analysis yet. Our job as analysts is to then process and filter our data till it is ready to be analyzed. Is every observation up to snuff? Or are there errors encoded in our data? Example filters we might consider - identified precisely – no ambiguous or imperfect “calls” - identified to genus or better - paleocoordinates (latitude AND longitude) - body fossils - collected after a certain date We might also want to standardize the form of our column names. Capital letters, spaces, and punctuation are all really frustrating to code around. Luckily, these operations are very easy to do with with dplyr and the janitor package. For example, let’s filter out the imprecise fossils and those not identified to at least the genus level, and have paleocoordinates. Let’s also make sure all the variable names have the same logic (they are all already fine, but this is a good habit to get into!) carnivora_filter &lt;- carnivora %&gt;% janitor::clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) This new tibble, carnivora_filter, is a subset of the ordinal data that should follow the rules we’ve laid out in the URL-based API call and the few lines of R code. If we gave this document to someone else, they could reproduce our dataset. The accepted_* variables in PBDB data correspond to the accepted, or best, identification of a fossil. Differences between identified_* and accepted_* variables are commonly due to re-identification or changes in taxonomy. While this is really convenient on its face, sometimes the accepted species names assumes too much confidence in the identification. For example, let’s take a close look at a few records. carnivora_table &lt;- carnivora_filter %&gt;% select(identified_name, identified_rank, accepted_name, accepted_rank) %&gt;% slice(50:60) knitr::kable(carnivora_table) %&gt;% kableExtra::kable_styling() identified_name identified_rank accepted_name accepted_rank Phlaocyon minor species Phlaocyon minor species Desmocyon thomsoni species Desmocyon thomsoni species Promartes lepidus species Promartes lepidus species Promartes lepidus species Promartes lepidus species Daphoenodon notionastes species Daphoenodon notionastes species Daphoenodon n. sp. notionastes species Daphoenodon notionastes species Phlaocyon achoros species Phlaocyon achoros species Cynarctoides lemur species Cynarctoides lemur species Cormocyon cf. copei species Cormocyon copei species Megalictis ? sp. genus Megalictis genus Phlaocyon ? cf. annectens species Phlaocyon annectens species In most cases there is very good correspondence between the identified name and the accepted name, but not always. For example, the ninth of this table corresponds to a fossil identified as “Cormocyon cv. copei” but is given the accepted name of “Cormocyon copei” – an identification that is arguably overconfident! But does it matter? That’s up to you and your research, but let’s assume it does for now. How do we resolve this and downgrade these overconfident identifications? The simplest way might be to downgrade any identified names that include punctuation and non-character symbols to just there genus. After all, “cf.”, “sp.” and “n. sp.” all involve punctuation. But how do we deal with text information? Turns out there is a whole special language for dealing with text: regular expressions. RegEx are sequences of characters that help us match specific patterns in text. In this example, I’m using a RegEx to identify all cases where there is punctuation present in the identified name – I don’t not care where the punctuation is, just that there is punctuation. To do this, I’m going to be using functions from the stringr package which provide for easier interaction with text and regular expressions than the functions in base R. I always spend a lot of time on Google figuring our RegEx before I use them as they are not intuitive, so don’t worry too much about understanding regular expressions early on – I’m using some special, easy to understand, forms that will be generally useful to you. carnivora_clean &lt;- carnivora_filter %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) carnivora_clean %&gt;% select(identified_name, accepted_name, improve_name) %&gt;% slice(50:60) %&gt;% knitr::kable() %&gt;% kable_styling() identified_name accepted_name improve_name Phlaocyon minor Phlaocyon minor Phlaocyon minor Desmocyon thomsoni Desmocyon thomsoni Desmocyon thomsoni Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Daphoenodon notionastes Daphoenodon notionastes Daphoenodon notionastes Daphoenodon n. sp. notionastes Daphoenodon notionastes Daphoenodon Phlaocyon achoros Phlaocyon achoros Phlaocyon achoros Cynarctoides lemur Cynarctoides lemur Cynarctoides lemur Cormocyon cf. copei Cormocyon copei Cormocyon Megalictis ? sp. Megalictis Megalictis Phlaocyon ? cf. annectens Phlaocyon annectens Phlaocyon If we really wanted to be slick, we could combine all of the above into a single block. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) 1.5.1 Binning Fossil occurrences in the PBDB have temporal information such as geologic stage, or a rough numerical age (max_ma, min_ma). The numerical age of a fossil occurrence is bounded between a max and a min. The nature of the fossil record means that we do not have exact ages for anything, instead we have ranges. This uncertainty in age presents a lot of problems that we have to deal with in our analyses, especially if the temporal order of our fossils matters to our question! An extremely common way to overcome this uncertainty is to coarsen the resolution of our fossils by binning them – assigning similarly aged fossils to a shared temporal unit. Each temporal bin can be said to have a “width” – the length of time covered by that bin. In our example, we may want to track diversity over time. We are going to do this by counting the number of unique genera present in our time bins. To do this, we have to determine how many bins there are and to which bin each fossil belongs. The age of each fossil, however, is a range and not a single value. We could use the midpoint of this range to assign each fossil to a bin, but what if the age range of some fossils is much larger than our bin width? First, let’s take a look at the amount of uncertainty there is in the age estimates of our fossil occurrences. carnivora_clean %&gt;% mutate(age_range = abs(max_ma - min_ma)) %&gt;% ggplot(aes(x = age_range)) + geom_histogram() + labs(x = &#39;Age range (My)&#39;, y = &#39;Count&#39;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can see that a lot of our fossils have age uncertainty of 5 million years or less, and a few of them have large ranges of 10 million years or more. Fossils with age ranges greater than 10 million years are potentially suspect or at least are not high quality – certainly trying to assign them to a single 2 million year bin isn’t going to be ideal as it adds confidence where there is none. In your own analyses you might consider these situations on a case-by-case basis or try and find more information from other sources, but for purposes of this tutorial we will exclude those occurrences who’s age ranges are 10 million years or greater. We can dither over an “optimal” bin width for our data, but for the purpose of this exercise let’s just assign all our fossil occurrences to 2 million year bins. Because binning data is so common, I’ve written a function, bin_ages() to do this. I present it here and will use it to bin our data. The bin_ages() function follows that convention that the youngest bin is bin 1. I’ve written documentation for this function using roxygen formatting. Feel free to modify it or write your own. #&#39; Break time data up into bins #&#39; #&#39; Have fun with this. basic rules. greater than equal to base, less than top. #&#39; #&#39; @param x vector of ages #&#39; @param by bin width #&#39; @param age logical bin age returned, not number (default FALSE, return bin number) #&#39; @return vector of bin memberships #&#39; @author Peter D Smits &lt;peterdavidsmits@gmail.com&gt; bin_ages &lt;- function(x, by = NULL, number = NULL, age = FALSE) { if(is.null(by) &amp; is.null(number)) { return(&#39;no scheme given. specify either bin width or number of bins.&#39;) } if(!is.null(by) &amp; !is.null(number)) { return(&#39;too much information. specify either bin width OR number of bins, not both.&#39;) } # range to bin top &lt;- ceiling(max(x)) bot &lt;- floor(min(x)) # create bins if(!is.null(by)) { unt &lt;- seq(from = bot, to = top, by = by) } else if(!is.null(number)) { unt &lt;- seq(from = bot, to = top, length.out = number + 1) } # bin top and bottom unt1 &lt;- unt[-length(unt)] unt2 &lt;- unt[-1] # assign memberships uu &lt;- map2(unt1, unt2, ~ which(between(x, left = .x, right = .y))) # what if we want the &quot;age&quot; of the bin, not just number? if(age == TRUE) { unt_age &lt;- map2_dbl(unt1, unt2, ~ median(c(.x, .y))) } # create output vector y &lt;- x for(ii in seq(length(uu))) { if(age == FALSE) { y[uu[[ii]]] &lt;- ii } else if(age == TRUE) { y[uu[[ii]]] &lt;- unt_age[ii] } } y } Let’s use this function to bin our data. carnivora_bin &lt;- carnivora_clean %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2)) carnivora_bin %&gt;% summarize(bin_number = n_distinct(bin)) ## # A tibble: 1 x 1 ## bin_number ## &lt;int&gt; ## 1 9 Ok, so now we have a column bin that identifies the temporal bin that each occurrence belongs to. The quick summary at the bottom demonstrates that we have broken our data into 9 bins of equal length. The limit here is that our bins are identified by their number and not their “age”. Luckily, the age parameter of the bin_ages() function that changes the output from bin number to bin age. Here it is in use. carnivora_bin &lt;- carnivora_bin %&gt;% mutate(bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) # take a look carnivora_bin %&gt;% select(mid_ma, bin, bin_age) %&gt;% slice(1:10) %&gt;% knitr::kable(.) %&gt;% kableExtra::kable_styling() mid_ma bin bin_age 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.6040 2 8 14.8950 5 14 13.7890 5 14 13.7890 5 14 As before, we can combine all of these operations into a set of piped statements. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) ## Warning: Duplicated column names deduplicated: &#39;cc&#39; =&gt; &#39;cc_1&#39; [47] ## Parsed with column specification: ## cols( ## .default = col_character(), ## occurrence_no = col_double(), ## reid_no = col_double(), ## flags = col_logical(), ## collection_no = col_double(), ## identified_no = col_double(), ## accepted_attr = col_logical(), ## accepted_no = col_double(), ## max_ma = col_double(), ## min_ma = col_double(), ## ref_pubyr = col_double(), ## reference_no = col_double(), ## plant_organ = col_logical(), ## plant_organ2 = col_logical(), ## abund_value = col_double(), ## lng = col_double(), ## lat = col_double(), ## collection_subset = col_logical(), ## paleolng = col_double(), ## paleolat = col_double(), ## zone = col_logical() ## # ... with 25 more columns ## ) ## See spec(...) for full column specifications. ## Warning: 1044 parsing failures. ## row col expected actual file ## 1071 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1071 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1073 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## .... ............... .................. ...... ............................................................................................. ## See problems(...) for more details. 1.6 Sharing So far we have basically two data “files”: the raw data output from the PBDB via our URL-based API call, and the cleaned data we’ve crafted with a bit of code. Both of these datasets are extremely important and should be shared with the audience. There are two ways to share the raw data associated with our study: the URL-based API call, and a spreadsheet of the downloaded information. Because the information on the PBDB updates over time, an API call made today might not yield an identical dataset. Earlier I hailed this as fantastic, which it is, but it is also limiting – someone might not be able completely reproduce your analysis using just this information. The API call is useful for improving and expanding on previous analyses, but by itself is not enough to reproduce your analysis. You also need to share the raw data download so that your complete analysis, including filtering and cleaning, is reproducible. You probably also want to save a local copy of your filtered and cleaned dataset so you don’t have to re-run your cleaning scripts all the time. It also means you can separate your cleaning from the rest of your analysis. You’ll end up with multiple R scripts and multiple datasets – that’s good. For example, my projects tend to have multiple subfolders: R/, data/, and results/. In the R directory, I’ll have multiple scripts – one for loading and cleaning data, one for visualizing this cleaned data, and at least one for analyzing the cleaned data. I save the raw data to the data/ directory, and the cleaned data and figured in the results/ directory. Here is a quick example of what I mean without referencing subdirectories: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) carnivora_clean &lt;- carnivora %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) write_csv(carnivora, &#39;carnivora_raw.csv&#39;) write_csv(carnivora_clean, &#39;carnivora_clean.csv&#39;) 1.7 Summary In this lesson we introduced the PBDB API and how to include URL-based API calls in our R code. We also covered multiple aspects of cleaning PBDB data including species identifications and temporal binning. Finally, we briefly covered exporting our data, both raw and cleaned, so that our analyses are more reproducible. All of these steps are fundamental in any analysis and form a great deal of the work associated with any analysis problem. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 3. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 1.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["introduction-to-bayesian-data-analysis.html", "2 Introduction to Bayesian data analysis 2.1 Objectives 2.2 Preface 2.3 Learning from data 2.4 Terms and theory 2.5 Summary", " 2 Introduction to Bayesian data analysis Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed. – Gelman et al BDA3: 4. Bayesian data analysis is just a logical procedure for processing information. – McElreath Statistical Rethinking: 12. 2.1 Objectives Learn what a (Bayesian) model actually means. Introduce the logic behind Bayesian updating. Dip our toes into numerical methods using brms. Cover the basics of summarizing a posterior distribution using tidybayes. 2.2 Preface We’re going to be using the tidyverse for data wrangling, and brms for modeling. Discussion and activities are derived from Statistical Rethinking by Richard McElreath, Bayesian Data Analysis 3 by Gelman et al., and Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. I’m also using code examples from this rewriting of Rethinking. library(tidyverse) library(brms) ## Loading required package: Rcpp ## Loading &#39;brms&#39; package (version 2.7.0). Useful instructions ## can be found by typing help(&#39;brms&#39;). A more detailed introduction ## to the package is available through vignette(&#39;brms_overview&#39;). ## Run theme_set(theme_default()) to use the default bayesplot theme. library(tidybayes) ## NOTE: As of tidybayes version 1.0, several functions, arguments, and output column names ## have undergone significant name changes in order to adopt a unified naming scheme. ## See help(&#39;tidybayes-deprecated&#39;) for more information. 2.3 Learning from data Bayesian inference is a fancy way of counting and comparing possibilities. As we collect and analyze our data, we learn which possibilities are more plausible than others. The logical strategy is “When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible.” 2.3.1 Counting and plausibility We’re going to use a simple example from Statistical Rethinking to start our thinking about Bayesian analysis. Setup: We’ve got a bag with four marbles. Each marble can be either blue (B) or white (W). We don’t know the composition of the bag. Our goal is to figure out which of possible configuration is most plausible, given whatever evidence we learn about the bag. A sequence of three marbles have pulled from the bag, one at a time, returned to the bag, and then the bag is shaken before drawing another. We draw the sequence [B W B]. How many ways are there to produce this draw? Try first this possible bag composition: [B W W W]. Here is the full table of ways to produce [B W B] given each possible bag composition. conjecture ways to produce [B W B] [W W W W] 0 x 4 x 0 = 0 [B W W W] 1 x 3 x 1 = 3 [B B W W] 2 x 2 x 2 = 8 [B B B W] 3 x 1 x 3 = 9 [B B B B] 4 x 0 x 4 = 0 What happens when we draw another marble from the bag? We update the counts! How do we update the counts? We multiply the prior counts by the new count, with the old counts are acting as our prior counts. Here is an example how we would update our counts if we were to draw an additional [B]. conjecture ways to produce [B] previous counts new count [W W W W] 0 0 0 x 0 = 0 [B W W W] 1 3 3 x 1 = 3 [B B W W] 2 8 8 x 2 = 16 [B B B W] 3 9 9 x 3 = 27 [B B B B] 4 0 0 x 4 = 0 Logically, what we’ve just done can be expressed as plausibility of [B W W W] after seeing [B W B] \\(\\propto\\) ways [B W W W] can produce [B W B] X prior plausibility of [B W W W]. But these are just counts and plausibilities; we want probabilities! First, to make explaining ourselves simpler, let’s define \\(p\\) as the proportion of blue marbles in the bag and \\(D_{new}\\) as our data. We now have: plausibility of \\(p\\) after \\(D_{new}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility of \\(p\\). We want to standardize the plausibility so that the sum of the plausibilities for all conjectures sums to 1. To standardize, we add up all the products, one for each \\(p\\), then divide each product by the sum of the products. possible combinations \\(p\\) ways to produce data plausibility [W W W W] 0 0 0 [B W W W] 0.25 3 0.15 [B B W W] 0.5 8 0.40 [B B B W] 0.75 9 0.45 [B B B B] 1 0 0 This process is equivalent to plausibility of \\(p\\) after \\(D_{new}\\) = (ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility \\(p\\)) / sum of products Each part of the calculations we’ve done so far correspond directly to quantities in applied probability theory. The conjectured proportion of blue marbles, \\(p\\), is usually called a parameter. The relative number of ways that \\(p\\) can produce the data is usually called the likelihood. The prior plausibility of a specific \\(p\\) is called the prior probability. The updated plausibility of any specific \\(p\\) is called the posterior probability. 2.3.2 Building a model Bayesian inference is made easier by working with probabilities instead of counts, but this makes everything look a lot harder. Setup: We’ve a globe with land and water sections. We want to know how much of the globe is water. Our strategy is to throw the globe up, catch it, and note the surface under the right index finger. Let’s do this 9 times. d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;)) # then rephrase in terms of trials and count of water (d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;))) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 To get the logic machinery working, we need to make some assumptions. These assumptions constitute the our model. An ideal design loop for designing a Bayesian model has three steps Data story Describe aspects of the underlying reality and the sampling process. Translate this description into a formal probability model. Acts as a framework for interpretation, but still just a story. Helps with realizing additional questions that must be answered as hypotheses are frequently vague. Updating Bayesian models begin with a set of plausibilities assigned to each possibility (Prior). Update those plausibilities based on the data to give posterior plausibility. Evaluate Certainty is no guarantee that the model is good or accurate. Supervise and critique your model! Check model’s adequacy for some purpose, or in light of stuff we don’t know. 2.3.2.1 A data story How did the data come to be? Describe aspects of the underlying reality as well as the sampling process, sufficient enough for specifying an algorithm to simulate new data. Write out the data story for this activity. 2.3.2.2 Bayesian updating A Bayesian model begins with one set of plausibilities assigned to each possible result: the prior plausibilities. These values are updated in light of data to produce our posterior plausibilities. This process is called Bayesian updating. sequence_length &lt;- 50 # how many points to calculate prob for d %&gt;% expand(n_trials, # for each value of ... p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% left_join(d, by = &quot;n_trials&quot;) %&gt;% group_by(p_water) %&gt;% # lag is the *previous* value mutate(lagged_n_success = lag(n_success, k = 1), lagged_n_trials = lag(n_trials, k = 1)) %&gt;% ungroup() %&gt;% # if first time, flat prior. # otherwise use previous posterior as new prior. mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), strip = str_c(&quot;n = &quot;, n_trials), # which draw is it? likelihood = dbinom(x = n_success, size = n_trials, prob = p_water)) %&gt;% # calculate likelihood for current draw # normalize the prior and the likelihood, making them probabilities group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + facet_wrap(~strip, scales = &quot;free_y&quot;) Side note on sample size: Bayesian estimates are valid and interpretable at any sample size. This fact is very much in contrast to the folk-wisdom around a minimum number of samples needed that you’ll hear in non-Bayesian contexts. In non-Bayesian contests, statistical inference is justified by behaviour at large samples sizes, called asymptotic behaviour. The reason Bayesian estimates are always valid and interpretable comes down to the prior. If the prior is bad, the resulting posterior could be misleading. Regardless, all estimates in either context are based on assumptions. 2.3.2.3 Evaluate Our model is learning from a “small world” of data. If there are important differences between the model and reality, then there is no guarantee of “large world” performance. Certainty is no guarantee that the model is good. As data increases, our model will become increasingly sure of the proportion of water. This sureness happens even if the model is seriously misleading because our estimates are conditional on our model. What is happening is that, given a specific model, we can be sure that plausible values are within a narrow range. It is important that you supervise and critique your model, and not just assume it worked or is correct because your code did not return an error. Anything that is not included in our model might not affect our inference directly, but might affect it indirectly because of that unmodeled dependence. For example, we currently are assuming that the order the data was collected in doesn’t matter (exchangeable), but what if the order of the observations actually did matter? Check the model’s inferences in light of aspects of the data that you know but the model doesn’t know about. This part of data analysis is an inherently creative endeavor that is up to you (the analyst) and your scientific community. Robots can’t do this step for you. The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right and are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions. Instead, our objective should be to test the model’s adequacy for some purpose. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you know as a domain expert and compare it to your model; if there is a conflict you should update your model (likelihood and/or prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently scientific questions, not statistical questions. As I said earlier, robot’s can’t do this step for you. 2.4 Terms and theory Common notation \\(y\\) observed data. \\(\\tilde{y}\\) unobserved data. \\(X\\) explanatory variables, covariates, etc. \\(\\Theta\\) parameters of interest. \\(p(\\cdot|\\cdot)\\) conditional probability distribution. \\(p(\\cdot)\\) marginal probability distribution. \\(p(\\cdot,\\cdot)\\) joint probability distribution. \\(Pr(\\cdot)\\) probability of an event. Likelihood Specifies the plausibility of data mathematically. Maps each conjecture onto the relative number of ways the data could occur, given that possibility. Sometimes written \\(p(y | \\Theta)\\) or \\(L(\\Theta | y)\\). Parameters Adjustable inputs. 1+ quantities we want to know about. Represent the different conjectures for causes or explanations of the data Difference between data and parameters is fuzzy and exploitable in Bayesian analysis –&gt; advanced topic Prior Every parameter you are trying to estimate must be provided a prior. The “initial conditions” of the plausibility of each possibility. Which parameters values do we think are more plausible than others? Constrain parameters to reasonable ranges. Express any knowledge we have about that parameter before any data is observed. An engineering assumption; helps us learn from our data. Regularizing or weakly informative priors are conservative in that they tend to guard against inferring strong associations between variables –&gt; advanced topics. Sometimes written \\(p(\\Theta)\\). Posterior Logical consequence of likelihood, the set of parameters to estimate, and priors for each parameter. The relative plausibility of different parameter values, conditional on the data. Sometimes written \\(p(\\Theta | y)\\). 2.4.1 Bayes’ Theorem The logic defining the posterior distribution is called Bayes’ Theorem. The theorem itself is an intuitive result from probability theory. First, describe the model and data as a joint probability. \\[ \\begin{align} p(y, \\Theta) &amp;= p(\\Theta | y) p(y) \\\\ p(y, \\Theta) &amp;= p(y | \\Theta) p(\\Theta) \\\\ \\end{align} \\] Then set equal to each other and solve for \\(p(\\Theta | y)\\): \\[ \\begin{align} p(\\Theta | y) p(y) &amp;= p(y | \\Theta) p(\\Theta) \\\\ p(\\Theta | y) &amp;= \\frac{p(y | \\Theta) p(\\Theta)}{p(y)} \\\\ \\end{align} \\] Et voilà, Bayes’ Theorem. The probability of any particular value of \\(\\Theta\\), given the data, is equal to the product of the likelihood and the prior, divided by \\(p(y)\\). “But what’s \\(p(y)\\)?” you ask. The term \\(p(y)\\) is a confusing one – it can be called the “average likelihood,” “evidence,” or “probability of the data.” The average likelihood here means it is averaged over the prior and its job is to standardize the posterior so it integrates to 1. \\(p(y)\\) is expressed mathematically as: \\[ p(y) = E(p(y | \\Theta)) = \\int p(y | \\Theta) p(\\Theta) d\\Theta \\] \\(E(\\cdot)\\) means to take the expectation, a (weighted) average. Notice also that \\(p(y)\\) is a type of marginal probability distribution; the process of integrating out a term (\\(\\Theta\\)) is called marginalization – we are averaging \\(y\\) over all values of \\(\\Theta\\). Remember also that an integral is like an average over a continuous distribution of values. 2.4.2 But how does it work? Our model has three parts: likelihood, parameters, and the prior. These values get put into a “motor” that gives us a posterior distribution. The motor goes through the process of conditioning the prior on the data. Turns out that knowing all the rules doesn’t necessarily help us with the calculations. For most interesting models we will ever consider, the necessary integrals in the Bayesian conditioning machinery have no closed form and can’t be calculated no matter how talented you are. Each new parameter effectively adds a new level to the integral used to calculate \\(p(y)\\). Instead, we need to rely on numerical techniques to approximate the mathematics defined in Bayes’ theorem. All of the numerical techniques we use produce only samples from the posterior distribution, not the distribution itself. Luckily, samples from the distribution are easier to work with than the actual distribution – this way we don’t have to do integrals. 2.4.2.1 Grid approximation One of the simplest conditioning techniques is grid approximation. While most parameters are continuous, we can get a decent approximation of them by considering only a finite grid of parameter values. For any particular value \\(p&#39;\\), compute the posterior probability by multiplying the prior probability of \\(p&#39;\\) by the likelihood of \\(p&#39;\\). Grid approximation is a teaching tool that forces you to really understand the nature of Bayesian updating. You will probably never use it in your actual work. Grid approximation scales very poorly as the number of parameters increases. How to do grid approximation: Define a grid (range of values to look at). Calculate the value of the prior at each parameter value of the grid. Compute the likelihood at each parameter value. Compute the unstandardized posterior at each parameter, but multiplying the prior by the likelihood. Standardize the posterior by dividing each value by the sum of all unstandardized values. The number of points you evaluate on the grid determines the precision of your estimates. More points, finer grained posterior. For example, here’s the posterior probability of the percent of water on the globe given our data evaluated at 5 points and 20 points. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # computer product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # compute product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) 2.4.2.2 Markov chain Monte Carlo For many models, grid approximation or quadratic approximation just aren’t good enough. Grid approximation takes too long as your model gets bigger. Quadratic approximation chokes on complex models. Instead, we end up having to use a technique like Markov chain Monte Carlo (MCMC). Unlike grid approximation, where we computed the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, where the frequencies of those values correspond to the posterior plausibilities. Let’s do a quick example where we fit this model using brms. This package acts as an interface with Stan probabilistic programming language which implements Hamiltonian Monte Carlo sampling, a fancy type of MCMC. The function brm() is the workhorse of the brms package, and builds and compiles a Stan model as defined in R. If you’ve used functions like lm() or glm(), some of the syntax should look familiar to you. # this can take a bit as the model compiles globe_brms &lt;- brm(data = list(w = 24), # generate data family = binomial(link = &quot;identity&quot;), # define likelihood distribution formula = w | trials(36) ~ 1, # define parameter prior = prior(uniform(0, 1), class = Intercept), # give prior to parameter control = list(adapt_delta = 0.95), # control sampling behavior --&gt; advanced topic refresh = 0, # silences a bunch of text iter = 2000, # how many draws from posterior? (default) warmup = 1000) # how many draws till we start recording (default = 1/2 iter) print(globe_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.51 0.80 985 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # look at the posterior distribution of proportion water globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_x_continuous(&#39;proportion water&#39;, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) 2.4.2.3 Aside on Interpreting Probabilities From Math with Bad Drawings. 2.4.3 Working with samples In applied Bayesian analysis we rarely work directly with the integrals required by Bayes’ theorem. Most numerical techniques we use, including MCMC methods, produce a set of samples that are individual draws from the posterior distribution. These samples transform a problem in calculus to a problem in data summary. It is easier to count the number of samples within an interval then calculate the integral for that interval. This section also serves as a brief introduction to summarizing posterior samples using tidybayes, which we will continue using in our next lesson. Once our model produces a posterior distribution, the model’s work is done. It is now our job to summarize and interpret that posterior. Common questions we might want to ask include: How much posterior probability lies below some parameter value? How much posterior probability lies between two parameter values? Which parameter value marks the lower 5% of the posterior probability? Which range of parameter values contains 90% of the posterior probability? Which parameter value has the highest posterior probability? 2.4.3.1 Intervals of defined boundaries What is the posterior probability that the proportion of water is less than 50%? Count the number of observations that are less than 0.5, then divide by the number of samples. globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &lt; 0.5) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.0202 What if we want to know the posterior probability that the proportion of water is between 0.5 and 0.75? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &gt; 0.5 &amp; b_Intercept &lt; 0.75) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.867 2.4.3.2 Intervals of defined mass An interval of defined mass report two parameter values that contain between them the specified amount of posterior probability, a probability mass. You probably have heard of confidence intervals – an interval of posterior probability is called a credible interval, though the distinction between the terms isn’t terribly important. There are two kinds of intervals of defined mass: percentile (or quantile) interval, and highest posterior density interval. 2.4.3.2.1 Percentile interval (PI) For example, you may want to know the boundaries of the lower 80% posterior interval. This interval has to start at 0, but where does it stop? How do we calculate the 80th percentile? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q80 = quantile(b_Intercept, 0.8)) ## # A tibble: 1 x 1 ## q80 ## &lt;dbl&gt; ## 1 0.724 What about the middle 80% interval? There are lots of ways to get this information, so here are two examples. # one way (qu &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q10 = quantile(b_Intercept, 0.1), q90 = quantile(b_Intercept, 0.9))) ## # A tibble: 1 x 2 ## q10 q90 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.560 0.755 # another way globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% quantile(prob = c(0.1, 0.9)) ## 10% 90% ## 0.5603904 0.7549126 We can also plot this interval on the distribution. # plot the distribution p1 &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(limits = c(0, 1)) # get back the density line calculation p1_df &lt;- ggplot_build(p1)$data[[1]] # this is messy # shade area under the distribution p1 + geom_area(data = subset(p1_df, x &gt; qu$q10 &amp; x &lt; qu$q90), aes(x=x,y=y), fill = &quot;black&quot;, color = NA) 2.4.3.2.2 Highest posterior density interval An HPDI is defined as the narrowest interval containing the specified probability mass. There are an infinite of posterior intervals with the same mass, but what if you want that interval that best represents the parameter values most consistent with the data AND you want the densest of these intervals. Here an example of a 50% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% hdi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6129107 0.7158852 # compare to 50% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% qi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6084597 0.7118306 Why are the PI and HPDI not equal? If the choice of interval makes a big difference in your summary, then you probably shouldn’t be using intervals to summarize the posterior! The entire distribution of samples is our actual estimate, these summaries are just there to help digest this wealth of information. Do not only work with simplified versions of our posterior estimates! 2.4.3.3 Point estimates Sometimes we only want a single point from the distribution. Given an entire posterior distribution, what value should we report? Mechanically, this task is simple – pick a summary (e.g. mean) and go. Conceptually, however, this task is actually quite complex. The Bayesian parameter estimate is the entire posterior distribution, and not just a single number. In most cases, it is unnecessary to choose a single point estimate. It is always better to report more than necessary about the posterior distribution than not enough. The three most common point estimates are the mean, median, and mode – you’re probably already familiar with all three of them. A principled way of choosing among these three estimates is considering as products of different loss functions. Loss functions are an advanced topic we will not cover today but I encourage you to read up on; here’s a good blog entry by John Myles White on the subject. The mode represents the parameter value with the highest posterior probability, or the maximum a posteriori estimate (MAP). In frequentist contexts, the maximum likelihood estimate is the equivalent to mode of the likelihood function. Calculating the mode of a distribution is an optimization problem and isn’t always easy. Luckily, tidybayes has a function which gets us a mode (and other information) from the posterior samples. Here is a code snippet that gives a mode and a middle 95% percentile interval: globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mode_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.651 0.506 0.797 0.95 mode qi We could also report a mean or median. Here are some ways to do get these estimates along with some kind of interval: # mean with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.506 0.797 0.95 mean qi # mean with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.511 0.801 0.95 mean hdci # median with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.660 0.506 0.797 0.95 median qi # median with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.660 0.511 0.801 0.95 median hdci Usually it is better to communicate as much about the posterior distribution as you can. 2.5 Summary So far we’ve introduced the conceptual mechanism driving Bayesian data analysis. This framework emphasizes the posterior probability distribution, which is the logical compromise between our previous information and whatever new information we’ve gained (given our model). Posterior probabilities state the relative plausibility of each conjectured possibility that could have produced the data, given our model. These plausibilities are updated in light of observations, a process known as Bayesian updating. We’ve defined the components of a Bayesian model: a likelihood, one or more parameters, and a prior for every parameter. The likelihood defines the plausibility of the data, given a fixed value for the parameters. The prior provides the initial plausibility of each parameter value, before accounting for the data. These components, when processed through Bayes’ Theorem, yield the posterior distribution. Many of the actual calculations necessary to yield the posterior distribution have no closed-form solution and must instead be approximated using numerical methods. We covered grid approximation as a gentle introduction to sampling. We also fit our basic model using brms, which uses Stan’s HMC engine to condition our estimates on the data. Given the posterior samples from our model fit, we also covered basic methods for summarizing a posterior distribution such as intervals and point estimates. "],
["methods.html", "3 Methods", " 3 Methods We describe our methods in this chapter. "],
["applications.html", "4 Applications 4.1 Example one 4.2 Example two", " 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["final-words.html", "5 Final Words", " 5 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
