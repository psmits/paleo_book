<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 Logistic regression | Analytical Paleobiology</title>
  <meta name="description" content="An informal course on analytical paleobiology">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 Logistic regression | Analytical Paleobiology" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An informal course on analytical paleobiology" />
  <meta name="github-repo" content="psmits/paleo_book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Logistic regression | Analytical Paleobiology" />
  <meta name="twitter:site" content="@PeterDSmits" />
  <meta name="twitter:description" content="An informal course on analytical paleobiology" />
  

<meta name="author" content="Peter D Smits">


<meta name="date" content="2019-05-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="interactions.html">
<link rel="next" href="poisson-regression-and-others-glms.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="...">Short-Course on Analytical Paleobiology</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html"><i class="fa fa-check"></i><b>1</b> Managing and Processing Data From the Paleobiology Database</a><ul>
<li class="chapter" data-level="1.1" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#objectives"><i class="fa fa-check"></i><b>1.1</b> Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#project-reading"><i class="fa fa-check"></i><b>1.2</b> Reading</a></li>
<li class="chapter" data-level="1.3" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#introduction"><i class="fa fa-check"></i><b>1.3</b> Introduction</a></li>
<li class="chapter" data-level="1.4" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#getting-data"><i class="fa fa-check"></i><b>1.4</b> Getting data</a></li>
<li class="chapter" data-level="1.5" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#processing-data"><i class="fa fa-check"></i><b>1.5</b> Processing data</a></li>
<li class="chapter" data-level="1.6" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#binning-observations"><i class="fa fa-check"></i><b>1.6</b> Binning observations</a></li>
<li class="chapter" data-level="1.7" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#sharing-data"><i class="fa fa-check"></i><b>1.7</b> Sharing data</a></li>
<li class="chapter" data-level="1.8" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#summary"><i class="fa fa-check"></i><b>1.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#objectives-1"><i class="fa fa-check"></i><b>2.1</b> Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#reading"><i class="fa fa-check"></i><b>2.2</b> Reading</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#learning-from-data"><i class="fa fa-check"></i><b>2.3</b> Learning from data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#counting-and-plausibility"><i class="fa fa-check"></i><b>2.3.1</b> Counting and plausibility</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#globe-example"><i class="fa fa-check"></i><b>2.4</b> Building a model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#a-data-story"><i class="fa fa-check"></i><b>2.4.1</b> A data story</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#bayes-updating-ex"><i class="fa fa-check"></i><b>2.4.2</b> Bayesian updating</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#evaluate"><i class="fa fa-check"></i><b>2.4.3</b> Evaluate</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#terms-and-theory"><i class="fa fa-check"></i><b>2.5</b> Terms and theory</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#bayes-theorem"><i class="fa fa-check"></i><b>2.6</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#but-how-does-it-work"><i class="fa fa-check"></i><b>2.7</b> But how does it <em>work</em>?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#grid-approximation"><i class="fa fa-check"></i><b>2.7.1</b> Grid approximation</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>2.7.2</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="2.7.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#aside-on-interpreting-probabilities"><i class="fa fa-check"></i><b>2.7.3</b> Aside on Interpreting Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#working-with-samples"><i class="fa fa-check"></i><b>2.8</b> Working with samples</a><ul>
<li class="chapter" data-level="2.8.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#intervals-of-defined-boundaries"><i class="fa fa-check"></i><b>2.8.1</b> Intervals of defined boundaries</a></li>
<li class="chapter" data-level="2.8.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#intervals-of-defined-mass"><i class="fa fa-check"></i><b>2.8.2</b> Intervals of defined mass</a></li>
<li class="chapter" data-level="2.8.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#point-estimates"><i class="fa fa-check"></i><b>2.8.3</b> Point estimates</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#summary-1"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="reg-intro.html"><a href="reg-intro.html"><i class="fa fa-check"></i><b>3</b> Introduction to linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="reg-intro.html"><a href="reg-intro.html#objectives-2"><i class="fa fa-check"></i><b>3.1</b> Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="reg-intro.html"><a href="reg-intro.html#reading-1"><i class="fa fa-check"></i><b>3.2</b> Reading</a></li>
<li class="chapter" data-level="3.3" data-path="reg-intro.html"><a href="reg-intro.html#linear-regression"><i class="fa fa-check"></i><b>3.3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="reg-intro.html"><a href="reg-intro.html#talking-about-models"><i class="fa fa-check"></i><b>3.3.1</b> Talking about models</a></li>
<li class="chapter" data-level="3.3.2" data-path="reg-intro.html"><a href="reg-intro.html#growing-a-regression-model"><i class="fa fa-check"></i><b>3.3.2</b> Growing a regression model</a></li>
<li class="chapter" data-level="3.3.3" data-path="reg-intro.html"><a href="reg-intro.html#sampling-from-the-model"><i class="fa fa-check"></i><b>3.3.3</b> Sampling from the model</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="reg-intro.html"><a href="reg-intro.html#adding-a-predictor-to-the-mix"><i class="fa fa-check"></i><b>3.4</b> Adding a predictor to the mix</a><ul>
<li class="chapter" data-level="3.4.1" data-path="reg-intro.html"><a href="reg-intro.html#aside-dummy-coding"><i class="fa fa-check"></i><b>3.4.1</b> Aside: Dummy coding</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="reg-intro.html"><a href="reg-intro.html#interpreting-the-model-fit"><i class="fa fa-check"></i><b>3.5</b> Interpreting the model fit</a><ul>
<li class="chapter" data-level="3.5.1" data-path="reg-intro.html"><a href="reg-intro.html#linear-predictor"><i class="fa fa-check"></i><b>3.5.1</b> Linear predictor</a></li>
<li class="chapter" data-level="3.5.2" data-path="reg-intro.html"><a href="reg-intro.html#posterior-prediction"><i class="fa fa-check"></i><b>3.5.2</b> Posterior prediction</a></li>
<li class="chapter" data-level="3.5.3" data-path="reg-intro.html"><a href="reg-intro.html#posterior-predictive-tests"><i class="fa fa-check"></i><b>3.5.3</b> Posterior predictive tests</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="reg-intro.html"><a href="reg-intro.html#summary-2"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reg-continue.html"><a href="reg-continue.html"><i class="fa fa-check"></i><b>4</b> Continuing with regression with continuous predictors</a><ul>
<li class="chapter" data-level="4.1" data-path="reg-continue.html"><a href="reg-continue.html#objectives-3"><i class="fa fa-check"></i><b>4.1</b> Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="reg-continue.html"><a href="reg-continue.html#reading-2"><i class="fa fa-check"></i><b>4.2</b> Reading</a></li>
<li class="chapter" data-level="4.3" data-path="reg-continue.html"><a href="reg-continue.html#our-first-example"><i class="fa fa-check"></i><b>4.3</b> Our first example</a></li>
<li class="chapter" data-level="4.4" data-path="reg-continue.html"><a href="reg-continue.html#a-single-continuous-predictor"><i class="fa fa-check"></i><b>4.4</b> A single continuous predictor</a><ul>
<li class="chapter" data-level="4.4.1" data-path="reg-continue.html"><a href="reg-continue.html#centering"><i class="fa fa-check"></i><b>4.4.1</b> Aside: Centering</a></li>
<li class="chapter" data-level="4.4.2" data-path="reg-continue.html"><a href="reg-continue.html#continue-ppc"><i class="fa fa-check"></i><b>4.4.2</b> Checking model fit</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="reg-continue.html"><a href="reg-continue.html#summary-3"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reg-multi.html"><a href="reg-multi.html"><i class="fa fa-check"></i><b>5</b> Multiple predictors in linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="reg-multi.html"><a href="reg-multi.html#objectives-4"><i class="fa fa-check"></i><b>5.1</b> Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="reg-multi.html"><a href="reg-multi.html#reading-3"><i class="fa fa-check"></i><b>5.2</b> Reading</a></li>
<li class="chapter" data-level="5.3" data-path="reg-multi.html"><a href="reg-multi.html#more-than-one-predictor"><i class="fa fa-check"></i><b>5.3</b> More than one predictor</a><ul>
<li class="chapter" data-level="5.3.1" data-path="reg-multi.html"><a href="reg-multi.html#dummy"><i class="fa fa-check"></i><b>5.3.1</b> Categorical predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="reg-multi.html"><a href="reg-multi.html#defining-our-model"><i class="fa fa-check"></i><b>5.4</b> Defining our model</a></li>
<li class="chapter" data-level="5.5" data-path="reg-multi.html"><a href="reg-multi.html#fitting-model-in-brms"><i class="fa fa-check"></i><b>5.5</b> Fitting model in <code>brms</code></a></li>
<li class="chapter" data-level="5.6" data-path="reg-multi.html"><a href="reg-multi.html#aside-standardizing"><i class="fa fa-check"></i><b>5.6</b> Aside: Standardizing</a></li>
<li class="chapter" data-level="5.7" data-path="reg-multi.html"><a href="reg-multi.html#multi-ppc"><i class="fa fa-check"></i><b>5.7</b> Checking model fit</a></li>
<li class="chapter" data-level="5.8" data-path="reg-multi.html"><a href="reg-multi.html#matrix-notation"><i class="fa fa-check"></i><b>5.8</b> Aside: Matrix Notation</a></li>
<li class="chapter" data-level="5.9" data-path="reg-multi.html"><a href="reg-multi.html#summary-4"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interactions.html"><a href="interactions.html"><i class="fa fa-check"></i><b>6</b> Interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="interactions.html"><a href="interactions.html#objectives-5"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="interactions.html"><a href="interactions.html#reading-4"><i class="fa fa-check"></i><b>6.2</b> Reading</a></li>
<li class="chapter" data-level="6.3" data-path="interactions.html"><a href="interactions.html#introduction-1"><i class="fa fa-check"></i><b>6.3</b> Introduction</a></li>
<li class="chapter" data-level="6.4" data-path="interactions.html"><a href="interactions.html#inter-initial"><i class="fa fa-check"></i><b>6.4</b> Data and inital model</a></li>
<li class="chapter" data-level="6.5" data-path="interactions.html"><a href="interactions.html#how-to-specify-an-interaction"><i class="fa fa-check"></i><b>6.5</b> How to specify an interaction</a><ul>
<li class="chapter" data-level="6.5.1" data-path="interactions.html"><a href="interactions.html#symmetry-of-interactions"><i class="fa fa-check"></i><b>6.5.1</b> Symmetry of interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="interactions.html"><a href="interactions.html#fitting-a-model-with-an-interaction"><i class="fa fa-check"></i><b>6.6</b> Fitting a model with an interaction</a></li>
<li class="chapter" data-level="6.7" data-path="interactions.html"><a href="interactions.html#interpreting-our-model"><i class="fa fa-check"></i><b>6.7</b> Interpreting our model</a></li>
<li class="chapter" data-level="6.8" data-path="interactions.html"><a href="interactions.html#have-we-improved-on-our-previous-model"><i class="fa fa-check"></i><b>6.8</b> Have we improved on our previous model?</a></li>
<li class="chapter" data-level="6.9" data-path="interactions.html"><a href="interactions.html#continuouscontinuous-interactions"><i class="fa fa-check"></i><b>6.9</b> Continuous–Continuous interactions</a></li>
<li class="chapter" data-level="6.10" data-path="interactions.html"><a href="interactions.html#summary-5"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic regression</a><ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#objectives-6"><i class="fa fa-check"></i><b>7.1</b> Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#reading-5"><i class="fa fa-check"></i><b>7.2</b> Reading</a></li>
<li class="chapter" data-level="7.3" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-2"><i class="fa fa-check"></i><b>7.3</b> Introduction</a></li>
<li class="chapter" data-level="7.4" data-path="logistic-regression.html"><a href="logistic-regression.html#foram-coiling"><i class="fa fa-check"></i><b>7.4</b> Foram coiling</a></li>
<li class="chapter" data-level="7.5" data-path="logistic-regression.html"><a href="logistic-regression.html#writing-out-a-model"><i class="fa fa-check"></i><b>7.5</b> Writing out a model</a><ul>
<li class="chapter" data-level="7.5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-logistic-regression-coefficients"><i class="fa fa-check"></i><b>7.5.1</b> Interpreting logistic regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="logistic-regression.html"><a href="logistic-regression.html#priors-for-our-model"><i class="fa fa-check"></i><b>7.6</b> Priors for our model</a></li>
<li class="chapter" data-level="7.7" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-our-model"><i class="fa fa-check"></i><b>7.7</b> Fitting our model</a></li>
<li class="chapter" data-level="7.8" data-path="logistic-regression.html"><a href="logistic-regression.html#checking-model-adequacy"><i class="fa fa-check"></i><b>7.8</b> Checking model adequacy</a></li>
<li class="chapter" data-level="7.9" data-path="logistic-regression.html"><a href="logistic-regression.html#summary-6"><i class="fa fa-check"></i><b>7.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html"><i class="fa fa-check"></i><b>8</b> Poisson regression and others GLMs</a><ul>
<li class="chapter" data-level="8.1" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html#outline"><i class="fa fa-check"></i><b>8.1</b> Outline</a></li>
<li class="chapter" data-level="8.2" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html#poisson-distribution"><i class="fa fa-check"></i><b>8.2</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html"><i class="fa fa-check"></i><b>9</b> Varying-intercept models</a><ul>
<li class="chapter" data-level="9.1" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#objectives-7"><i class="fa fa-check"></i><b>9.1</b> Objectives</a></li>
<li class="chapter" data-level="9.2" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#reading-6"><i class="fa fa-check"></i><b>9.2</b> Reading</a></li>
<li class="chapter" data-level="9.3" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#introduction-3"><i class="fa fa-check"></i><b>9.3</b> Introduction</a></li>
<li class="chapter" data-level="9.4" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#dummy-example"><i class="fa fa-check"></i><b>9.4</b> A basic model with categorical predictor</a></li>
<li class="chapter" data-level="9.5" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#notation-for-multilevel-models"><i class="fa fa-check"></i><b>9.5</b> Notation for multilevel models</a></li>
<li class="chapter" data-level="9.6" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#multilevel-example"><i class="fa fa-check"></i><b>9.6</b> From categorical predictors to varying-intercept</a></li>
<li class="chapter" data-level="9.7" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#fitting-a-multilevel-model"><i class="fa fa-check"></i><b>9.7</b> Fitting a multilevel model</a></li>
<li class="chapter" data-level="9.8" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#understanding-our-multilevel-model"><i class="fa fa-check"></i><b>9.8</b> Understanding our multilevel model</a></li>
<li class="chapter" data-level="9.9" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#a-more-complex-varying-intercept-model"><i class="fa fa-check"></i><b>9.9</b> A more complex varying-intercept model</a></li>
<li class="chapter" data-level="9.10" data-path="varying-intercept-models.html"><a href="varying-intercept-models.html#summary-7"><i class="fa fa-check"></i><b>9.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="vary-all.html"><a href="vary-all.html"><i class="fa fa-check"></i><b>10</b> Varying slopes and intercepts</a></li>
<li class="chapter" data-level="11" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>11</b> Model Comparison</a><ul>
<li class="chapter" data-level="11.1" data-path="model-comparison.html"><a href="model-comparison.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>11.1</b> Bias-Variance Trade-off</a><ul>
<li class="chapter" data-level="11.1.1" data-path="model-comparison.html"><a href="model-comparison.html#rmse"><i class="fa fa-check"></i><b>11.1.1</b> RMSE</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="model-comparison.html"><a href="model-comparison.html#regularization"><i class="fa fa-check"></i><b>11.2</b> Regularization</a></li>
<li class="chapter" data-level="11.3" data-path="model-comparison.html"><a href="model-comparison.html#information-criteria-and-waic"><i class="fa fa-check"></i><b>11.3</b> Information Criteria and WAIC</a></li>
<li class="chapter" data-level="11.4" data-path="model-comparison.html"><a href="model-comparison.html#looic"><i class="fa fa-check"></i><b>11.4</b> LOO(IC)</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>12</b> Time Series</a><ul>
<li class="chapter" data-level="12.1" data-path="time-series.html"><a href="time-series.html#objectives-8"><i class="fa fa-check"></i><b>12.1</b> Objectives</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analytical Paleobiology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">7</span> Logistic regression</h1>
<div id="objectives-6" class="section level2">
<h2><span class="header-section-number">7.1</span> Objectives</h2>
<ul>
<li>Introduce logistic regression</li>
<li>Interpret regression coefficients from logistic regression</li>
<li>Assess model adequacy</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pacman)

<span class="kw">p_load</span>(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot,
       readxl, arm, modelr)

<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</code></pre></div>
</div>
<div id="reading-5" class="section level2">
<h2><span class="header-section-number">7.2</span> Reading</h2>
<p>The following materials are recommended pre-readings before starting this tutorial.</p>
<ul>
<li>Chapter 5 “Logistic regression” from <a href="https://www.cambridge.org/core/books/data-analysis-using-regression-and-multilevelhierarchical-models/32A29531C7FD730C3A68951A17C9D983"><strong>Data Analysis Using Regression and Multilevel/Hierarchical Models</strong> by Gelman and Hill</a>.</li>
<li>Chapter 10 “Counting and Classification” from <a href="https://xcelab.net/rm/statistical-rethinking/"><strong>Statistical Rethinking</strong> by Richard McElreath</a>.</li>
<li>OPTIONAL Chapter 9 “Big Entropy and the Generalized Linear Model” from <a href="https://xcelab.net/rm/statistical-rethinking/"><strong>Statistical Rethinking</strong> by Richard McElreath</a>.</li>
</ul>
<p>A lot of the material and advice for interpreting logistic regression coefficients, both as probabilities and odds, is based on Chapter 5 of <a href="https://www.cambridge.org/core/books/data-analysis-using-regression-and-multilevelhierarchical-models/32A29531C7FD730C3A68951A17C9D983">Gelman and Hill</a>.</p>
</div>
<div id="introduction-2" class="section level2">
<h2><span class="header-section-number">7.3</span> Introduction</h2>
<p>Not all response variables we might be interested in are continuous variables between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. In some cases our variable of interest might be binary (0/1). For example, we might be interested in if a snail shell coils to the left or right, or if a species survives from one time to the next. The <a href="introduction-to-bayesian-data-analysis.html#globe-example">globe tossing exercise from an earlier lesson</a> is an example of this type of data. But instead of asking for the probability of an outcome, we’ll now lean how to incorporate predictor variables into that type of model.</p>
<p>Logistic regression is the standard way to model a binary response variable. We will be modeling the response variable, <span class="math inline">\(y\)</span>, as following a Bernoulli distribution. The Bernoulli distribution has a single parameter, <span class="math inline">\(\theta\)</span>, which is the probability of a “positive” outcome i.e. a 1 and not a 0. The Bernoulli distribution is related to the Binomial distribution introduced with the <a href="introduction-to-bayesian-data-analysis.html#globe-example">globe tossing exercise</a>, but describes the probability of individual events as opposed to the probability of those “positive” events in aggregate.</p>
<p>Just because the type of the response variable has changed doesn’t mean we have to completely reinvent the wheel. While we aren’t going to use the Gaussian distribution to model our outcome like in linear regression, we’re still going to use an additive series of covariates multiplied by regression coefficients. But we have one huge complication: the Bernoulli distribution’s only parameter <span class="math inline">\(\theta\)</span> is defined from 0 to 1 while a linear model <span class="math inline">\(X \beta\)</span> is defined from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>. We need a tool (i.e. mathematical function) to make these definitions talk to each other.</p>
<p>The logit, or log-odds, function does this exactly.</p>
<p>The logit function is defined as the logarithm of the odds of some probability <span class="math inline">\(p\)</span>: <span class="math display">\[
\text{logit}(p) = \text{log} \frac{p}{1 - p}. 
\]</span> In the case of logistic regression, we would substitute the <span class="math inline">\(\theta\)</span> parameter from the Binomial which gives us an expression to map out linear model to: <span class="math display">\[
\text{logit}(\theta) = \text{log} \frac{\theta}{1 - \theta} = X \beta.
\]</span> Sometimes you will see the previous statement written using the inverse logit function, which is actually called the logistic function, which is defined <span class="math display">\[
\theta = \text{logit}^{-1} (X \beta) = \frac{e^{X \beta}}{1 + e^{X \beta}}.
\]</span> The logistic function is where logistic regression gets its name.</p>
<p>Let’s take a look at how the logistic transformation maps values from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> onto the (0, 1) space. Some people find it easier to work with the inverse logit (logit<span class="math inline">\(^{-1}\)</span>) formulation because it can be easier to focus on the mapping of the linear predictor on to the probabilities, as opposed to the reverse. However, both of these formulations are equally valid so understanding both is important when reading the literature.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">-10</span>, <span class="dt">to =</span> <span class="dv">10</span>, <span class="dt">by =</span> <span class="fl">.1</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> arm<span class="op">::</span><span class="kw">invlogit</span>(x))         <span class="co"># inverse logit</span>

df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;logit scale&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;probability scale&#39;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/logisitic-demo-1.png" width="672" /></p>
<p>This plot of the logistic function is very revealing. When <code>x</code> has a magnitude of 3 , <code>y</code> is either close to its maxima or minima. This behavior means that the approximate slope of the logistic function for values of <code>x</code> between -3 and 3 is much larger than the approximate slope of the logistic function for values of <code>x</code> with a magnitude of 3+. We will explore the importance of this behavior in a latter part of this lesson. In general, any change on the logit scale is compressed at the ends of the probability scale – a natural part of keeping probability bounded between 0 and 1.</p>
<p>Mapping functions, like the logit and logistic functions, let lot us model a parameter by a linear model, is frequently referred to as a <strong>link function</strong>.</p>
<p>Now that we’ve defined a way to model the <span class="math inline">\(\theta\)</span> parameter of the Bernoulli distribution as a linear function, we can start building a logistic regression and explore how to interpret our model’s regression coefficients.</p>
</div>
<div id="foram-coiling" class="section level2">
<h2><span class="header-section-number">7.4</span> Foram coiling</h2>
<p>For this scion, we will develop a logistic regression model to describe the probability of a foram being dextrally coiling as a function of multiple measures of that foram’s size and shape.</p>
<p>To get to that point, we will first explore the foram measurement data. Once we have a handle on the data, we’ll develop and write out a Bayesian logistic regression model and then fit that model using <code>brms</code>. Finally, as always, we’ll then explore the adequacy of our model at describing out data.</p>
<p>The data we will be focusing on is from a paper by <a href="https://bioone.org/journals/paleobiology/volume-40/issue-1/13004/Evolution-and-speciation-in-the-Eocene-planktonic-foraminifer-Turborotalia/10.1666/13004.full">Pearson and Ezard</a> that explored changes to morphology associated with speciation. In the original study, the morphological measures were analyzed as time series in an effort to characterize their evolution. For this lesson, we are instead tackling a much simpler question: do measures of foram size and shape predict if an individual is dextrally coiled or not? Are differently coiled forams simple mirror images or is there something more going on?</p>
<p>We’re going to be ignoring the time structure of the data for now. We’ll cover time series models in a later lesson. For now, we are interested how size and shape measures predict coiling with the assumption that this relationship is constant over time.</p>
<p>Let’s bring the data into the memory and start playing around with it. The Peason and Ezard paper describes a few derived measures, like compression ratio, that we can quickly recalculate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># read in excel data directly with function from readxl</span>
(foram &lt;-<span class="st"> </span><span class="kw">read_xlsx</span>(<span class="dt">path =</span> here<span class="op">::</span><span class="kw">here</span>(<span class="st">&#39;data&#39;</span>, <span class="st">&#39;pearson_ezard&#39;</span>, 
                                     <span class="st">&#39;turborotalia_data.xlsx&#39;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># useful derived measures</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">compression =</span> diameter <span class="op">/</span><span class="st"> </span>axis,
         <span class="dt">chamber_aspect =</span> chamber_height <span class="op">/</span><span class="st"> </span>chamber_width,
         <span class="dt">aperture_aspect =</span> aperture_height <span class="op">/</span><span class="st"> </span>aperture_width,
         <span class="dt">dextral =</span> <span class="kw">as.factor</span>(dextral)))</code></pre></div>
<pre><code>## # A tibble: 10,200 x 22
##    core  section interval depth   age    id baseline diameter  axis radius
##    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 3H          1 18.5-18…  18.5  34.7     1     389.     597.  381.   252.
##  2 3H          1 18.5-18…  18.5  34.7     2     388.     571.  364.   244.
##  3 3H          1 18.5-18…  18.5  34.7     3     387.     570.  355.   242.
##  4 3H          1 18.5-18…  18.5  34.7     4     367.     513.  341.   244.
##  5 3H          1 18.5-18…  18.5  34.7     5     335.     554.  341.   292.
##  6 3H          1 18.5-18…  18.5  34.7     6     319.     518.  308.   218.
##  7 3H          1 18.5-18…  18.5  34.7     7     293.     526.  259.   207.
##  8 3H          1 18.5-18…  18.5  34.7     8     445.     703.  463.   319.
##  9 3H          1 18.5-18…  18.5  34.7     9     387.     567.  351.   228.
## 10 3H          1 18.5-18…  18.5  34.7    10     290.     496.  286.   229.
## # … with 10,190 more rows, and 12 more variables: aperture_width &lt;dbl&gt;,
## #   aperture_height &lt;dbl&gt;, chamber_width &lt;dbl&gt;, chamber_height &lt;dbl&gt;,
## #   umbilical_angle &lt;dbl&gt;, periphery &lt;dbl&gt;, area &lt;dbl&gt;,
## #   chamber_number &lt;dbl&gt;, dextral &lt;fct&gt;, compression &lt;dbl&gt;,
## #   chamber_aspect &lt;dbl&gt;, aperture_aspect &lt;dbl&gt;</code></pre>
<p>A common occurrence with measurement data is that many of the measures are derived from each other. The obvious examples here are the aspect ratios – these are directly calculated from 2 other measures. So which measures do we include in our analyses? I’m going to use the original paper as a guide and stick with compression, area, umbilical angle, aperture aspect ratio, chamber aspect ratio, and the number of chambers in the final whorl.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">foram <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(dextral, compression, area, umbilical_angle, aperture_aspect, 
                chamber_aspect, chamber_number) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> key, <span class="dt">value =</span> value, <span class="op">-</span>dextral) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">fill =</span> dextral)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>key, <span class="dt">scales =</span> <span class="st">&#39;free&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_fill_viridis</span>(<span class="dt">discrete =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="paleo_book_files/figure-html/explore_foram-1.png" width="672" /></p>
</div>
<div id="writing-out-a-model" class="section level2">
<h2><span class="header-section-number">7.5</span> Writing out a model</h2>
<p>Now that we’ve defined our response variable (<code>dextral</code>) and isolated our predictors of interest, we can now start describing out model to predict if a specimen will be dextrally or sinistrally coiled.</p>
<p>Let’s define the coiling of each observation as <span class="math inline">\(y_{i}\)</span> where <span class="math inline">\(i\)</span>, where <span class="math inline">\(i = 1, 2, ..., N\)</span> and <span class="math inline">\(N\)</span> is the total number of observations. We can then define <span class="math inline">\(X\)</span> as our <a href="reg-multi.html#matrix-notation">design matrix</a> which is an <span class="math inline">\(N\)</span> by <span class="math inline">\(D + 1\)</span> matrix, where <span class="math inline">\(D\)</span> is the number of covariates. The additional column in <span class="math inline">\(X\)</span> is entirely 1s, so that it corresponds to the intercept term of the regression. Let’s define the first column of our design matrix as the one with all 1s. Lastly for now, let’s define a vector of regression coefficients <span class="math inline">\(\beta\)</span> which has length <span class="math inline">\(D + 1\)</span> and the first element corresponds to the intercept.</p>
<p>Given these definitions, we can write out most of our logistic regression model. <span class="math display">\[
\begin{align}
y_{i} &amp;\sim \text{Bernoulli}(\theta) \\
\text{logit}(\theta) &amp;= X_{i} \beta \\
\end{align}
\]</span> What key component(s) of this model are missing? The priors! In order to assign priors for our regression coefficients, we first need to understand what those coefficients <em>mean</em>.</p>
<div id="interpreting-logistic-regression-coefficients" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Interpreting logistic regression coefficients</h3>
<p>The definition of a regression coefficient is that it describes the expected change in the response per unit change in its predictor. However, the logit (or inverse logit) function introduced into our model creates a nonlinearity which complicates the simplicity of this interpretation.</p>
<p>There are two genres of effects that a regression coefficient in logistic regression can describe. The regression coefficient by itself describes the <strong>relative effect</strong> on the outcome expected from a change in the predictor. In contrast, the <strong>absolute effect</strong> of a regression coefficient is measured by the change in the probability of the outcome. This measure depends on the other parameters and describes the practical impact of a change in a predictor.</p>
<p>Let’s explore what this means with two examples. Suppose we have a logistic model defined <span class="math display">\[
\begin{align}
y &amp;\sim \text{Bernoulli}(\theta) \\
\text{logit}(\theta) &amp;= -1 + -0.5 x \\
\end{align}
\]</span></p>
<p>The curvature of the logistic function requires us to choose where to evaluate changes, if we want to interpret on the probability scale.</p>
<p>But how do we interpret regression coefficients with values like -0.5? The mean of the input variables is a good starting point. As with linear regression, the intercept of a logistic can only be interpreted assuming zero values for all the other predictors. This point is the central point of the estimated logistic function (where on the probability axis the logistic function crosses <span class="math inline">\(x = 0\)</span>). For this example, <span class="math inline">\(Pr(y = 1) = \text{logit}^{-1}(-1) \approx 0.27\)</span>. With mean-centered data, the central point describes the baseline probability of the result being a 1. For this example, at mean value of x (i.e. 0) there is an approximately 27% chance of <span class="math inline">\(y = 1\)</span>.</p>
<p>A positive increase in <span class="math inline">\(x\)</span> corresponds to a negative difference of approximately -0.5 in the logit probability of <span class="math inline">\(y\)</span>. This is the relative effect of the predictor in <span class="math inline">\(y\)</span>.</p>
<p>We can calculate this difference in probability when x = 0 and when x = 1. So an increase of <span class="math inline">\(x\)</span> by 1 would decrease the probability of <span class="math inline">\(y = 1\)</span> by <span class="math inline">\(\text{logit}^{-1}(-1 + -0.5 * 1) - \text{logit}^{-1}{-1 + -0.5 * 0} \approx -0.09\)</span>. This is the absolute effect of the predictor on <span class="math inline">\(y\)</span>.</p>
<div id="odds-ratios" class="section level4">
<h4><span class="header-section-number">7.5.1.1</span> Odds ratios</h4>
<p>Another common way to interpret logistic regression coefficients is in terms of <strong>odds ratios</strong>.</p>
<p>If two outcomes have the probabilities <span class="math inline">\((p, 1 - p)\)</span>, then <span class="math inline">\(p / (1 - p)\)</span> is the odds. An odds of 1 means the outcomes are equally likely because odds of 1 is equivalent to a probability of 0.5 (<span class="math inline">\(1 = 0.5 / (1 - 0.5)\)</span>). Similarly, odds of 0.5 or 2 represent probabilities of 1/3 and 2/3, respectively.</p>
<p>The ratio of two odds (e.g. <span class="math inline">\((p_{1} / (1 - p_{1})) / (p_{2} / (1 - p_{2}))\)</span>) is called an odds ratio. An odds ratio of 2 corresponds to a change from <span class="math inline">\(p = 0.33\)</span> to <span class="math inline">\(p = 0.5\)</span>, or a change from <span class="math inline">\(p = 0.5\)</span> to <span class="math inline">\(p = 0.67\)</span>.</p>
<p>An advantage of working with odds ratios instead of probabilities is that it is possible to keep scaling up odds ratios indefinitely without running into the boundary points of 0 and 1.</p>
<p>Exponentiated logistic regression coefficients can be interpreted as odds ratios. Let’s illustrate this with an example model with only one predictor: <span class="math display">\[
\log \left( \frac{\text{Pr}(y = 1 | x)}{\text{Pr}(y = 0 | x)} \right) = \alpha + \beta x
\]</span> Adding 1 to <span class="math inline">\(x\)</span> in this equation as the effect of adding to both sides of the equation. Exponentiating both sides means that the odds are then multiplied by <span class="math inline">\(e^{\beta}\)</span>. So if <span class="math inline">\(\beta = 0.3\)</span>, then a unit change in <span class="math inline">\(x\)</span> corresponds to multiplicative change of <span class="math inline">\(e^{0.3} \approx 1.35\)</span> in the odds.</p>
<p>I prefer to interpret coefficients on the probability scale and tend to recommend it. Most researchers don’t understand odds in the first place, so the more obscure odds ratio only increases confusion.<a href="https://www.theanalysisfactor.com/the-difference-between-relative-risk-and-odds-ratios/">Most researchers don’t know the difference between an odds and a relative risk</a>. People tend to understand the probability scale. By working with probabilities your interpretations are on the same scale as the data, not a transform of the data. It is important to understand the odds ratio interpretation, however, because it shows up often enough in the literature.</p>
</div>
</div>
</div>
<div id="priors-for-our-model" class="section level2">
<h2><span class="header-section-number">7.6</span> Priors for our model</h2>
<p>When thinking about how to define the priors for our logistic regression model, let’s return to our graph of the logistic function. Most of the action in this graph takes place between -3 and 3, and input values with magnitude greater than 5 barely move the value of the logistic function. When we think of this in terms possible regression coefficient values, we probably wouldn’t expect an intercept term with magnitude much greater than 5 or a regression coefficient with magnitude much greater than 2.</p>
<p>Given our explorations, we can provide some weakly informative priors for reasonable regression coefficient values which reflect our understanding of the logistic function. Here is the full definition of our model including priors. <span class="math display">\[
\begin{align}
y_{i} &amp;\sim \text{Bernoulli}(\theta) \\
\text{logit}(\theta) &amp;= X_{i} \beta \\
\beta_{d} &amp;\sim
  \begin{cases}
  \text{Normal}(0, 5) $ \text{if } d = 1 \\ % for intercept
  \text{Normal}(0, 1) $ \text{if } d \neq 1 \\
  \end{cases}
\end{align}
\]</span> Because the Bernoulli distribution only has a single parameter, we do not need to define a parameter for a scale term (e.g. standard deviation) like in a linear regression model.</p>
<p>This is our first time exploring and writing out a logistic regression, so we should be careful to fully understand the implications of our priors. As discussed previously, the <strong>prior predictive distribution</strong> is this – the distribution of expected outcomes given our prior information.</p>
<p>There are many ways we can go about simulating responses from the prior predictive distribution of our model. Two common ways are writing out an R function to simulate response data, and using the <code>brm()</code> function with the <code>sample_prior</code> argument given “only”.</p>
<p>Let’s demonstrate both these options. First, though, we’re going to need to prepare our data. While our model hasn’t seen our data, we can still see what it’ll guess for their coiling states.</p>
<p>Before we analyze our data, we should mean-center and rescale our predictors by two times their standard deviations so that all regression coefficients are more interpretable and comparable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">foram_ready &lt;-<span class="st"> </span>
<span class="st">  </span>foram <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(dextral,
                compression, 
                area, 
                umbilical_angle,
                aperture_aspect, 
                chamber_aspect, 
                chamber_number) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate_at</span>(<span class="dt">.vars =</span> <span class="kw">vars</span>(<span class="op">-</span>dextral),
            <span class="co"># mean center, 2*sd standardize</span>
            <span class="dt">.funs =</span> <span class="kw">list</span>(<span class="dt">rescale =</span> <span class="op">~</span><span class="st"> </span>arm<span class="op">::</span><span class="kw">rescale</span>(.))) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(dextral, <span class="kw">ends_with</span>(<span class="st">&#39;rescale&#39;</span>))</code></pre></div>
<p>Writing an R function to simulate from the prior preductive distribution is probably the simplest but most time consuming approach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#&#39; Prior predictive distribution</span>
<span class="co">#&#39;</span>
<span class="co">#&#39; What it says on the tin. Simulate prior predictive draws from our coiling model. This function gives the probability of dextral coiling.</span>
<span class="co">#&#39;</span>
<span class="co">#&#39; @param x design matrix incl. intercept (default is 1x1 identity matrix)</span>
<span class="co">#&#39; @param transform </span>
coil_prior_prob &lt;-<span class="st"> </span><span class="cf">function</span>(x) {

  <span class="co"># simulate values for each parameter</span>
  <span class="co"># how many predictors -&gt; ncol(x)</span>
  b &lt;-<span class="st"> </span><span class="kw">double</span>(<span class="dt">length =</span> <span class="kw">ncol</span>(x))

  <span class="cf">if</span>(<span class="kw">length</span>(b) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
    b &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>) 
  } <span class="cf">else</span> <span class="cf">if</span>(<span class="kw">length</span>(b) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {
    b[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>)
    b[<span class="op">-</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(b) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)
  }
    
  <span class="co"># make linear function</span>
  lin &lt;-<span class="st"> </span>x <span class="op">%*%</span><span class="st"> </span>b
  out &lt;-<span class="st"> </span><span class="kw">invlogit</span>(lin)
  out
}</code></pre></div>
<p>Now that we have a function to simulate prior predictive draws from data, let’s apply it to our foram data. This means yielding a prior design matrix from <code>foram_ready</code> with an intercept column and all. We can stick that matrix right into our function and get a single simulate dataset from the prior predictive distribution. Let’s look at the distribution of 0s and 1s from the empirical data and compare it to the distribution from four simulated data sets</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">foram_design &lt;-<span class="st"> </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">model_matrix</span>(dextral <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.matrix</span>()

<span class="co"># rerun for different simulations</span>
draw &lt;-<span class="st"> </span><span class="kw">rerun</span>(<span class="dv">4</span>, <span class="kw">coil_prior_prob</span>(foram_design))

sim_results &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">sim =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,
         <span class="dt">prob =</span> draw,
         <span class="dt">dextral_predict =</span> <span class="kw">map</span>(draw, <span class="op">~</span><span class="st"> </span><span class="kw">ifelse</span>(.x <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)),
         <span class="dt">dextral =</span> <span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="op">~</span><span class="st"> </span>foram_ready<span class="op">$</span>dextral)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unnest</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dextral_predict =</span> <span class="kw">case_when</span>(dextral_predict <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> &#39;dextral&#39;</span>,
                                     dextral_predict <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">~</span><span class="st"> &#39;sinistral&#39;</span>),
         <span class="dt">dextral =</span> <span class="kw">case_when</span>(dextral <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">~</span><span class="st"> &#39;dextral&#39;</span>,
                             dextral <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">~</span><span class="st"> &#39;sinistral&#39;</span>))

sim_results <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dextral_predict, <span class="dt">y =</span> dextral), <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>sim) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Predicted coiling state&#39;</span>,
       <span class="dt">y =</span> <span class="st">&#39;Actual coiling state&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/hand_sim-1.png" width="672" /></p>
<p>The above plot resembles a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> except we present a cloud of points in each of the quadrants as opposed to a count. However, the principle is the same – we want to compare actual versus predicted.</p>
<p>The big take away from this plot is that our priors imply equally random change of that observation being dextrally or sinestrally coiled.</p>
<p>Another way to simulate from the prior predictive distribution is by using the <code>brm()</code> function that we usually use to fit our models. This approach doesn’t involve writing our own function so it is more fool-proof. We still need to do an awkward amount by hand, though. This approach does require our model to be compiled and uses the HMC sampler to get our prior predictive draws, which means this process can take a lot longer.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">foram_prior &lt;-<span class="st"> </span>
<span class="st">  </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">brm</span>(<span class="dt">data =</span> .,
      <span class="dt">family =</span> <span class="kw">bernoulli</span>(),
      <span class="dt">formula =</span> <span class="kw">bf</span>(dextral <span class="op">~</span><span class="st"> </span>.),
      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),
                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)),
      <span class="dt">sample_prior =</span> <span class="st">&#39;only&#39;</span>,
      <span class="dt">iter =</span> <span class="dv">1004</span>,                     <span class="co"># only need 4 after warmup</span>
      <span class="dt">warmup =</span> <span class="dv">1000</span>,
      <span class="dt">chains =</span> <span class="dv">4</span>,                      
      <span class="dt">cores =</span> <span class="dv">4</span>,
      <span class="dt">refresh =</span> <span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> foram_prior,
                   <span class="dt">n =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dextral_predict =</span> <span class="kw">ifelse</span>(.value <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="kw">aes</span>(<span class="dt">x =</span> dextral_predict, <span class="dt">y =</span> dextral), <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>.draw)</code></pre></div>
<p><img src="paleo_book_files/figure-html/brm_sim_plot-1.png" width="672" /></p>
<p>With complex models it is easier to use <code>brm()</code> to simulate from the prior predictive distribution.</p>
</div>
<div id="fitting-our-model" class="section level2">
<h2><span class="header-section-number">7.7</span> Fitting our model</h2>
<p>We’ve explored our variables. We’ve written out a model. We’ve explored the implications of our priors. Now let’s fit our model to our data! We’ve already coded up our model using <code>brm()</code> earlier when looking at the prior preditive distribution. Let’s do that again but this time increase the number of samples and leave off the <code>sample_prior</code> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_<span class="dv">1</span> &lt;-<span class="st"> </span>
<span class="st">  </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">brm</span>(<span class="dt">data =</span> .,
      <span class="dt">family =</span> <span class="kw">bernoulli</span>(),
      <span class="dt">formula =</span> <span class="kw">bf</span>(dextral <span class="op">~</span><span class="st"> </span>.),
      <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dt">class =</span> Intercept),
                <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> b)),
      <span class="dt">iter =</span> <span class="dv">2000</span>,
      <span class="dt">warmup =</span> <span class="dv">1000</span>,
      <span class="dt">chains =</span> <span class="dv">4</span>,
      <span class="dt">cores =</span> <span class="dv">4</span>,
      <span class="dt">refresh =</span> <span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(m_<span class="dv">1</span>)</code></pre></div>
<pre><code>##  Family: bernoulli 
##   Links: mu = logit 
## Formula: dextral ~ compression_rescale + area_rescale + umbilical_angle_rescale + aperture_aspect_rescale + chamber_aspect_rescale + chamber_number_rescale 
##    Data: . (Number of observations: 10200) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample
## Intercept                  -0.26      0.02    -0.31    -0.21       6274
## compression_rescale        -1.75      0.08    -1.91    -1.60       4259
## area_rescale               -0.92      0.05    -1.02    -0.81       5517
## umbilical_angle_rescale    -0.26      0.05    -0.36    -0.17       5323
## aperture_aspect_rescale    -0.46      0.05    -0.57    -0.36       5142
## chamber_aspect_rescale      1.63      0.06     1.50     1.75       3856
## chamber_number_rescale     -0.69      0.05    -0.80    -0.59       5135
##                         Rhat
## Intercept               1.00
## compression_rescale     1.00
## area_rescale            1.00
## umbilical_angle_rescale 1.00
## aperture_aspect_rescale 1.00
## chamber_aspect_rescale  1.00
## chamber_number_rescale  1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Looking directly at the parameter estimates let’s us compare the relative effects of the predictors on the expected outcome. Let’s plot the posterior estimates for the regression coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_<span class="dv">1</span> <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">tidy_draws</span>() <span class="op">%&gt;%</span>
<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>(.chain<span class="op">:</span>.draw), <span class="op">-</span>(lp__<span class="op">:</span>energy__)) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">gather</span>(<span class="dt">key =</span> key, <span class="dt">value =</span> value) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">key =</span> <span class="kw">fct_inorder</span>(key),
        <span class="dt">key =</span> <span class="kw">fct_rev</span>(key)) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">y =</span> key)) <span class="op">+</span>
<span class="st"> </span><span class="kw">geom_halfeyeh</span>(<span class="dt">.width =</span> <span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.5</span>)) <span class="op">+</span><span class="st"> </span><span class="co"># alternative to ggridges</span>
<span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Posterior estimate&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Regression coefficient&#39;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_coef_relative-1.png" width="672" /> Remember, these parameters as written describe the expected change in logit <span class="math inline">\(y\)</span> per unit change in their respective variable – this is the relative effect. To look at the absolute effect of these covariates we need to explore their effects on the probability scale.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_<span class="dv">1</span> <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">tidy_draws</span>() <span class="op">%&gt;%</span>
<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>(.chain<span class="op">:</span>.draw), <span class="op">-</span>(lp__<span class="op">:</span>energy__)) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">transmute</span>(<span class="dt">midpoint =</span> <span class="kw">invlogit</span>(b_Intercept),
           <span class="dt">compression =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_compression_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>),
           <span class="dt">area =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_area_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>),
           <span class="dt">umbilical_angle =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_umbilical_angle_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>),
           <span class="dt">aperture_aspect =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_aperture_aspect_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>),
           <span class="dt">chamber_aspect =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_chamber_aspect_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>),
           <span class="dt">chamber_number =</span> <span class="kw">invlogit</span>(b_Intercept <span class="op">+</span><span class="st"> </span>b_chamber_number_rescale <span class="op">*</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">transmute_at</span>(<span class="dt">.vars =</span> <span class="kw">vars</span>(<span class="op">-</span>midpoint),
              <span class="dt">.funs =</span> <span class="kw">list</span>(<span class="dt">change =</span> <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>midpoint)) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">gather</span>(<span class="dt">key =</span> key, <span class="dt">value =</span> value) <span class="op">%&gt;%</span>
<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">y =</span> key)) <span class="op">+</span>
<span class="st"> </span><span class="kw">geom_halfeyeh</span>(<span class="dt">.width =</span> <span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.5</span>)) <span class="op">+</span><span class="st"> </span><span class="co"># alternative to ggridges</span>
<span class="st"> </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Change in probability at midpoint for 1 unit increase in predictor&#39;</span>, 
      <span class="dt">y =</span> <span class="st">&#39;Predictor&#39;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_coef_probability-1.png" width="672" /></p>
<p>We can also visualize the relationship between the individual covariates and coiling state. This involves holding all predictors constant except for one of them, and calculating the logistic function for the whole range of values.</p>
<p>Let’s look at the effects of three of the six covariates on expected coiling direction. Remember, when we want to interpret the effects of one of these regression coefficients, we vary the one predictor of interest and hold all the others constant. With mean-centered data and holding all other predictors at 0, we can visuallize the effect of that one predictor on observations that are average for all other predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">line_data &lt;-<span class="st"> </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(<span class="dt">compression_rescale =</span> <span class="kw">seq_range</span>(compression_rescale, <span class="dt">n =</span> <span class="dv">100</span>),
            <span class="dt">area_rescale =</span> <span class="dv">0</span>,
            <span class="dt">umbilical_angle_rescale =</span> <span class="dv">0</span>,
            <span class="dt">aperture_aspect_rescale =</span> <span class="dv">0</span>,
            <span class="dt">chamber_aspect_rescale =</span> <span class="dv">0</span>,
            <span class="dt">chamber_number_rescale =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> m_<span class="dv">1</span>,
                   <span class="dt">n =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

<span class="kw">ggplot</span>(line_data) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">data =</span> foram_ready,
              <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> compression_rescale,
                            <span class="dt">y =</span> <span class="kw">as.numeric</span>(dextral) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>),
              <span class="dt">alpha =</span> <span class="fl">0.1</span>,
              <span class="dt">width =</span> <span class="dv">0</span>,
              <span class="dt">height =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> compression_rescale,
                          <span class="dt">y =</span> .value,
                          <span class="dt">group =</span> .draw), 
            <span class="dt">alpha =</span> <span class="fl">0.1</span>,
            <span class="dt">colour =</span> <span class="st">&#39;skyblue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Compression ratio (sd units)&#39;</span>, 
       <span class="dt">y =</span> <span class="st">&#39;Coiling state (1 = dextral)&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_compression-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">line_data &lt;-<span class="st"> </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(<span class="dt">compression_rescale =</span> <span class="dv">0</span>,
            <span class="dt">area_rescale =</span> <span class="dv">0</span>,
            <span class="dt">umbilical_angle_rescale =</span> <span class="dv">0</span>,
            <span class="dt">aperture_aspect_rescale =</span> <span class="dv">0</span>,
            <span class="dt">chamber_aspect_rescale =</span> <span class="kw">seq_range</span>(chamber_aspect_rescale, <span class="dt">n =</span> <span class="dv">100</span>),
            <span class="dt">chamber_number_rescale =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> m_<span class="dv">1</span>,
                   <span class="dt">n =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

<span class="kw">ggplot</span>(line_data) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">data =</span> foram_ready,
              <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> chamber_aspect_rescale,
                            <span class="dt">y =</span> <span class="kw">as.numeric</span>(dextral) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>),
              <span class="dt">alpha =</span> <span class="fl">0.1</span>,
              <span class="dt">width =</span> <span class="dv">0</span>,
              <span class="dt">height =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> chamber_aspect_rescale,
                          <span class="dt">y =</span> .value,
                          <span class="dt">group =</span> .draw), 
            <span class="dt">alpha =</span> <span class="fl">0.1</span>,
            <span class="dt">colour =</span> <span class="st">&#39;skyblue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Chamber aspect ratio (sd units)&#39;</span>, 
       <span class="dt">y =</span> <span class="st">&#39;Coiling state (1 = dextral)&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_chamber_aspect-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">line_data &lt;-<span class="st"> </span>foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data_grid</span>(<span class="dt">compression_rescale =</span> <span class="dv">0</span>,
            <span class="dt">area_rescale =</span> <span class="dv">0</span>,
            <span class="dt">umbilical_angle_rescale =</span> <span class="kw">seq_range</span>(umbilical_angle_rescale, <span class="dt">n =</span> <span class="dv">100</span>),
            <span class="dt">aperture_aspect_rescale =</span> <span class="dv">0</span>,
            <span class="dt">chamber_aspect_rescale =</span> <span class="dv">0</span>,
            <span class="dt">chamber_number_rescale =</span> <span class="dv">0</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_fitted_draws</span>(<span class="dt">model =</span> m_<span class="dv">1</span>,
                   <span class="dt">n =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>()

<span class="kw">ggplot</span>(line_data) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">data =</span> foram_ready,
              <span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> umbilical_angle_rescale,
                            <span class="dt">y =</span> <span class="kw">as.numeric</span>(dextral) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>),
              <span class="dt">alpha =</span> <span class="fl">0.1</span>,
              <span class="dt">width =</span> <span class="dv">0</span>,
              <span class="dt">height =</span> <span class="fl">0.05</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> umbilical_angle_rescale,
                          <span class="dt">y =</span> .value,
                          <span class="dt">group =</span> .draw), 
            <span class="dt">alpha =</span> <span class="fl">0.1</span>,
            <span class="dt">colour =</span> <span class="st">&#39;skyblue&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Umbilical angle (sd units)&#39;</span>, 
       <span class="dt">y =</span> <span class="st">&#39;Coiling state (1 = dextral)&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_umbilical_angle-1.png" width="672" /></p>
</div>
<div id="checking-model-adequacy" class="section level2">
<h2><span class="header-section-number">7.8</span> Checking model adequacy</h2>
<p>Now that we’ve fit out model and taken a look at our parameter estimates, we should really investigate if our model is an adequate description of our data. How confident can we be in interpreting the results of our analysis?</p>
<p>For our linear regression models, we could compare properties like the density or median of the response to a distribution of those measures from our poseterior predictive distribution. With binary data it isn’t that easy. Instead, we have to explore other ways of summarizing our model’s adequacy.</p>
<p>A very simple measure of performance with binary data is to measure our model’s <strong>accuracy</strong> – does our model predict a 1 when our data is a 1? A 0 when the data is a 0? While other metrics exist (like <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC/AUC</a>, we’re going to cover accuracy in this lesson.</p>
<p>We can plot the (mis)matches like we did when we explored our prior predictive distribution and create a visual confusion matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot predicted versus observed for 4 posterior predictive draws</span>
foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predicted_draws</span>(<span class="dt">model =</span> m_<span class="dv">1</span>, <span class="dt">n =</span> <span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="kw">aes</span>(<span class="dt">x =</span> .prediction, <span class="dt">y =</span> dextral),
              <span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Predicted coiling state&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;Observed coiling state&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>.draw) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_confusion-1.png" width="672" /></p>
<p>We can also calculate accuracy as the percent of predictions that are correct for a single posterior predictive draw, repeat this 100s of times, and present a posterior distribution of accuracies.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculate accuracy, look at distribution from 100 posterior preditive draws</span>
foram_ready <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">add_predicted_draws</span>(<span class="dt">model =</span> m_<span class="dv">1</span>, <span class="dt">n =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">accurate =</span> <span class="kw">if_else</span>(dextral <span class="op">==</span><span class="st"> </span>.prediction, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(.draw) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">accuracy =</span> <span class="kw">sum</span>(accurate) <span class="op">/</span><span class="st"> </span><span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_density</span>(<span class="kw">aes</span>(<span class="dt">x =</span> accuracy)) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&#39;Posterior Predictive Accuracy&#39;</span>, <span class="dt">y =</span> <span class="st">&#39;&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="ot">NULL</span></code></pre></div>
<p><img src="paleo_book_files/figure-html/side_model_accuracy-1.png" width="672" /></p>
<p>Our model appears to correctly guess the coiling state of any observation about 62-63% of the time. Would you be confident in a model that is only this accurate? How might we improve upon this model? Is this even a valid scientific question worth continuing with?</p>
</div>
<div id="summary-6" class="section level2">
<h2><span class="header-section-number">7.9</span> Summary</h2>
<p>This lesson introduced logistic regression and many of its gory details. We introduced the concept of a mapping or link function to connect our linear predictor to a parameter defined over a different range of values (e.g. <span class="math inline">\(\text{logit}(p) = X\beta\)</span>). We spent a lot of time discussing how to interpret the regression coefficients of a logistic regression, introducing the difference between relative and absolute effects and emphasizing interpreting coefficients on the probability scale. We explored the prior predictive distribution of our logistic regression model in order to get a handle on the implications of our priors. We also covered visualizing our regression coefficients and the effects of our predictors on the probability of our outcome (shell coiling direction). Finally, after fitting out logistic regression model, we measured our models adequacy using accuracy as a measure of performance.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interactions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="poisson-regression-and-others-glms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": [["analytical_paleobiology.pdf", "PDF"], ["analytical_paleobiology.epub", "EPUB"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
