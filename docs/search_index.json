[
["index.html", "Analytical Paleobiology Preface", " Analytical Paleobiology Peter D Smits 2019-03-18 Preface This book is a series of tutorials on data analysis with examples drawn from paleobiology, macroevolution, and macroecology. Each chapter of this book can act as a 2-hour tutorial, with each lesson building on the previous ones. I emphasize Bayesian data analysis approaches throughout this text. Parameter inference is done using the brms package which is a flexible tool for implementing Stan-based models in R. This book uses the tidyverse collection of R packages with a particular emphasis on dplyr, ggplot2, and purrr. Other tidyverse packages are used as necessary (e.g. modelr). Management and processing of posterior estimates, as well as some some aspects of visualization, is done using the tidybayes package. The pacman package is used throughout to ensure that all packages are both installed and loaded into namespace. The here package is used to ensure safe file paths. I attempt to stick to the tidyverse style guide as much as possible. A lot of material in this book is derived from material and examples presented in Statistical Rethinking by Richard McElreath, Bayesian Data Analysis 3 by Gelman et al., and Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. Additionally, some of the code used in this book is derived from this rewriting of Statistical Rethinking. "],
["managing-and-processing-data-from-the-paleobiology-database.html", "1 Managing and Processing Data From the Paleobiology Database 1.1 Objectives 1.2 Reading 1.3 Introduction 1.4 Getting data 1.5 Processing data 1.6 Binning observations 1.7 Sharing data 1.8 Summary", " 1 Managing and Processing Data From the Paleobiology Database 1.1 Objectives Introduce the data stored in the Paleobiology Database. Learn how to programatically download PBDB data. Introduce tidy data and some good practices when managing data. Learn how to make PBDB cleaner and tidier library(pacman) p_load(tidyverse, janitor, knitr, kableExtra) theme_set(theme_bw()) 1.2 Reading The following materials are recommended pre-readings before starting this section. You do not have to read all of them, just pick at least one. Verde Arregotia et al. 2018 “Good practices for sharing analysis-ready data in mammalogy and biodiversity research” Hystrix, the Italian Journal of Mammalogy. Wilson et al. 2017 “Good enough practices in scientific computing” PLoS Computational Biology. Bryan “Project oriented workflow” tidyverse.org. Bryan “Zen and aRt of Workflow Maintenance” talk. Bryan “Code Smells and Feels” talk. Bryan 2017 “Excuse me, do you have a moment to talk about version control?” PeerJ. Wickham 2014 “Tidy Data”. 1.3 Introduction Any project you work on as a scientist has multiple parts: data, documentation, reports, code, etc. Managing and keeping track of these parts is not a simple task. Today we will discuss a small part of this process: data wrangling and sharing using the tidyverse set of packages and syntax. This lesson is in three parts: getting data, processing data, and sharing data. This tutorial assumes a fair amount of familiarity with the tidyverse, in particular dplyr. For a tutorial on using dplyr and purrr I recommend - R for Data Science - https://dplyr.tidyverse.org/articles/dplyr.html. 1.4 Getting data One of the greatest resources in paleobiology is the aptly named Paleobiology Database, or PBDB for short. The PBDB is an internet repository of fossil occurrences, collections, taxonomic opinions, and lots of other information and is freely avalibale to everyone. The standard way to access information in the PBDB is through the classic Download Generator webform. Downloading data using forms like this makes replicating previous PBDB downloads very difficult – with so many manual options, it is hard to easily record them all or share them with someone else. The modern Download Generator (at time of this writing) has one major feature for increasing the reproducibility of downloads – a URL. Every option updates a URL that calls our data from the PBDB. This URL is then a direct link to the results of that call – giving the URL to a friend means they can download the same dataset. Play around with the download options and see how the URL changes. That URL is a call to the PBDB’s API, which is the data service for interfacing with the material stored in the underlying database. This means we can share the URL along with our study so that other researchers can make the same data call. The API documentation leaves something to be desired, but as you interact with the docs and start writing your own API calls, it should become clearer what some of the terms mean. A fossil occurrence is the core data type of the PBDB and probably the most important data type in all of paleobiology – the unique recording of an organism at a particular location in space and time. Normally we want a list of fossil occurrences that correspond to our study system or period of time. For data output from the PBDB for occurrences, each row is an observation and each column is a property of that fossil or metadata corresponding to its collection, identification, and entering in the PBDB. We are going to focus on downloading information about fossil occurrences. Here are a few example URLs which make calls to the PBDB API. Use the API documentation to discern and describe the differences between the different calls. You can even visit these websites if you want – it will reveal the plain-text file of the data. https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;show=all https://paleobiodb.org/data1.2/occs/list.json?base_name=Cetacea&amp;interval=Miocene&amp;taxon_status=valid&amp;show=all https://paleobiodb.org/data1.2/occs/list.txt?base_name=Cetacea&amp;interval=Miocene&amp;idreso=genus&amp;show=all https://paleobiodb.org/data1.2/occs/taxa.txt?base_name=Cetacea&amp;interval=Miocene&amp;show=attr The best part of using a URL based call is that we can embed them in our R scripts. Here is a simple example (note I’m suppressing warnings here, so don’t get scared when you see a lot of them): url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) The carnivora object now has the information from our PBDB call in data.frame form. By using a direct API call to the PBDB instead of relying on a previously downloaded file our analyses can instaly be updated when new data is added to the PBDB. Because the URL points directly to a CSV (or JSON) file, we don’t have to save a copy of the data to our local machine and we can instead just have it live in memory during our analyses – though you might want to download and store the data every so often (e.g. write_csv()) so you can work offline or share you data with a less savvy colleague. I find tibbles easier to process than data.frame-s, so API calls in my own code tend to look like this: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() If you play around with the carnivora object you’ll notice it has TONS of columns – 118! Each of these columns records some bit of information about that fossil – taxonomic identity, location, source, enterer, etc. You can check the API documentation for a description of each column. Frustrating many of these fields might be empty or inconsistently entered – I’m looking at you lithology1 and environment. This inconsistency is the source of some of the warning messages you’ll get form readr functions when trying to parse PBDB data because columns will be inconsitently typed. Additionally, a lot of our fossils might not be identified to the species or even genus level, or are not identified with confidence. This serves as an important point about the PBDB: the data isn’t perfect. This means that the next step of any analysis of PBDB is “cleaning” or “tidying” our data until we can actually analyze it! WORD OF CAUTION: If an analysis relies entirely on the PBDB’s API to filter the data, then that analysis should be considered suspect because the authors did not actually consider the quality or nature of their data in their analysis. 1.5 Processing data Processing PBDB data is not a pretty task but it is extremely important and necessary before any analysis – it build familiarity with the data and ensures that the data you are analyzing is the data you actually want to analyze. The dplyr package from the tidyverse is a collection of tools for processing data.frame objects, making it easier to get our data into a usable form. When combined with the pipe operator (%&gt;%) from magrittr, we can write clear, human readable code for cleaning our data. Example filters we might consider - identified precisely – no ambiguous or imperfect “calls” - identified to genus or better - paleocoordinates (latitude AND longitude) - body fossils - collected after a certain date We might also want to standardize the form of our column names. Capital letters, spaces, and punctuation are all really frustrating to code around. Lucky for us, our data is already considered “tidy” because each row is an observation and each column is a variable. This does not mean our data is ready for analysis, however. Our job as analysts is to process and filter our data till we believe every observation is ready to be analyzed. Is every observation up to snuff? Or are there errors encoded in our data? For example, let’s filter out the imprecise fossils, fossils no identified to at least the genus level, and fossils lacking paleocoordinates. Let’s also make sure all the variable names have the same logic (they are all already useable, but this is a good habit to get into!) carnivora_filter &lt;- carnivora %&gt;% janitor::clean_names() %&gt;% # standardizes names filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) This new tibble, carnivora_filter, is a subset of the ordinal data that should follow the rules we’ve laid out in the URL-based API call and the few lines of R code. If we gave the URL and this code snippet to someone else, they could reproduce all of the “rules” governing our dataset. The accepted_* variables in PBDB data correspond to the accepted, or best, identification of a fossil. Differences between identified_* and accepted_* variables are commonly due to re-identification or changes in taxonomy. While this is really convenient on its face, sometimes the accepted species names assumes too much confidence in the identification. For example, let’s take a close look at a few records. carnivora_table &lt;- carnivora_filter %&gt;% select(identified_name, identified_rank, accepted_name, accepted_rank) %&gt;% slice(50:60) knitr::kable(carnivora_table) %&gt;% kableExtra::kable_styling() identified_name identified_rank accepted_name accepted_rank Phlaocyon minor species Phlaocyon minor species Desmocyon thomsoni species Desmocyon thomsoni species Promartes lepidus species Promartes lepidus species Promartes lepidus species Promartes lepidus species Daphoenodon notionastes species Daphoenodon notionastes species Daphoenodon n. sp. notionastes species Daphoenodon notionastes species Phlaocyon achoros species Phlaocyon achoros species Cynarctoides lemur species Cynarctoides lemur species Cormocyon cf. copei species Cormocyon copei species Megalictis ? sp. genus Megalictis genus Phlaocyon ? cf. annectens species Phlaocyon annectens species In most cases there is good correspondence between the identified name and the accepted name, but not always. For example, the ninth of this table corresponds to a fossil identified as “Cormocyon cv. copei” but is given the accepted name of “Cormocyon copei” – an identification that is arguably overconfident. But does it matter? That’s up to you and your research, but let’s assume it does for this tutorial. How do we resolve this and downgrade these overconfident identifications? The simplest way might be to downgrade any identified names that include punctuation and non-character symbols to just there genus. After all, “cf.”, “sp.” and “n. sp.” all involve punctuation. But how do we deal with text information? Turns out there is a whole special language for dealing with text called regular expressions, or RegEx for short. RegEx are sequences of characters that help us match specific patterns in text. In this example, I’m using a specialized bit of RegEx to identify all cases where there is punctuation present in the identified name – I don’t not care where the punctuation is, just that there is punctuation. To do this, I’m going to be using functions from the stringr package which provide for easier interaction with text and regular expressions than using the factions available base R. I find RegEx unintuitive before I use them as they are not intuitive, so don’t worry too much if you find regular expressions bewildering when you start using them. I always spend a lot of time on Google and StackExchange figuring out the correct RegEx I need for the task at hand. carnivora_clean &lt;- carnivora_filter %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) carnivora_clean %&gt;% select(identified_name, accepted_name, improve_name) %&gt;% slice(50:60) %&gt;% knitr::kable() %&gt;% kable_styling() identified_name accepted_name improve_name Phlaocyon minor Phlaocyon minor Phlaocyon minor Desmocyon thomsoni Desmocyon thomsoni Desmocyon thomsoni Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Promartes lepidus Daphoenodon notionastes Daphoenodon notionastes Daphoenodon notionastes Daphoenodon n. sp. notionastes Daphoenodon notionastes Daphoenodon Phlaocyon achoros Phlaocyon achoros Phlaocyon achoros Cynarctoides lemur Cynarctoides lemur Cynarctoides lemur Cormocyon cf. copei Cormocyon copei Cormocyon Megalictis ? sp. Megalictis Megalictis Phlaocyon ? cf. annectens Phlaocyon annectens Phlaocyon If we really wanted to be slick, we could combine all of the above into a single block. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) 1.6 Binning observations Fossil occurrences in the PBDB have temporal information such as geologic stage, or a rough numerical age (max_ma, min_ma). The nature of the fossil record means that we do not have exact ages for anything, instead we have ranges. The numerical age of a fossil occurrence is bounded between a max and a min. This uncertainty in age presents a lot of problems that we have to deal with in our analyses, especially if the temporal order of our fossils matters to our question! An extremely common way to overcome this uncertainty is to coarsen the resolution of our fossils by binning them – assigning similarly aged fossils to a shared temporal unit. Each temporal bin can be said to have a “width” – the length of time covered by that bin. In our example, we may want to track diversity over time. We are going to do this by counting the number of unique genera present in our time bins. To do this, we have to determine how many bins there are and to which bin each fossil belongs. The age of each fossil, however, is a range and not a single value. We could use the midpoint of this range to assign each fossil to a bin, but what if the age range of some fossils is much larger than our bin width? First, let’s take a look at the amount of uncertainty there is in the age estimates of our fossil occurrences. carnivora_clean %&gt;% mutate(age_range = abs(max_ma - min_ma)) %&gt;% ggplot(aes(x = age_range)) + geom_histogram() + labs(x = &#39;Age range (My)&#39;, y = &#39;Count&#39;) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can see that a lot of our fossils have age uncertainty of 5 million years or less, and a few of them have large ranges of 10 million years or more. Fossils with age ranges greater than 10 million years are potentially suspect or at least are not high quality – certainly trying to assign them to a single 2 million year bin isn’t going to be ideal as it adds confidence where there is none. In your own analyses you might consider these situations on a case-by-case basis or try and find more information from other sources, but for purposes of this tutorial we will exclude those occurrences who’s age ranges are 10 million years or greater. We can dither over an “optimal” bin width for our data, but for the purpose of this exercise let’s just assign all our fossil occurrences to 2 million year bins. Because binning data is so common, I’ve written a function, bin_ages() to do this. I present it here and will use it to bin our data. The bin_ages() function follows that convention that the youngest bin is bin 1. I’ve written documentation for this function using roxygen formatting. Feel free to modify it or write your own. This function requires you to assign a single age to the fossil, after which it is bined. In the example that follows, I calculate the midpoint age of each fossil and proceed from there. #&#39; Break time data up into bins #&#39; #&#39; Have fun with this. basic rules. greater than equal to base, less than top. #&#39; #&#39; @param x vector of ages #&#39; @param by bin width #&#39; @param age logical bin age returned, not number (default FALSE, return bin number) #&#39; @return vector of bin memberships #&#39; @author Peter D Smits &lt;peterdavidsmits@gmail.com&gt; bin_ages &lt;- function(x, by = NULL, number = NULL, age = FALSE) { if(is.null(by) &amp; is.null(number)) { return(&#39;no scheme given. specify either bin width or number of bins.&#39;) } if(!is.null(by) &amp; !is.null(number)) { return(&#39;too much information. specify either bin width OR number of bins, not both.&#39;) } # range to bin top &lt;- ceiling(max(x)) bot &lt;- floor(min(x)) # create bins if(!is.null(by)) { unt &lt;- seq(from = bot, to = top, by = by) } else if(!is.null(number)) { unt &lt;- seq(from = bot, to = top, length.out = number + 1) } # bin top and bottom unt1 &lt;- unt[-length(unt)] unt2 &lt;- unt[-1] # assign memberships uu &lt;- map2(unt1, unt2, ~ which(between(x, left = .x, right = .y))) # what if we want the &quot;age&quot; of the bin, not just number? if(age == TRUE) { unt_age &lt;- map2_dbl(unt1, unt2, ~ median(c(.x, .y))) } # create output vector y &lt;- x for(ii in seq(length(uu))) { if(age == FALSE) { y[uu[[ii]]] &lt;- ii } else if(age == TRUE) { y[uu[[ii]]] &lt;- unt_age[ii] } } y } Let’s use this function to bin our data. Notice how I first calculate the midpoint age for each fossil and use that derived quantity to assign each occurrence to a temporal bin. carnivora_bin &lt;- carnivora_clean %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2)) carnivora_bin %&gt;% summarize(bin_number = n_distinct(bin)) ## # A tibble: 1 x 1 ## bin_number ## &lt;int&gt; ## 1 9 Ok, so now we have a column bin that identifies the temporal bin that each occurrence belongs to. The quick summary at the bottom demonstrates that we have broken our data into 9 bins of equal length. The limit here is that our bins are identified by their number and not their “age”. Luckily, the age parameter of the bin_ages() function that changes the output from bin number to bin age. Here it is in use. carnivora_bin &lt;- carnivora_bin %&gt;% mutate(bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) # take a look carnivora_bin %&gt;% select(mid_ma, bin, bin_age) %&gt;% slice(1:10) %&gt;% knitr::kable(.) %&gt;% kableExtra::kable_styling() mid_ma bin bin_age 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.0165 2 8 7.6040 2 8 14.8950 5 14 13.7890 5 14 13.7890 5 14 As before, we can combine all of these operations into a set of piped statements. url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) %&gt;% filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) ## Warning: Duplicated column names deduplicated: &#39;cc&#39; =&gt; &#39;cc_1&#39; [47] ## Parsed with column specification: ## cols( ## .default = col_character(), ## occurrence_no = col_double(), ## reid_no = col_double(), ## flags = col_logical(), ## collection_no = col_double(), ## identified_no = col_double(), ## accepted_attr = col_logical(), ## accepted_no = col_double(), ## max_ma = col_double(), ## min_ma = col_double(), ## ref_pubyr = col_double(), ## reference_no = col_double(), ## plant_organ = col_logical(), ## plant_organ2 = col_logical(), ## abund_value = col_double(), ## lng = col_double(), ## lat = col_double(), ## collection_subset = col_logical(), ## paleolng = col_double(), ## paleolat = col_double(), ## zone = col_logical() ## # ... with 25 more columns ## ) ## See spec(...) for full column specifications. ## Warning: 1048 parsing failures. ## row col expected actual file ## 1071 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1071 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1072 regionalbed 1/0/T/F/TRUE/FALSE 3 &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## 1073 regionalsection 1/0/T/F/TRUE/FALSE ValAH &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; ## .... ............... .................. ...... ............................................................................................. ## See problems(...) for more details. 1.7 Sharing data So far we have generated two “kinds” of data tables: the raw data output from the PBDB that is returned from our URL-based API call, and the cleaned data we’ve crafted with a bit of code. Both of these datasets are extremely important and should be shared with the audience. There are two ways to share the raw data associated with our study: the URL-based API call, and a spreadsheet of the downloaded information. Because the information on the PBDB updates over time, an API call made today might not yield an identical dataset. Earlier I hailed this as fantastic, which it is, but it is also limiting – someone might not be able completely reproduce your analysis using just this information because the data could be different. The API call is useful for improving and expanding on previous analyses, but by itself is not enough to reproduce your analysis. You also need to share the raw data download so that your complete analysis, including filtering and cleaning, is reproducible. You probably also want to save a local copy of your filtered and cleaned dataset so you don’t have to re-run your cleaning scripts all the time. It also means you can separate your cleaning from the rest of your analysis. You’ll end up with multiple R scripts and multiple datasets – that’s good. For example, my projects tend to have multiple subdirectories: R/, data/, and results/. In the R directory, I’ll have multiple scripts – one for loading and cleaning data, one for visualizing this cleaned data, and at least one for analyzing the cleaned data. I save the raw data to the data/ directory, and the cleaned data and figured in the results/ directory. Here is a quick example of what I mean without referencing subdirectories: url &lt;- &#39;https://paleobiodb.org/data1.2/occs/list.txt?base_name=Carnivora&amp;interval=Miocene&amp;show=full&#39; carnivora &lt;- read_csv(file = url) carnivora_clean &lt;- carnivora %&gt;% as_tibble() %&gt;% clean_names() %&gt;% filter(accepted_rank %in% c(&#39;genus&#39;, &#39;species&#39;), # either is good !is.na(paleolng), !is.na(paleolat)) %&gt;% mutate(improve_name = if_else(str_detect(identified_name, pattern = &#39;[[:punct:]]&#39;), true = genus, false = accepted_name)) filter(abs(max_ma - min_ma) &lt; 10) %&gt;% mutate(mid_ma = (max_ma + min_ma) / 2, bin = bin_ages(mid_ma, by = 2), bin_age = bin_ages(mid_ma, by = 2, age = TRUE)) write_csv(carnivora, &#39;carnivora_raw.csv&#39;) write_csv(carnivora_clean, &#39;carnivora_clean.csv&#39;) For more information and advice on how to organize your projects, check out Jenny Bryan’s excellent material on project oriented workflows, linked above. 1.8 Summary In this lesson we introduced the PBDB API and how to include URL-based API calls in our R code. We also covered multiple aspects of cleaning PBDB data including species identifications and temporal binning. Finally, we briefly covered exporting our data, both raw and cleaned, so that our analyses are more reproducible. All of these steps are fundamental in any analysis and form a great deal of the work associated with any analysis problem. "],
["introduction-to-bayesian-data-analysis.html", "2 Introduction to Bayesian data analysis 2.1 Objectives 2.2 Reading 2.3 Learning from data 2.4 Building a model 2.5 Terms and theory 2.6 Bayes’ Theorem 2.7 But how does it work? 2.8 Working with samples 2.9 Summary", " 2 Introduction to Bayesian data analysis Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed. – Gelman et al BDA3: 4. Bayesian data analysis is just a logical procedure for processing information. – McElreath Statistical Rethinking: 12. 2.1 Objectives Learn what a (Bayesian) model actually means. Introduce the logic behind Bayesian updating. Dip our toes into numerical methods using brms. Cover the basics of summarizing a posterior distribution using tidybayes. 2.2 Reading The following materials are recommended pre-readings before starting this tutorial. Chapter 1 “The Golem of Prague” from Statistical Rethinking by Richard McElreath. Chapter 2 “Small Worlds and Large Worlds” from Statistical Rethinking by Richard McElreath. OPTIONAL Chapter 3 “Sampling the Imaginary” from Statistical Rethinking by Richard McElreath. Most of this section is based on material presented in Statistical Rethinking and code material from Solomon Kurz’s rewrite of Statistical Rethinking. library(pacman) p_load(tidyverse, brms, tidybayes) theme_set(theme_bw()) 2.3 Learning from data Bayesian inference is a fancy way of counting and comparing possibilities. As we collect and analyze our data, we learn which possibilities are more plausible than others. The logical strategy is “When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible.” 2.3.1 Counting and plausibility We’re going to use a simple example from Statistical Rethinking to start our thinking about Bayesian analysis. Setup: We’ve got a bag with four marbles. Each marble can be either blue (B) or white (W). We don’t know the composition of the bag. Our goal is to figure out which of possible configuration is most plausible, given whatever evidence we learn about the bag. A sequence of three marbles have pulled from the bag, one at a time, returned to the bag, and then the bag is shaken before drawing another. We draw the sequence [B W B]. How many ways are there to produce this draw? Try first this possible bag composition: [B W W W]. Here is the full table of ways to produce [B W B] given each possible bag composition. conjecture ways to produce [B W B] [W W W W] 0 x 4 x 0 = 0 [B W W W] 1 x 3 x 1 = 3 [B B W W] 2 x 2 x 2 = 8 [B B B W] 3 x 1 x 3 = 9 [B B B B] 4 x 0 x 4 = 0 What happens when we draw another marble from the bag? We update the counts! How do we update the counts? We multiply the prior counts by the new count, with the old counts are acting as our prior counts. Here is an example how we would update our counts if we were to draw an additional [B]. conjecture ways to produce [B] previous counts new count [W W W W] 0 0 0 x 0 = 0 [B W W W] 1 3 3 x 1 = 3 [B B W W] 2 8 8 x 2 = 16 [B B B W] 3 9 9 x 3 = 27 [B B B B] 4 0 0 x 4 = 0 Logically, what we’ve just done can be expressed as plausibility of [B W W W] after seeing [B W B] \\(\\propto\\) ways [B W W W] can produce [B W B] X prior plausibility of [B W W W]. But these are just counts and plausibilities; we want probabilities! First, to make explaining ourselves simpler, let’s define \\(p\\) as the proportion of blue marbles in the bag and \\(D_{new}\\) as our data. We can now rewrite the previous statement as: plausibility of \\(p\\) after \\(D_{new}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility of \\(p\\). We want to standardize the measure of plausibility so that the sum of the plausibilities for all conjectures sums to 1. To standardize, we add up all the products, one for each \\(p\\), then divide each product by the sum of the products. possible combinations \\(p\\) ways to produce data plausibility [W W W W] 0 0 0 [B W W W] 0.25 3 0.15 [B B W W] 0.5 8 0.40 [B B B W] 0.75 9 0.45 [B B B B] 1 0 0 This process is equivalent to plausibility of \\(p\\) after \\(D_{new}\\) = (ways \\(p\\) can produce \\(D_{new}\\) X prior plausibility \\(p\\)) / sum of products Each part of the calculations we’ve done so far correspond directly to quantities in applied probability theory. The conjectured proportion of blue marbles, \\(p\\), is usually called a parameter. The relative number of ways that \\(p\\) can produce the data is usually called the likelihood. The prior plausibility of a specific \\(p\\) is called the prior probability. The updated plausibility of any specific \\(p\\) is called the posterior probability. 2.4 Building a model Bayesian inference is made easier by working with probabilities instead of counts, but this makes everything look a lot harder. Let’s explore this with a new problem. Setup: We’ve a globe with land and water sections. We want to know how much of the globe is water. Our strategy is to throw the globe up, catch it, and note the surface under the right index finger. Let’s do this 9 times. d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;)) # then rephrase in terms of trials and count of water (d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;))) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 To get the logic machinery working, we need to make some assumptions. These assumptions constitute the our model. An ideal design loop for developing a Bayesian model has three steps: Data story Describe aspects of the underlying reality and the sampling process. Translate this description into a formal probability model. Acts as a framework for interpretation, but still just a story. Helps with realizing additional questions that must be answered as hypotheses are frequently vague. Updating Bayesian models begin with a set of plausibilities assigned to each possibility (Prior). Update those plausibilities based on the data to give posterior plausibility. Evaluate Certainty is no guarantee that the model is good or accurate. Supervise and critique your model! Check model’s adequacy for some purpose, or in light of stuff we don’t know. 2.4.1 A data story How did the data come to be? A data story is a description of the aspects of the reality underlying our data, including the sampling process. This story should be sufficient enough to specifying an algorithm to simulate new data. Write out the data story for the globe tossing activity. 2.4.2 Bayesian updating A Bayesian model begins with one set of plausibilities assigned to each possible result: the prior plausibilities. These values are updated in light of data to produce our posterior plausibilities. This process is called Bayesian updating. Here is an illustration of Bayesian updating applied to our globe tossing experiment. The posterior generated at each observation becomes the prior for the subsequent observation. sequence_length &lt;- 50 # how many points to calculate prob for d %&gt;% expand(n_trials, # for each value of ... p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% left_join(d, by = &quot;n_trials&quot;) %&gt;% group_by(p_water) %&gt;% # lag is the *previous* value mutate(lagged_n_success = lag(n_success, k = 1), lagged_n_trials = lag(n_trials, k = 1)) %&gt;% ungroup() %&gt;% # if first time, flat prior. # otherwise use previous posterior as new prior. mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), strip = str_c(&quot;n = &quot;, n_trials), # which draw is it? likelihood = dbinom(x = n_success, size = n_trials, prob = p_water)) %&gt;% # calculate likelihood for current draw # normalize the prior and the likelihood, making them probabilities group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + facet_wrap(~strip, scales = &quot;free_y&quot;) Side note on sample size: Bayesian estimates are valid and interpretable at any sample size. This fact is very much in contrast to the folk-wisdom around a minimum number of samples needed that you’ll hear in non-Bayesian contexts. In non-Bayesian contests, statistical inference is justified by behaviour at large samples sizes, called asymptotic behaviour. The reason Bayesian estimates are always valid and interpretable comes down to the prior. If the prior is bad, the resulting posterior could be misleading. Regardless, all estimates in either context are based on assumptions – the assumptions made during Bayesian analysis are just more obvious and capable of being directly interrogated. 2.4.3 Evaluate Our model is learning from a “small world” of data. If there are important differences between the model and reality, then there is no guarantee of “large world” performance. Certainty is no guarantee that the model is good, either. As the amount of data increases, our model will become increasingly sure of the proportion of water. This sureness happens even if the model is seriously misleading because our estimates are conditional on our model. What is happening is that, given a specific model, we can be sure that plausible values are within a narrow range. It is important that you supervise and critique your model, and not just assume it worked or is correct because your code did not return an error. Anything that is not included in our model might not affect our inference directly, but might affect it indirectly because of that unmodeled dependence. For example, we currently are assuming that the order the data was collected in doesn’t matter (exchangeable), but what if the order of the observations actually did matter? Check the model’s inferences in light of aspects of the data that you know but the model doesn’t know about. This part of data analysis is an inherently creative endeavor that is up to you (the analyst) and your scientific community. Robots can’t do this step for you. The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right as they are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions. Instead, our objective should be to test the model’s adequacy for some purpose. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you already know as a domain expert and compare this knowledge to your model’s estimates/predictions; if there is a conflict you should update your model (likelihood and/or prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently scientific questions, not statistical questions. As I said earlier, robot’s can’t do this step for you. 2.5 Terms and theory Common notation \\(y\\) observed data. \\(\\tilde{y}\\) unobserved data. \\(X\\) explanatory variables, covariates, etc. \\(\\Theta\\) parameters of interest. \\(p(\\cdot|\\cdot)\\) conditional probability distribution. \\(p(\\cdot)\\) marginal probability distribution. \\(p(\\cdot,\\cdot)\\) joint probability distribution. \\(Pr(\\cdot)\\) probability of an event. Likelihood Specifies the plausibility of data mathematically. Maps each conjecture onto the relative number of ways the data could occur, given that possibility. Sometimes written \\(p(y | \\Theta)\\) or \\(L(\\Theta | y)\\). Parameters Adjustable inputs. 1+ quantities we want to know about. Represent the different conjectures for causes or explanations of the data Difference between data and parameters is fuzzy and exploitable in Bayesian analysis –&gt; advanced topic Prior Every parameter you are trying to estimate must be provided a prior. The “initial conditions” of the plausibility of each possibility. Which parameters values do we think are more plausible than others? Constrain parameters to reasonable ranges. Express any knowledge we have about that parameter before any data is observed. An engineering assumption; helps us learn from our data. Regularizing or weakly informative priors are conservative in that they tend to guard against inferring strong associations between variables –&gt; advanced topics. Sometimes written \\(p(\\Theta)\\). Posterior Logical consequence of likelihood, the set of parameters to estimate, and priors for each parameter. The relative plausibility of different parameter values, conditional on the data. Sometimes written \\(p(\\Theta | y)\\). 2.6 Bayes’ Theorem The logic defining the posterior distribution is called Bayes’ Theorem. The theorem itself is an intuitive result from probability theory. First, describe the model and data as a joint probability. \\[ \\begin{align} p(y, \\Theta) &amp;= p(\\Theta | y) p(y) \\\\ p(y, \\Theta) &amp;= p(y | \\Theta) p(\\Theta) \\\\ \\end{align} \\] Then set equal to each other and solve for \\(p(\\Theta | y)\\): \\[ \\begin{align} p(\\Theta | y) p(y) &amp;= p(y | \\Theta) p(\\Theta) \\\\ p(\\Theta | y) &amp;= \\frac{p(y | \\Theta) p(\\Theta)}{p(y)} \\\\ \\end{align} \\] Et voilà, Bayes’ Theorem. The probability of any particular value of \\(\\Theta\\), given the data, is equal to the product of the likelihood and the prior, divided by \\(p(y)\\). “But what’s \\(p(y)\\)?” you ask. The term \\(p(y)\\) is a confusing one – it can be called the “average likelihood,” “evidence,” or “probability of the data.” The average likelihood here means it is averaged over the prior and its job is to standardize the posterior so it integrates to 1. \\(p(y)\\) is expressed mathematically as: \\[ p(y) = E(p(y | \\Theta)) = \\int p(y | \\Theta) p(\\Theta) d\\Theta \\] \\(E(\\cdot)\\) means to take the expectation, a (weighted) average. Notice also that \\(p(y)\\) is a type of marginal probability distribution; the process of integrating out a term (\\(\\Theta\\)) is called marginalization – we are averaging \\(y\\) over all values of \\(\\Theta\\). Remember also that an integral is like an average over a continuous distribution of values. 2.7 But how does it work? Our model has three parts: likelihood, parameters, and the prior. These values get put into a “motor” that gives us a posterior distribution. The motor goes through the process of conditioning the prior on the data. Turns out that knowing all the rules doesn’t necessarily help us with the calculations. For most interesting models we will ever consider, the necessary integrals in the Bayesian conditioning machinery have no closed form and can’t be calculated no matter how talented you are. Each new parameter effectively adds a new level to the integral used to calculate \\(p(y)\\). Instead, we need to rely on numerical techniques to approximate the mathematics defined in Bayes’ theorem. All of the numerical techniques we use produce only samples from the posterior distribution, not the distribution itself. Luckily, samples from the distribution are easier to work with than the actual distribution – this way we don’t have to do integrals. 2.7.1 Grid approximation One of the simplest conditioning techniques is grid approximation. While most parameters are continuous, we can get a decent approximation of them by considering only a finite grid of parameter values. For any particular value \\(p&#39;\\), compute the posterior probability by multiplying the prior probability of \\(p&#39;\\) by the likelihood of \\(p&#39;\\). Grid approximation is a teaching tool that forces you to really understand the nature of Bayesian updating. You will probably never use it in your actual work mostly because grid approximation scales very poorly as the number of parameters increases. How to do grid approximation: Define a grid (range of values to look at). Calculate the value of the prior at each parameter value of the grid. Compute the likelihood at each parameter value. Compute the unstandardized posterior at each parameter, but multiplying the prior by the likelihood. Standardize the posterior by dividing each value by the sum of all unstandardized values. The number of points you evaluate on the grid determines the precision of your estimates. More points, finer grained posterior. For example, here’s the posterior probability of the percent of water on the globe given our data evaluated at 5 points and 20 points. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # computer product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid), # compute likelihood at each value in grid unstd_posterior = likelihood * prior, # compute product of likelihood and prior posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% # standardize the posterior, so it sums to 1 # make a plot ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) 2.7.2 Markov chain Monte Carlo For many models, grid approximation or quadratic approximation just aren’t good enough. Grid approximation takes too long as your model gets bigger. Quadratic approximation chokes on complex models. Instead, we end up having to use a technique like Markov chain Monte Carlo (MCMC). Unlike grid approximation, where we computed the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, where the frequencies of those values correspond to the posterior plausibilities. Let’s do a quick example where we fit this model using brms. This package acts as an interface with Stan probabilistic programming language which implements Hamiltonian Monte Carlo sampling, a fancy type of MCMC. The function brm() is the workhorse of the brms package, and builds and compiles a Stan model as defined in R. If you’ve used functions like lm() or glm(), some of the syntax should look familiar to you. # this can take a bit as the model compiles globe_brms &lt;- brm(data = list(w = 24), # generate data family = binomial(link = &quot;identity&quot;), # define likelihood distribution formula = w | trials(36) ~ 1, # define parameter prior = prior(uniform(0, 1), class = Intercept), # give prior to parameter control = list(adapt_delta = 0.95), # control sampling behavior --&gt; advanced topic refresh = 0, # silences a bunch of text iter = 2000, # how many draws from posterior? (default) warmup = 1000) # how many draws till we start recording (default = 1/2 iter) print(globe_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.50 0.79 1214 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # look at the posterior distribution of proportion water globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_x_continuous(&#39;proportion water&#39;, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) 2.7.3 Aside on Interpreting Probabilities From Math with Bad Drawings. 2.8 Working with samples In applied Bayesian analysis we rarely work directly with the integrals required by Bayes’ theorem. Most numerical techniques we use, including MCMC methods, produce a set of samples that are individual draws from the posterior distribution. These samples transform a problem in calculus to a problem in data summary. It is easier to count the number of samples within an interval then calculate the integral for that interval. This section also serves as a brief introduction to summarizing posterior samples using tidybayes, which we will continue using in our next lesson. Once our model produces a posterior distribution, the model’s work is done. It is now our job to summarize and interpret that posterior. Common questions we might want to ask include: How much posterior probability lies below some parameter value? How much posterior probability lies between two parameter values? Which parameter value marks the lower 5% of the posterior probability? Which range of parameter values contains 90% of the posterior probability? Which parameter value has the highest posterior probability? 2.8.1 Intervals of defined boundaries What is the posterior probability that the proportion of water is less than 50%? Count the number of observations that are less than 0.5, then divide by the number of samples. globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &lt; 0.5) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.0278 What if we want to know the posterior probability that the proportion of water is between 0.5 and 0.75? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(p_val = sum(b_Intercept &gt; 0.5 &amp; b_Intercept &lt; 0.75) / length(b_Intercept)) ## # A tibble: 1 x 1 ## p_val ## &lt;dbl&gt; ## 1 0.862 2.8.2 Intervals of defined mass An interval of defined mass report two parameter values that contain between them the specified amount of posterior probability, a probability mass. You probably have heard of confidence intervals – an interval of posterior probability is called a credible interval, though the distinction between the terms isn’t terribly important. There are two kinds of intervals of defined mass: percentile (or quantile) interval, and highest posterior density interval. 2.8.2.1 Percentile interval (PI) For example, you may want to know the boundaries of the lower 80% posterior interval. This interval has to start at 0, but where does it stop? How do we calculate the 80th percentile? globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q80 = quantile(b_Intercept, 0.8)) ## # A tibble: 1 x 1 ## q80 ## &lt;dbl&gt; ## 1 0.723 What about the middle 80% interval? There are lots of ways to get this information, so here are two examples. # one way (qu &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% dplyr::summarize(q10 = quantile(b_Intercept, 0.1), q90 = quantile(b_Intercept, 0.9))) ## # A tibble: 1 x 2 ## q10 q90 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.555 0.754 # another way globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% quantile(prob = c(0.1, 0.9)) ## 10% 90% ## 0.5549006 0.7542126 We can also plot this interval on the distribution. # plot the distribution p1 &lt;- globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_line(stat = &#39;density&#39;) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(limits = c(0, 1)) # get back the density line calculation p1_df &lt;- ggplot_build(p1)$data[[1]] # this is messy # shade area under the distribution p1 + geom_area(data = subset(p1_df, x &gt; qu$q10 &amp; x &lt; qu$q90), aes(x=x,y=y), fill = &quot;black&quot;, color = NA) 2.8.2.2 Highest posterior density interval An HPDI is defined as the narrowest interval containing the specified probability mass. There are an infinite of posterior intervals with the same mass, but what if you want that interval that best represents the parameter values most consistent with the data AND you want the densest of these intervals. Here an example of a 50% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% hdi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6137909 0.7158464 # compare to 50% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% pull() %&gt;% qi(.width = 0.5) ## [,1] [,2] ## [1,] 0.6082956 0.7113937 Why are the PI and HPDI not equal? If the choice of interval makes a big difference in your summary, then you probably shouldn’t be using intervals to summarize the posterior! The entire distribution of samples is our actual estimate, these summaries are just there to help digest this wealth of information. Do not only work with simplified versions of our posterior estimates! 2.8.3 Point estimates Sometimes we only want a single point from the distribution. Given an entire posterior distribution, what value should we report? Mechanically, this task is simple – pick a summary (e.g. mean) and go. Conceptually, however, this task is actually quite complex. The Bayesian parameter estimate is the entire posterior distribution, and not just a single number. In most cases, it is unnecessary to choose a single point estimate. It is always better to report more than necessary about the posterior distribution than not enough. The three most common point estimates are the mean, median, and mode – you’re probably already familiar with all three of them. A principled way of choosing among these three estimates is considering as products of different loss functions. Loss functions are an advanced topic we will not cover today but I encourage you to read up on; here’s a good blog entry by John Myles White on the subject. The mode represents the parameter value with the highest posterior probability, or the maximum a posteriori estimate (MAP). In frequentist contexts, the maximum likelihood estimate is the equivalent to mode of the likelihood function. Calculating the mode of a distribution is an optimization problem and isn’t always easy. Luckily, tidybayes has a function which gets us a mode (and other information) from the posterior samples. Here is a code snippet that gives a mode and a middle 95% percentile interval: globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mode_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.662 0.497 0.794 0.95 mode qi We could also report a mean or median. Here are some ways to do get these estimates along with some kind of interval: # mean with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.497 0.794 0.95 mean qi # mean with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% mean_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.658 0.499 0.795 0.95 mean hdci # median with 95% PI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_qi() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.661 0.497 0.794 0.95 median qi # median with 95% HPDI globe_brms %&gt;% spread_draws(b_Intercept) %&gt;% select(b_Intercept) %&gt;% median_hdci() ## # A tibble: 1 x 6 ## b_Intercept .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.661 0.499 0.795 0.95 median hdci Usually it is better to communicate as much about the posterior distribution as you can. 2.9 Summary So far we’ve introduced the conceptual mechanism driving Bayesian data analysis. This framework emphasizes the posterior probability distribution, which is the logical compromise between our previous information and whatever new information we’ve gained (given our model). Posterior probabilities state the relative plausibility of each conjectured possibility that could have produced the data, given our model. These plausibilities are updated in light of observations, a process known as Bayesian updating. We’ve defined the components of a Bayesian model: a likelihood, one or more parameters, and a prior for every parameter. The likelihood defines the plausibility of the data, given a fixed value for the parameters. The prior provides the initial plausibility of each parameter value, before accounting for the data. These components, when processed through Bayes’ Theorem, yield the posterior distribution. Many of the actual calculations necessary to yield the posterior distribution have no closed-form solution and must instead be approximated using numerical methods. We covered grid approximation as a gentle introduction to sampling. We also fit our basic model using brms, which uses Stan’s HMC engine to condition our estimates on the data. Given the posterior samples from our model fit, we also covered basic methods for summarizing a posterior distribution such as intervals and point estimates. "],
["reg-intro.html", "3 Introduction to linear regression 3.1 Objectives 3.2 Reading 3.3 Linear regression 3.4 Adding a predictor to the mix 3.5 Interpreting the model fit 3.6 Summary", " 3 Introduction to linear regression 3.1 Objectives Set up a linear regression model. Interpret the parameters of a regression model. Communicate a model descriptions to others. Fit regression models in brms. Basics of summarizing and visualizing a model fit. library(pacman) p_load(tidyverse, modelr, brms, tidybayes, here) theme_set(theme_bw()) 3.2 Reading The following materials are recommended pre-readings before starting this tutorial. Chapter 4 “Linear Models” from Statistical Rethinking by Richard McElreath. OPTIONAL Chapter 3 “Sampling the Imaginary” from Statistical Rethinking by Richard McElreath. OPTIONAL Chapter 3 “Linear regression: the basics” from Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. 3.3 Linear regression Linear regression refers to a large family of statistical models which attempt to learn about the mean and variance of some measurement, using an additive combination of other measures. Linear regression is a descriptive model that corresponds to many different processes. Normally regression is presented as a way of representing the relationship between two or more variables. I believe this description obscures the interpretation and meaning of regression results. Instead, we’re going to think of linear regression as a method that summarizes how the average or expected values of a numerical outcome vary as a linear functions of one or more predictors. Regression can be used to predict an outcome given a linear function of those predictors, and regression coefficients can be thought of as comparisons across predicted values or as comparisons among averages in the data. A regression coefficient describes the expected change in the response per unit change in its predictor. Linear regression uses a Gaussian/Normal distribution to describe the distribution of our measurement of interest. Like any model, linear regression is not universally applicable. But linear regression is pretty foundational in statistics because once you can build and interpret a linear regression model, it is easy to move on to other types of regression for when things aren’t Normal. Models of normally distributed data are very common: t-test, single regression, multiple regression, ANOVA, ANCOVA, MANOVA, MANCOVA, etc. All of these models are functionally equivalent. Learning and understanding each of these special cases is a lot of unnecessary work; instead, we’re going to focus on a general modeling strategy that subsumes all of these special cases as just variations on a theme. Additionally, we’re going to cover a statistical notation for communicating a model that encodes all of our assumptions and presents them clearly for other readers (including yourself in the future). 3.3.1 Talking about models Here are the choices encoded in a model description: Outcome variable or variables that we hope to predict or understand (\\(y\\)). Likelihood distribution that defines the plausibility of the individual observations. Predictors or covariates – a set of other measurements that we hope to use to predict or understand the outcome (\\(X\\)). Relation between the shape of the likelihood distribution (e.g. location and scale) to the predictor variables. The nature of this relation forces us to define all the parameters of the model. Priors for all of the parameters in the model. Here’s the globe tossing model from last week \\[ \\begin{align} w &amp;\\sim \\text{Binomial}(n, p) \\\\ p &amp;\\sim \\text{Uniform}(0, 1). \\\\ \\end{align} \\] The \\(\\sim\\) symbol indicates a stochastic relationship. A stochastic relationship means that the variable or parameter is mapped onto a distribution of values. It is stochastic because no single instance of the variable on the left is known with certainty. This relationship is probabilistic as some values are more plausible than others, though there are many plausible values under any model. Can you identify the various parts of this model description? This notation allows us to specify and communicate our model clearly so that other people can understand what we’re doing. This language is general, and can be used to describe all model types. It is ok if you do not immediately understand this notation; that’s a normal part of learning. By continuing to use this notation, we will build familiarity with this syntax. 3.3.2 Growing a regression model We’re going to use a real dataset of invertebrate valve sizes and slowly build up a regression model to describe differences between taxonomic groups. We will begin with a proto-regression model and then add predictors. The dataset we’re going to have fun with as part of today’s activities is the Bivalve and Brachiopod body size data from Payne et al. 2014 ProcB. Their data includes location, age, type, and valve length. Load the “occurrence” tab-delimited file and start exploring the distribution of body sizes. # `here` allows us to grab from subdirectory without formally specifying path # allows portability r &lt;- read_tsv(here(&#39;data&#39;, &#39;payne_bodysize&#39;, &#39;Occurrence_PaleoDB.txt&#39;)) ## Parsed with column specification: ## cols( ## taxon_name = col_character(), ## pbdb_collection_no = col_double(), ## p_lat = col_double(), ## p_lng = col_double(), ## int_midpoint = col_double(), ## taxon = col_character(), ## sub = col_character(), ## size = col_double() ## ) # modify raw data for use # genus can occur 1+ times (d &lt;- r %&gt;% group_by(taxon_name, taxon) %&gt;% dplyr::summarize(size = mean(size)) %&gt;% # is this always a good idea? mutate(size_log = log(size)) %&gt;% # log transform ungroup()) ## # A tibble: 3,980 x 4 ## taxon_name taxon size size_log ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abra Biv 38 3.64 ## 2 Abrekia Bra 11.6 2.45 ## 3 Abruptolopha Biv 98.8 4.59 ## 4 Acambona Bra 39.9 3.69 ## 5 Acanthalosia Bra 37.5 3.62 ## 6 Acanthambonia Bra 1.74 0.556 ## 7 Acanthatia Bra 23.5 3.16 ## 8 Acanthocardia Biv 102. 4.63 ## 9 Acanthocosta Bra 24.5 3.20 ## 10 Acanthopecten Biv 12.1 2.50 ## # … with 3,970 more rows # look at the data d %&gt;% gather(key, value, size, size_log) %&gt;% ggplot(aes(x = value)) + stat_bin() + facet_wrap(~ key, scales = &#39;free_x&#39;, switch = &#39;x&#39;) + labs(x = &#39;valve length&#39;) ## Warning: &#39;switch&#39; is deprecated. ## Use &#39;strip.position&#39; instead. ## See help(&quot;Deprecated&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Linear regression uses a Gaussian distribution to describe a continuous variable of interest. The Gaussian distribution at the center of linear regression has two parameters describing the shape of the distribution – mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Here’s a quick picture of a Gaussian distribution, play around with the mean and standard deviation parameters. ggplot(data = tibble(x = seq(from = -25, to = 25, by = 0.1)), aes(x = x, y = dnorm(x, mean = 5, sd = 1000))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) The way Bayesian updating works means we want to consider all possible combinations of \\(\\mu\\) and \\(\\sigma\\) and rank them by posterior plausibility. The posterior is in effect a distribution of plausible Gaussian distributions. To define the log valve length as Gaussian distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we write \\[ s_{i} \\sim \\text{Normal}(\\mu, \\sigma). \\] What do the different terms and symbols in this statement mean? To complete this model we need to define the priors for the parameters \\(\\mu\\) and \\(\\sigma\\). We need a joint prior \\(Pr(\\mu, \\sigma)\\), but most of the time we can just define independent priors for each parameter – this is equivalent to saying \\(Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)\\). Here’s a more complete look at the Gaussian model of log valve length: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(3, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] What do each of these three lines mean? Let’s discuss the choice of priors for \\(\\mu\\) and \\(\\sigma\\). It is a good idea to plot your priors so that you can better understand your assumptions. # prior for mean of log valve length ggplot(data = tibble(x = seq(from = -50, to = 50, by = .1)), aes(x = x, y = dnorm(x, mean = 3, sd = 10))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu)) # prior for standard deviation of log valve length ggplot(data = tibble(x = seq(from = 0, to = 30, by = .1)), aes(x = x, y = dunif(x, min = 0, max = 20))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(sigma)) But what do these distributions mean for the distribution of log valve lengths? These individual priors imply the full distribution of lengths, so let’s simulate them together and plot those results: n &lt;- 1e4 # number of samples tibble(sample_mu = rnorm(n, mean = 5, sd = 5), sample_sigma = runif(n, min = 0, max = 10)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% # joint distribution ggplot(aes(x = x)) + stat_density() + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&quot;Prior predictive distribution for &quot;, italic(s[i])))) The resulting distribution describes the relative prior plausibilities we’ve defined for log valve lengths. Play around with the numbers in the priors and see its effect on the prior probability density of log valve lengths. 3.3.3 Sampling from the model Now that we’ve completely defined out model (likelihood, data to condition on, parameters, and priors for all parameters) we can estimate the posterior plausibilities of our parameter values. We’re going to use the brms package to fit our model. This package was briefly introduced at the end of last lesson, and we’re going to get more experience with it today. After we fit the model, we’ll use functions from tidybayes that will help us extract and visualize our posterior distribution. m_1 &lt;- brm(data = d, family = gaussian(), formula = bf(size_log ~ 1), prior = c(prior(normal(3, 10), class = Intercept), prior(uniform(0, 20), class = sigma)), iter = 2000, # default warmup = 1000, # 1/2 iter chains = 4, # each chain is a set of samples cores = 4, # parallel processing; this might not work on your computer refresh = 0) # less text output # what does the brm object look like? print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: size_log ~ 1 ## Data: d (Number of observations: 3980) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.20 0.01 3.17 3.23 2991 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.92 0.01 0.90 0.94 3196 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # what do the estimates and chains look like? plot(m_1) # what are the parameter named? get_variables(m_1) ## [1] &quot;b_Intercept&quot; &quot;sigma&quot; &quot;lp__&quot; &quot;accept_stat__&quot; ## [5] &quot;stepsize__&quot; &quot;treedepth__&quot; &quot;n_leapfrog__&quot; &quot;divergent__&quot; ## [9] &quot;energy__&quot; # extract the samples m_1 %&gt;% spread_draws(b_Intercept, sigma) ## # A tibble: 4,000 x 5 ## .chain .iteration .draw b_Intercept sigma ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 3.20 0.919 ## 2 1 2 2 3.20 0.913 ## 3 1 3 3 3.19 0.919 ## 4 1 4 4 3.19 0.931 ## 5 1 5 5 3.21 0.903 ## 6 1 6 6 3.21 0.907 ## 7 1 7 7 3.21 0.916 ## 8 1 8 8 3.20 0.932 ## 9 1 9 9 3.21 0.910 ## 10 1 10 10 3.21 0.909 ## # … with 3,990 more rows We now have 4000 samples from the joint posterior. How do we want to summarize them? Here are some examples: # median and 95% quantile interval m_1 %&gt;% gather_draws(b_Intercept, sigma) %&gt;% median_qi() # ?brms::point_interval for documentation ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 3.20 3.17 3.23 0.95 median qi ## 2 sigma 0.916 0.896 0.936 0.95 median qi # posterior predictive distribution m_1 %&gt;% add_predicted_draws(newdata = d, # original data gives to simulate from model = ., # the model we want the PPD from n = 100) %&gt;% # how many draws from PPD per observation ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = d, # compare to original data distribution mapping = aes(x = size_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &#39;log valve length&#39;) We will be discussing the posterior predictive distribution further later in the lesson. For now, think of this as the distribution of outcomes implied by our model – the distribution of plausible distributions. 3.4 Adding a predictor to the mix Currently, our model doesn’t really resemble what we think of as “regression.” Typically, we want to understand how the mean of our outcome variable is related to one or more predictor variables. Let’s start with a basic analysis questions: do Bivalves and Brachiopods differ in valve length and how? Start by looking at the data. d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) To build a linear model the strategy is to make the parameter for the mean of the Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters we invent. A linear model means that we assume that the predictors have a perfectly constant and additive relationship to the mean of the outcome. Currently, our model looks like this: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(3, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] How do we add “Bivalve vs Brachiopod” to our model of valve size? First, let the mathematical name of the column “taxon” from the tibble d be called \\(x\\). This variable \\(x\\) is a predictor variable that can take one of two values: 0 for Biv(alve), and 1 for Bra(chiopod). Additionally, this vector has the same length as \\(s\\). How do we then express how \\(x\\) describes or predicts the values of \\(s\\)? To get taxonomic group into the model, we need to define \\(\\mu\\) as a function of the values in \\(x\\). Here’s how we could do this: \\[ \\begin{align} s_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta x_{i}\\\\ \\alpha &amp;\\sim \\text{Normal}(3, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0, 20) \\\\ \\end{align} \\] What do each of the lines in the model represent? What has changed and why? But where did \\(\\alpha\\) and \\(\\beta\\) come from? We made them up! \\(\\mu\\) and \\(\\sigma\\) are necessary and sufficient for describing the Gaussian distribution. \\(\\alpha\\) and \\(\\beta\\) are devices we invent for manipulating \\(\\mu\\), allowing it vary across cases in our data. Making up new parameters like \\(\\alpha\\) and \\(\\beta\\) is a common strategy for expanding the amount of information in our model. These parameters are targets of our learning – each must be described in the posterior density. When we want to learn something, we invent a parameter (or parameters) describing it. In the case of the linear model, \\(\\mu_{i} = \\alpha + \\beta x_{i}\\), we are now asking two questions about the mean of \\(s\\) instead of just one. What is the expected log valve length when \\(x_{i} = 0\\) (e.g. species is a Bivalve)? The parameter \\(\\alpha\\) answers this question. For this reason, \\(\\alpha\\) is called the intercept. What is the change in expected log valve length, when \\(x_{i}\\) changes by 1 unit? The parameter \\(\\beta\\) answers this questions, and is often called a slope. These two parameters along with \\(x\\) describe a line that passes through \\(\\alpha\\) when \\(x_{i} = 0\\) and has slope \\(\\beta\\). Remember that \\(x\\) is a binary vector – it only takes on one of two values: 0 or 1. For a binary predictor, the regression coefficient is interpreted as the difference between the averages of the two groups. Let’s think more about this choice of prior. Do you think there is an equal chance that average brachiopod valves are larger or smaller than bivalves? In this context, we have so much data that this is harmless. In other contexts, our sampler might need more information to finds its target. So let’s fit the model: m_2 &lt;- brm(data = d, family = gaussian(), formula = size_log ~ 1 + taxon, prior = c(prior(normal(3, 10), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 20), class = sigma)), iter = 2000, # default warmup = 1000, # 1/2 iter chains = 4, # each chain is a set of samples cores = 4, # parallel processing; this might not work on your computer! refresh = 0) # less text output # look at the brm summary print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: size_log ~ 1 + taxon ## Data: d (Number of observations: 3980) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.69 0.02 3.65 3.74 3739 1.00 ## taxonBra -0.77 0.03 -0.82 -0.71 3535 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.84 0.01 0.82 0.86 3978 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # look at the brm plot plot(m_2) 3.4.1 Aside: Dummy coding brm, and R in general, will automatically translate categorical predictors like our taxon variable into what’s called dummy coding. Here’s an illustration of what that means using the subtype variable “sub” from our raw data: r %&gt;% distinct(sub) ## # A tibble: 3 x 1 ## sub ## &lt;chr&gt; ## 1 Het ## 2 nonHet ## 3 inart r %&gt;% model_matrix(~ sub) %&gt;% # from modelr distinct() ## # A tibble: 3 x 3 ## `(Intercept)` subinart subnonHet ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 0 1 ## 3 1 1 0 r %&gt;% model_matrix(~ sub) ## # A tibble: 164,402 x 3 ## `(Intercept)` subinart subnonHet ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 ## 7 1 0 0 ## 8 1 0 0 ## 9 1 0 0 ## 10 1 0 0 ## # … with 164,392 more rows One of the values of the column is designated to be the intercept. In R, the default intercept is the first value of the vector, alphabetically. The other variables are called contrasts or dummy variables – they describe the difference in mean value when compared to the intercept. By adding the intercept and the regression coefficient, you get the estimated mean for that group. If the categorical variable has \\(k\\) states, then there are \\(k - 1\\) contrasts or dummy variables. There are many other strategies for encoding categorical variables, but dummy coding is by far the most common – it helps that it is the R default. Don’t worry too much about understanding dummy coding yet – just understand that it exists and that we will return to it in a later lesson. 3.5 Interpreting the model fit There are two broad categories of how we process model fit: tables, and plotting. Tables are fine, but plotting is key. It is easy to feel like you understand a table while still getting it wrong. Plotting the implications of your estimates will allow you to inquire about several things that are sometimes hard to read from tables: Whether or not the model fitting procedure worked correctly. The absolute magnitude, rather than merely relative magnitude, or a relationship between outcome and predictor. The uncertainty surrounding an average relationship. The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty. With practice extracting estimates from your model and plotting them, you can ask any question you can think of, for any model. Let’s start by getting a basic summary of our posterior: m_2 %&gt;% gather_draws(b_Intercept, b_taxonBra, sigma) %&gt;% median_qi() ## # A tibble: 3 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 3.69 3.65 3.74 0.95 median qi ## 2 b_taxonBra -0.766 -0.823 -0.714 0.95 median qi ## 3 sigma 0.840 0.822 0.859 0.95 median qi The first row corresponds to the model intercept, which we called \\(\\alpha\\). This parameter corresponds to the expected log valve length when \\(x = 0\\). In the case of this model, this is easily interpreted as the expected log valve length of a Bivalve species. The second row is the slope term \\(\\beta\\). This parameter describes the expected change in log valve size associated with unit change in \\(x\\). In this case of this model, this parameter is easier to describe: this is an estimate of the expected difference in log valve length between Bivalves (\\(x = 0\\)) and Brachiopods (\\(x = 1\\)). By adding \\(\\beta\\) and \\(\\alpha\\), we get the estimate for Brachiopod expected log valve length. The third line is the standard deviation term \\(\\sigma\\), which describes the with of the distribution of log valve lengths. A useful trick for interpreting \\(\\sigma\\) is that about 95% of the probability of a Gaussian distribution lies between plus/minus two standard deviations from the mean. In this case, the estimate tells us that 95% of plausible log valve lengths lie within 1.68 log millimeters (\\(2\\sigma\\)) of the mean log valve length. 3.5.1 Linear predictor Our regression model describes a line with an intercept and a slope. As demonstrated above, this is true even in the case of a binary predictor – even though our predictor can only take one of two values, the formula still describes a line. The function brms::add_fitted_draws() estimates the expected log valve length from the linear model part of our model. Remember that the linear model describes only the mean log valve length, and not the spread of log valve length. This function is a convenient way to help us visualize this part of our model. Here’s an illustration of the linear relationship between taxonomic group and log valve size as described by the median estimates for each taxonomic group: # parameters of the line d_fitted &lt;- d %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% # 100 posterior estimates ungroup() %&gt;% group_by(taxon) %&gt;% # want to know taxon summary dplyr::summarize(value = median(.value)) %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) # make text clearer d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + geom_line(data = d_fitted, mapping = aes(x = taxon, y = value, group = 1), size = 1, colour = &#39;blue&#39;) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) The above plot uses the median point estimates and includes none of our uncertainty about the relationship between the taxonomic groups and log valve length. There are a few ways we can demonstrate our uncertainty about the linear relationship. We can plot multiple lines at the same time: d_fitted &lt;- d %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) # make text clearer d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% # make text clearer ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + geom_line(data = d_fitted, mapping = aes(x = taxon, y = .value, group = .draw), size = 1, colour = &#39;blue&#39;, alpha = 0.1) + labs(x = &#39;taxon&#39;, y = &#39;log valve length&#39;) 3.5.2 Posterior prediction An aspect of Bayesian models we’ve yet to discuss at length is the posterior predictive distribution \\(p(\\tilde{y} | y)\\). Prediction is when, given our model and parameter estimates, we want to estimate the outcome for some combination of covariates. The posterior predictive distribution is the distribution of outcomes defined by the plausible parameter values and data – instead of a single prediction, we have a distribution of predictions. For example, we might want to predict the log valve length of some species given our model and parameter estimates. For each possible value of a parameter, there is an implied distribution of outcomes. If we compute the distribution of outcomes for each value, this gives us a posterior predictive distribution. Our full model describes log valve length as a Gaussian distribution with a mean (as a linear model) and a standard deviation. Our previous plots only considered the linear model aspect which describes the mean of the distribution, but there is still more information in our model that we’ve yet to consider: the estimated standard deviation \\(\\sigma\\). The brms::add_predicted_draws() function does this exactly. For each “new” observation, we obtain a series of predictions about that observations log valve length, not just the expected value of log valve length. This function is similar to the brms::add_fitted_draws() function we used above, but instead of predicting from the linear model we are predicting from the entire distribution. Let’s illustrate this by comparing our log valve length data to our posterior predictive distribution for that data: d_predicted &lt;- d %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) d %&gt;% mutate(taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% ggplot(aes(x = taxon, y = size_log)) + geom_violin(fill = &#39;grey60&#39;, colour = NA) + geom_jitter(height = 0, alpha = 0.5) + stat_lineribbon(data = d_predicted, mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), size = 0) + scale_fill_brewer() 3.5.3 Posterior predictive tests What if we want to test how well our model fits specific parts of our data, and not just compare the similarities of their distributions? For example, how well does our model estimate the mean log valve size of each taxon? Here’s the logic of posterior predictive tests: if data simulated from our posterior predictive distribution is able to reproduce a specific aspect of the original data, then the model might be doing something right. And if there are systematic failures in our model’s ability to predict the data, then the model must be doing something wrong. Remember the warnings from last lesson: The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right and are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions. Instead, our objective should be to test the model’s adequacy for some purpose. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you know as a domain expert and compare it to your model; if there is a conflict you should update your model (likelihood and/or prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently scientific questions, not statistical questions. As I said earlier, robot’s can’t do this step for you. Here’s are a few example posterior predictive tests: # summarize the original data d_sum &lt;- d %&gt;% group_by(taxon) %&gt;% dplyr::summarize(mean = mean(size_log), sd = sd(size_log), iqr = IQR(size_log)) %&gt;% # lots of options, none is best gather(key, value, mean, sd, iqr) %&gt;% mutate(key = recode(key, mean = &#39;Mean&#39;, sd = &#39;Std Dev&#39;, iqr = &#39;Inter Quart Range&#39;), taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) d %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% # calculate from n PPD draws group_by(taxon, .draw) %&gt;% dplyr::summarize(mean = mean(.prediction), sd = sd(.prediction), iqr = IQR(.prediction)) %&gt;% ungroup() %&gt;% gather(key, value, mean, sd, iqr) %&gt;% mutate(key = recode(key, mean = &#39;Mean&#39;, sd = &#39;Std Dev&#39;, iqr = &#39;Inter Quart Range&#39;), taxon = recode(taxon, Biv = &#39;Bivalvia&#39;, Bra = &#39;Brachiopoda&#39;)) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &#39;grey80&#39;, colour = NA) + geom_vline(data = d_sum, mapping = aes(xintercept = value), size = 1.5, colour = &#39;grey20&#39;) + facet_grid(taxon ~ key, scales = &#39;free&#39;, switch = &#39;y&#39;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = &#39;&lt;stat&gt; log valve length&#39;) Where does our model do well? Where does our model do poorly? Why? How could we modify our model to overcome these failures? Even though it should be obvious and mandatory to do go through with and report an extended posterior predictive checking process, it is relatively rare in macroevolutionary biology and paleobiology. Indeed, so rare that posterior predictive checking, if done creatively and well, can get you a paper: Example 1, Example 2, Example 3. 3.6 Summary To review, this lesson was an exercise in developing, communicating, and summarizing a Bayesian model – specifically, a linear regression model. We’ve slowly developed a linear regression model by expanding a Gaussian distribution to include the effects of predictor information. We first developed our model using the symbolic representation of a statistical model, and we then implemented our model using functions from brms. We explored a number of ways of representing and visualizing posterior distributions; these included tables and figures. We briefly covered the difference between fitted predictions and the posterior predictive distribution. Finally, we discussed the concept of posterior predictive tests. "],
["reg-continue.html", "4 Continuing with regression with continuous predictors 4.1 Objectives 4.2 Reading 4.3 Our first example 4.4 A single continuous predictor 4.5 Summary", " 4 Continuing with regression with continuous predictors 4.1 Objectives In the previous lesson we introduced linear regression with a single, binary predictor. This lesson expands on that initial introduction by introducing and explaining continuous predictors. Along the way I will continue to emphasize checking the quality or adequacy of model fit as an important part of both understanding our model and improving out model. Including a continuous predictor in a regression model. Learn to interpret continuous predictors. Continue to focus on evaluating model fit as major step in modeling. library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, modelr) theme_set(theme_bw()) 4.2 Reading The following materials are recommended pre-readings before starting this tutorial. Chapter 4 “Linear Models” from Statistical Rethinking by Richard McElreath. OPTIONAL Chapter 3 “Linear regression: the basics” from Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. OPTIONAL Chapter 4 “Linear regression: before and after fitting the model” from Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. 4.3 Our first example For this lesson we will be analyzing data from the PanTHERIA} database, a large collection of trait data for extant mammals. For a more detailed explanation of the dataset and each variable, you can review the data dictionary. A key detail to PanTHERIA is that missing data is coded as -999.00 and not as NA or blank cells. This knowledge is something we can use when importing our data so that it translates easily into the R environment. Also, a lot of the variables have terrible names which are mixes of punctuation, capital letters, begin with numbers, etc. – all are difficult to program around and need to be dealt with and standardized (using the janitor package). That all being said, let’s import the dataset, clean it up a bit, and then start visualizing it. pantheria &lt;- read_tsv(here(&#39;data&#39;, &#39;PanTHERIA_1-0_WR05_Aug2008.txt&#39;), na = &#39;-999.00&#39;) %&gt;% clean_names() %&gt;% # even after cleaning, the variables are rough so let&#39;s smooth them out mutate(mass_log = log(x5_1_adult_body_mass_g), range_group_log = log(x22_1_home_range_km2), range_indiv_log = log(x22_2_home_range_indiv_km2), density_log = log(x21_1_population_density_n_km2), activity_cycle = case_when(x1_1_activity_cycle == 1 ~ &#39;nocturnal&#39;, x1_1_activity_cycle == 2 ~ &#39;mixed&#39;, x1_1_activity_cycle == 3 ~ &#39;diurnal&#39;), trophic_level = case_when(x6_2_trophic_level == 1 ~ &#39;herbivore&#39;, x6_2_trophic_level == 2 ~ &#39;omnivore&#39;, x6_2_trophic_level == 3 ~ &#39;carnivore&#39;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## MSW05_Order = col_character(), ## MSW05_Family = col_character(), ## MSW05_Genus = col_character(), ## MSW05_Species = col_character(), ## MSW05_Binomial = col_character(), ## References = col_character() ## ) ## See spec(...) for full column specifications. pantheria %&gt;% drop_na(activity_cycle) %&gt;% ggplot(aes(x = trophic_level, y = range_group_log)) + geom_violin(fill = &#39;grey80&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) + geom_jitter(height = 0, alpha = 0.5, mapping = aes(colour = msw05_order)) + scale_colour_viridis(discrete = TRUE, name = &#39;Order&#39;) + labs(x = &#39;Trophic level&#39;, y = expression(paste(&#39;Group range size &#39;, log(km^2))), title = &#39;Group range size differences between trophic levels, \\norders highlighted&#39;) ## Warning: Removed 1079 rows containing non-finite values (stat_ydensity). ## Warning: Removed 1079 rows containing missing values (geom_point). pantheria %&gt;% ggplot(aes(x = mass_log, y = density_log, colour = msw05_order)) + geom_point() + scale_colour_viridis(discrete = TRUE, name = &#39;Order&#39;) + labs(x = expression(paste(&#39;Body mass &#39;, log(g^2))), y = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Body mass and population density, orders highlighted&#39;) ## Warning: Removed 4469 rows containing missing values (geom_point). There are tons of ways we could deconstruct this dataset, some much more logical than others. For this tutorial, we’re going to focus on trying to understand how population density varies across mammals. There are tons of factors that can influence the population density of a species, so our process will be to slowly build up a model one predictor at a time while continually checking the quality of the model’s fit to the data. Let’s begin our population density model in a similar fashion to our previous one: an intercept-only model. Once this simplest model is established, we can then add our first continuous predictor from there. Our response variable density_log is a continuous value from \\(-\\infty\\) to \\(\\infty\\) so a good place to start is by assuming that this variable can be approximated by a Normal distribution, as is common with linear regression. The Normal distribution two parameters: mean and standard deviation. Sometimes the standard deviation term of the Normal distribution is replaced with a variance or precision parameter – we’re not going to deal with those alternative parameterizations. Let’s write this out. Let \\(y\\) be density_log, \\(\\mu\\) be the mean of density_log, and \\(\\sigma\\) be the standard deviation of density_log. \\[ y \\sim \\text{Normal}(\\mu, \\sigma) \\] Can you recall what all the of the parts of the above statement mean? What are we missing? Priors! We can probably stick with pretty vague priors here – the mean is probably somewhere between -10 and 10 log(millimeters) and probably has at least that much range. Here’s my starting point. Could I improve it? Do we have enough data that it probably won’t matter? \\[ \\begin{align} y &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 10) \\\\ \\end{align} \\] Something is new here – what is this Cauchy\\(^{+}\\) distribution? The Cauchy distribution is a thick-tailed probability distribution closely related to the Student’s t distribution. The Cauchy\\(^{+}\\) distribution is the half-Cauchy distribution – this means it is only defined for all positive real values (including 0.) You can think of this distribution as a weakly regularizing prior for standard deviations – most of the mass is concentrated towards low values and 0, but the heavy tail means there is non-zero probably of large values for \\(\\sigma\\). The half-Cauchy distribution has two parameters: location and scale. The location is the middle of the full distribution and the scale describes the “width” of the distribution. For the half-Cauchy distribution the location parameter is mostly a formality as it defines the “backstop” of the distribution – that it is defined for values of 0 or greater, with a default being 0. Here is a quick visualization of the Cauchy distribution’s behavior as you vary the scale parameter. The half-Cauchy is just this distribution reflected about the origin. df &lt;- tibble(x = seq(from = -20, to = 20, by = 0.1)) %&gt;% mutate(scale_1 = dcauchy(x, location = 0, scale = 1), scale_5 = dcauchy(x, location = 0, scale = 5), scale_10 = dcauchy(x, location = 0, scale = 10), scale_20 = dcauchy(x, location = 0, scale = 20), scale_50 = dcauchy(x, location = 0, scale = 50)) df %&gt;% gather(key = &#39;key&#39;, value = &#39;value&#39;, -x) %&gt;% separate(key, c(&#39;type&#39;, &#39;scale&#39;)) %&gt;% mutate(scale = factor(scale, levels = sort(order(scale)))) %&gt;% ggplot(aes(x = x, y = value, colour = scale)) + geom_line(size = 2) + scale_y_continuous(NULL, breaks = NULL) + scale_colour_viridis(discrete = TRUE) + labs(colour = &#39;Scale&#39;) + NULL As you can see, at scales of 20 or greater the Cauchy really begins to resemble the uniform distribution but with a bump in density around 0. Let’s implement our intercept-only model in brms. To fit this model we are going to need to remove all species from the dataset that are missing values for density_log. Is getting rid of all this data ideal? Let’s assume it has no effect on our results for now, but if you are interested in learning more about handling missing values in our models, look up data imputation – an advanced topic we will not be covering introductory lessons. # remove missing values pantheria &lt;- pantheria %&gt;% drop_na(density_log) # fit basic model m_1 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1), prior = c(prior(normal(0, 10), class = Intercept), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 ## Data: . (Number of observations: 956) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.86 0.09 3.68 4.05 3270 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 2.97 0.07 2.84 3.10 3508 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As with any model, we should see how well it describes our data – maybe this simple model does a good enough job? Let’s compare our observed distribution of population densities versus our posterior predictive distribution. pantheria %&gt;% add_predicted_draws(model = m_1, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted.&#39;) + NULL While we might be describing the overall mean and standard deviation of our data, I do not think our simple model is capable of capturing a lot of the complexity in our data. Our data appears to be multimodal and has a very different spread than those datasets simulated from the posterior predictive distribution, especially on the right-hand side. We are probably going to have to include more information we if want to better descriptive model of our data. 4.4 A single continuous predictor In linear regression, our predictors tend to describe change in mean \\(y\\) has a function of an intercept, one or more regression coefficients, and one or more predictor. Just like in our previous lesson, we’re going to try and improve upon on model by adding a predictor. What’s new to this lesson is that that predictor is a continuous variable: average individual mass in log grams or mass_log. To do this, we need to define \\(\\mu\\) as a function of our predictor. Do you remember from last lesson how we did this? Try writing out a model definition by hand. First, let’s define \\(x\\) as mass_log. Also, let’s define two more variables: let \\(\\alpha\\) be the intercept of our regression, and let \\(\\beta\\) be the regression coefficient for \\(x\\). Given this new information, here is how we can write out our regression model. \\[ \\begin{align} y_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta x_{i} \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(-1, 5) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 5) \\\\ \\end{align} \\] Are the choice of priors reasonable? Take a closer look at the prior for \\(\\beta\\) – this is a weakly informative prior. I’m guessing that the slope is probably negative, but allow for the possibility of a 0 or positive slope – albeit less than that of a negative slope. Is this justified? Think of the physical definition of the variables – what would you guess the relationship between body size and population density to be? You might notice this model is functionally identical to the model from our previous lesson where we had only a single binary predictor. The difference is all on the data end, and not the model, as \\(x\\) can take on any value and not just 0 and 1. How do we interpret all of these parameters? Our previous lesson gave us all the information we needed to describe each parameter, but I will reiterate them here because this is really important. If we don’t know what our parameters precisely mean, we cannot interpret them. \\(\\mu\\) average value of \\(y\\) \\(\\sigma\\) standard deviation of \\(y\\) \\(\\alpha\\) intercept, average value of \\(y\\) when \\(x\\) = 0 \\(\\beta\\) slope, expected change in \\(y\\) per unit change in \\(x\\) Let’s implement this model in brms. Like before, we are going to ignore species that have missing data for either density_log or mass_log – brms() can do this for us automatically, but let’s do it by hand here again. So, our first set is filter down the pantheria tibble again and then fit our new model. We’ve already dropped all observations missing density values, so we just need to do the same for mass values. Conveniently, there is no harm in checking for missing density_log values again so that everything is clear. pantheria &lt;- pantheria %&gt;% drop_na(density_log, mass_log) m_2 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(-1, 10), class = b), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log ## Data: . (Number of observations: 947) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 8.94 0.16 8.63 9.25 4248 1.00 ## mass_log -0.74 0.02 -0.78 -0.70 4051 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.95 0.04 1.86 2.03 4715 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.4.1 Aside: Centering The intercept of a linear regression model is normally interpreted as the average value of \\(y\\) when all predictors equal 0. A consequence of this definition means that the value of the intercept is frequently uninterpretable without also studying the regression coefficients. This is also the reason that we commonly need very weak priors for intercepts. A trick for improving our interpretation of the intercept \\(\\alpha\\) is centering our (continuous) predictors. Centering is the procedure of subtracting the mean of a variable from each value. Namely: pantheria &lt;- pantheria %&gt;% mutate(mass_log_center = mass_log - mean(mass_log)) \\(\\alpha\\) is still the expected value of the outcome variable when the predictor is equal to zero. But now the mean value of the predictor is also zero. So the intercept now means: the expected value of the outcome, when the predictor is at its average value. This makes interpreting the intercept a lot easier. To illustrate this, let’s refit the model with the newly centered data. m_3 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_center), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(1, 5), class = b), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_center ## Data: . (Number of observations: 947) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 3.85 0.06 3.72 3.97 4372 1.00 ## mass_log_center -0.74 0.02 -0.78 -0.70 3917 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.95 0.04 1.86 2.03 3656 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Can you explain why centering changes the value of \\(\\alpha\\) but not \\(\\beta\\)? Centering will not change our models posterior predictive performance, but really improves the interpretability of our model parameters. Centering can also be beneficial for estimating parameter values by decreasing posterior correlation among the parameters. I recommend always centering your (continuous) predictors. 4.4.2 Checking model fit Now let’s see how much adding this predictor improves our ability to describe \\(y\\). We can also visualize our data as a scatter plot with the linear predictor overlain to demonstrate our model estimates of mean population density as a function of species mass. pantheria %&gt;% add_fitted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + geom_line(mapping = aes(y = .value, group = .draw), alpha = 1 / 20, colour = &#39;blue&#39;) + geom_point(data = pantheria, size = 2) + scale_fill_brewer() The previous plot only covers our model’s estimates for mean population density but this is not all our model is telling us. As with our earlier posterior predictive comparisons from the intercept-only model, we can use the full posterior predictive distribution to compare our observed data to 100 datasets drawn from the posterior predictive distribution. Because the posterior predictive distribution also takes into account the estimated scale of our data (\\(\\sigma\\)) and thus estimates individual values of \\(y\\) and not just the expected value of \\(y\\), these types of comparisons give us a fuller appreciation of how well our model is or is not representing out data. pantheria %&gt;% data_grid(mass_log_center = seq_range(mass_log_center, n = 1000)) %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), size = 0) + geom_point(data = pantheria, size = 2) + scale_fill_brewer() + NULL We can also do explicit posterior predictive tests to see how well our model captures specific parts of our data such as the overall density, the median, or differences between unmodeled classes. For our first posterior predictive test, let’s do a comparison between the density of our data, \\(y\\), and the densities of 100 simulated datasets drawn from our posterior predictive distribution, \\(y^{\\tilde}\\). pantheria %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted.&#39;) + NULL Let’s then see how well our model reproduces the median population density even though our model is defined for the mean of population density. pantheria_summary &lt;- pantheria %&gt;% dplyr::summarize(median = median(density_log)) %&gt;% pull() pantheria_summary_ppc &lt;- pantheria %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw) %&gt;% dplyr::summarize(density_median_ppc = median(.prediction)) med_ppc &lt;- pantheria_summary_ppc %&gt;% dplyr::summarize(per = sum(density_median_ppc &gt; pantheria_summary) / n()) %&gt;% pull() pantheria_summary_ppc %&gt;% ggplot(aes(x = density_median_ppc)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(xintercept = pantheria_summary, size = 2) + labs(subtitle = paste(med_ppc, &#39;% of estimates greater than observed&#39;), x = &#39;Median population density&#39;, y = &#39;&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s see if our model can reproduce the differences in population density between trophic levels even though that information was not included in our model. This requires us dropping a bit more of our data because trophic level is not assigned for as many species as density or body mass. pan_cut &lt;- pantheria %&gt;% drop_na(density_log, mass_log_center, trophic_level) pan_trophic_summary &lt;- pan_cut %&gt;% group_by(trophic_level) %&gt;% dplyr::summarize(median = median(density_log)) pan_trophic_summary_ppc &lt;- pan_cut %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw, trophic_level) %&gt;% dplyr::summarize(density_median_ppc = median(.prediction)) pan_trophic_summary_ppc %&gt;% ggplot(aes(x = density_median_ppc)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(data = pan_trophic_summary, mapping = aes(xintercept = median), size = 2) + facet_wrap(~ trophic_level) + labs(x = &#39;Median population density&#39;, y = &#39;&#39;) + NULL ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Or we can even look at how well our posterior predictive distribution does across the different trophic levels, even though those are not encoded in the model. pan_cut &lt;- pantheria %&gt;% drop_na(density_log, mass_log_center, trophic_level) pan_cut %&gt;% data_grid(mass_log_center = seq_range(mass_log_center, n = 1000), trophic_level) %&gt;% add_predicted_draws(model = m_3, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_center, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), size = 0) + geom_point(data = pan_cut, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL While our model does an okay job at predicting median population density of omnivores and herbivores, it is very bad at estimating the population density of carnivores. Given all of the above plots, in particular the failure of our model to capture the differences between the (unmodeled) trophic levels, it appears that while this model does slightly better job at approximating our data than our earlier model it still fails are adequately representing our data. Even though we did not model the differences between the trophic groups, by seeing if our model captures these differences we get a better idea about how we might improve our model. In this case, because we know that while there are differences between trophic groups, these differences are not captured by the model. If we want to make sure we are describing the different sources of variation, we might want to include the tropic levels as additional predictors on top of body size. But what is this about adding more predictors? What does that mean? That is the subject of our next lesson. 4.5 Summary In this lesson we introduced continuous predictors in linear regression. We also covered a lot of examples of how to inspect your model’s adequacy at describing our data. This process involved checking our model’s ability to represent unmodeled variation in the data that we know about but that our model does not. Our next step will be including these previously unmodeled factors in a new expanded model. In our next lesson we will cover multivariate models, or models with more than one predictor (e.g body mass AND trophic level). "],
["multiple-predictors-and-interactions-in-linear-regression.html", "5 Multiple predictors and interactions in linear regression 5.1 Objectives 5.2 Reading 5.3 More than one predictor 5.4 Defining our model 5.5 Fitting model in brms 5.6 Aside: Standardizing 5.7 Checking model fit 5.8 Aside: Matrix Notation 5.9 Summary", " 5 Multiple predictors and interactions in linear regression 5.1 Objectives Interpret regression models with a categorical predictor that has more than two levels. Learn to interpret regression models with more than one predictor. Introduce statistical interactions in regression models. Cover strategies for visualizing multivariate regression models. library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, modelr, forcats) theme_set(theme_bw()) 5.2 Reading The following materials are recommended pre-readings before starting this tutorial. Chapter 5 “Multivariate Linear Models” from Statistical Rethinking by Richard McElreath. OPTIONAL Chapter 3 “Linear regression: the basics” from Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. OPTIONAL Chapter 4 “Linear regression: before and after fitting the model” from Data Analysis Using Regression and Multilevel/Hierarchical Models by Gelman and Hill. 5.3 More than one predictor Correlation between variables is very common in nature. In large datasets, all pairs of variables have a statistically discernible non-zero correlation. We should never be surprised to find that two variables are correlated. But simple correlation tells us nothing of mechanism – the “biology” of the system. We need tools to distinguist between mere association from evidence of causation. This is where multivariate regression (regression with more than one predictor variable) comes into play. The three general reasons for using a multivariate model are: Confounds or variables that may be correlated with variable of interest. Confounds can easily hide important variables or produce false signals. Check out Simpson’s Paradox for an example of how a relationship can be reversed in the presence of a confound. Multiple causation. Reality is rarely simple, to the point that we can safely assume complexity. By considering that complexity, we can improve our understanding. Additionally, when causation is multiple one cause can hide another unless they are considered simultaneously. Interactions. Even in the absence of correlation, effects of two or more variables might depend on each other. An example would be plant growth as a function of light and water – both are needed and only having one yields no benefit. In order to make effective inference about one variable, we need to consider the others as well. So far all of the linear regression models we have encountered have either been intercept-only or with a single predictor. For this lesson we’re going to continue our focus on three variables from the PanTHERIA dataset: population density, body size, and trophic level. Population density, measured as the number of individuals per square-kilometer, is our variable of interest – we want to define a model which describes how population density varies between mammal species. We have previously investigated body size as a predictor of population density, but the posterior predictive analysis of our model of population density with only body mass as a predictor demonstrates that this model does not adequately describe the data. We were able to do this by asking if our model is able to describe differences in the data that we know about but that our model does not (e.g. trophic level). We’re going to be using the PanTHERIA dataset like we did in the previous lesson. Just like last time, here is a quick clean up of the dataset before we do anything “principled.” pantheria &lt;- read_tsv(here(&#39;data&#39;, &#39;PanTHERIA_1-0_WR05_Aug2008.txt&#39;), na = &#39;-999.00&#39;) %&gt;% clean_names() %&gt;% mutate(mass_log = log(x5_1_adult_body_mass_g), range_group_log = log(x22_1_home_range_km2), range_indiv_log = log(x22_2_home_range_indiv_km2), density_log = log(x21_1_population_density_n_km2), activity_cycle = case_when(x1_1_activity_cycle == 1 ~ &#39;nocturnal&#39;, x1_1_activity_cycle == 2 ~ &#39;mixed&#39;, x1_1_activity_cycle == 3 ~ &#39;diurnal&#39;), trophic_level = case_when(x6_2_trophic_level == 1 ~ &#39;herbivore&#39;, x6_2_trophic_level == 2 ~ &#39;omnivore&#39;, x6_2_trophic_level == 3 ~ &#39;carnivore&#39;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## MSW05_Order = col_character(), ## MSW05_Family = col_character(), ## MSW05_Genus = col_character(), ## MSW05_Species = col_character(), ## MSW05_Binomial = col_character(), ## References = col_character() ## ) ## See spec(...) for full column specifications. 5.3.1 Categorical predictor Trophic level is a categorical predictor with three levels. The previous categorical predictor we dealt with (Bivalve versus Brachiopod) was binary which made it very easy to interpret. In that lesson I briefly introduced the idea of dummy coding and demonstrated how that would work with a three level categorical variable. I’m going to reiterate and expand on that demonstration here. brm, and R in general, will automatically translate categorical predictors like our trophic_level variable into what’s called dummy coding. When we dummy code a categorical variable what we are doing is turning one variable with \\(k\\) states into \\(k - 1\\) binary variables. One state of the categorical variable is considered the “baseline” or the default condition for any observation. The other \\(k - 1\\) binary variables then describe if the observation is different from the default – these are called contrasts. The standard behavior for this in R is that the first state, alphabetically, is made the baseline. Here is this in action: (pantheria %&gt;% drop_na(trophic_level) %&gt;% model_matrix(~ trophic_level)) ## # A tibble: 2,161 x 3 ## `(Intercept)` trophic_levelherbivore trophic_levelomnivore ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 1 0 ## 7 1 1 0 ## 8 1 1 0 ## 9 1 0 1 ## 10 1 0 1 ## # … with 2,151 more rows The model_matrix() function returns a tibble with \\(k\\) columns. Take a closer look at the column names. First, focus on the second and third columns. Each title is the variable name (trophic_level) combined with one of the levels (e.g. trophic_levelherbivore). One of the factor’s levels, carnivore, is not named as one of the columns, instead that level is subsumed in the (Intercept) column. The default state of any observation is this “carnivore.” A 0 in the trophic_levelherbivore or trophic_levelomnivore columns means that the observation is not an herbivore or omnivore, respectively. If there is a 1 in the herbivore or omnivore column that means that observation is an herbivore or omnivore and not a carnivore – hence why the \\(k - 1\\) binary variables are called contrasts (with respect to the default condition/intercept). Importantly, an observation cannot/should not have a 1 in more than one of the \\(k - 1\\) binary variables associated with a single categorical predictor. Let’s see what happens when we suppress the intercept using R’s formula syntax. (pantheria %&gt;% drop_na(trophic_level) %&gt;% model_matrix(~ -1 + trophic_level)) ## # A tibble: 2,161 x 3 ## trophic_levelcarnivore trophic_levelherbivore trophic_levelomnivore ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 ## 2 0 0 1 ## 3 0 0 1 ## 4 1 0 0 ## 5 1 0 0 ## 6 0 1 0 ## 7 0 1 0 ## 8 0 1 0 ## 9 0 0 1 ## 10 0 0 1 ## # … with 2,151 more rows By suppressing the intercept in our model matrix, we’ve dramatically changed the first column of our tibble. Instead of being labeled (Intercept) and being a column of only 1s, we have a column named trophic_levelcarnivora which is a binary variable indicating if an observation is a carnivore or not. I do not recommend suppressing the intercept when developing a model, especially if you are generating your contrasts independent of the model formula (e.g. when using brm()), and extra especially if you have more predictors in your model than just the categorical variable being coded out. If you are not careful, you might end up with the equivalent of two intercepts which causes a model to be completely undefined and useless. As as been discussed before, R tacitly converts our categorical variables (e.g. variables where our observations are character data) into dummy coding. While this is convenient, it can lead to confusion if you aren’t super familiar with categorical variables. For example, I once had a colleague ask, “How are there 5 parameters when we only have 1 predictor in our linear regression (they expected three)?” Something I like to do is make the most common or most general class be the intercept. This adds a bit of logic to the contrasts, as they are now “in contrast” to the most common state. The most common class in trophic_level is “herbivore”. To change which class becomes the intercept, we need to manipulate the R structure of trophic_level. I’m choosing to do this using functions from the forcats package. (pantheria %&gt;% drop_na(trophic_level) %&gt;% mutate(trophic_level = fct_infreq(trophic_level)) %&gt;% # reorder by frequency model_matrix(~ trophic_level)) ## # A tibble: 2,161 x 3 ## `(Intercept)` trophic_levelomnivore trophic_levelcarnivore ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 ## 2 1 1 0 ## 3 1 1 0 ## 4 1 0 1 ## 5 1 0 1 ## 6 1 0 0 ## 7 1 0 0 ## 8 1 0 0 ## 9 1 1 0 ## 10 1 1 0 ## # … with 2,151 more rows By controlling which of the classes from our categorical variable is considered the intercept, we can make inference easier. Carnivores are relatively rare in our dataset, so having them as the “default” condition for our model is a bit confusing – most of our observations would be “exceptions” or “in contrast to” our default. By having herbivores act as are default, we inherently are describing more parts of our data before looking at the contrasts. There are many other strategies for encoding categorical variables, but dummy coding is by far the most common – it helps that it is the R default. 5.4 Defining our model Now that we have refreshed our understanding of how categorical covariates are handled in a regression model, let’s write out a model describing how density_log varies as a linear function of our trophic_level categorical predictor and the mass_log continuous covariate. As always, let’s write out our model in statistical notation before implementing it in brms. Multivariable regression models add more parameters and variables to the defintion of the mean of the Normal distribution, \\(\\mu_{i}\\). For each predictor we will essentially do three things: Nominate the predictor variable of interest. Create a parameter that will measure the association between that variable and the outcome. Multiply the parameter by the variable and add that term to the linear model. To demonstrate this, I’m going to be a bit more formal in defining our statistical model than before because it is good practice and because this model is more complicated than the ones we have seen before. We have \\(N\\) total observations (species) in our dataset. We can define our response variable, density_log, as \\(y_{i}\\) where \\(i\\) index which species that measure is from and \\(i = 1, 2, \\ldots, N\\). We have our two predictor variables mass_log and trophic_level which are continuous and categorical, respectively. Our first predictor is the continuous mass_log of each observation – let’s call this variable \\(x_{i}\\) where \\(i = 1, 2, \\ldots, N\\). Our second variable is the categorical trophic_level which has three classes: carnivore, herbivore, omnivore. As discussed before, we are going to rewrite this variable as a set of \\(k - 1\\) contrasts where \\(k\\) is the number of levels in the variable (i.e. three). Let’s name herbivore to be the baseline, with carnivore and omnivore being the contrasts. We can define these two binary predictors as \\(c_{i}\\) and \\(o_{i}\\), respectively. This covers all of our data. Now lets define the parameters that describe our the bits of our data are related. We need to define a regression model describing how the expected value of \\(y\\) varies as a function of \\(x\\), \\(c\\), and \\(o\\). Let’s assume a Normal distribution is a suitable descriptor of population density, the core assumption of linear regression. We can the define mean and standard deviation of this distribution as \\(\\mu_{i}\\) and \\(\\sigma\\). \\(\\mu\\) is indexed by observation \\(i\\) where \\(i = 1, 2, \\ldots, N\\) because we are going to define it as a linear function of our predictors which vary for each of our \\(N\\) observations. We then can define the linear function describing \\(mu_{i}\\) as an additive relationship of each covariate multiplied by its own regression coefficient plus an intercept. Let’s identify the intercept as \\(\\alpha\\) and the three coefficients as \\(\\beta_{1}\\), \\(\\beta_{2}\\), and \\(\\beta_{3}\\). Now that we’ve defined all of our data terms and parameters, we need to define the remaining priors for the regression coefficients and standard deviation parameter. I’m going to stick with what the priors we used from our previous lesson, with a slightly informative prior for the effect of body mass and general weakly informative priors for the other regression coefficients. \\[ \\begin{align} y_{i} &amp;\\sim \\text{Normal}(\\mu_{i}, \\sigma) \\\\ \\mu_{i} &amp;= \\alpha + \\beta_{1} m_{i} + \\beta_{2} o_{i} + \\beta_{3} h_{i} \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta_{1} &amp;\\sim \\text{Normal}(-1, 5) \\\\ \\beta_{2} &amp;\\sim \\text{Normal}(0, 5) \\\\ \\beta_{3} &amp;\\sim \\text{Normal}(0, 5) \\\\ \\sigma &amp;\\sim \\text{Cauchy}^{+}(0, 5) \\\\ \\end{align} \\] But how do we interpret a regression coefficient when we have more than one regression coefficient? The standard advice is to interpret an individual coefficient while assuming that all other predictors are held constant. The meaning of each regression coefficient is independent of the others because we are assuming the covariates are independent of one another – for example, that body size has noting to do with trophic level. Is this assumption correct? Probably not. Is it defensible? Possibly. 5.5 Fitting model in brms Now that we’ve been able to write out our complete statistical model, we can not implement it in brms. Our first task limiting our data to not include observations with missing values for our variables of interest. After that, we need to center our continuous predictor so that our intercept has a clean interpretation. After our data is prepared for analysis, we can then write our brm() call. pantheria_fit &lt;- pantheria %&gt;% drop_na(density_log, mass_log, trophic_level) %&gt;% mutate(mass_log_center = mass_log - mean(mass_log), trophic_level = fct_infreq(trophic_level)) m_1 &lt;- pantheria_fit %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_center + trophic_level), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 5), class = b), prior(normal(-1, 5), class = b, coef = mass_log_center), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_center + trophic_level ## Data: . (Number of observations: 746) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept 3.15 0.10 2.95 3.35 4610 ## mass_log_center -0.82 0.02 -0.87 -0.78 4434 ## trophic_levelherbivore 1.30 0.15 1.00 1.60 4493 ## trophic_levelcarnivore -1.31 0.21 -1.71 -0.92 4422 ## Rhat ## Intercept 1.00 ## mass_log_center 1.00 ## trophic_levelherbivore 1.00 ## trophic_levelcarnivore 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.80 0.05 1.72 1.90 4410 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 5.6 Aside: Standardizing Remember, regression coefficients (e.g. \\(\\beta_{1}\\)) are interpreted as the expected change in \\(y\\) per unit change in the predictor (e.g. \\(m\\)). If our predictors are not on the same scale, it is “unfair” to compare them directly. For example, how can we compare the effect size of body mass versus being an herbivore on population density? While the regression coeffcient for being an herbivore (instead of an omnivore) has a greater magnitude than the coefficient for mass, but this is misleading because a complete difference in category to a 1-km\\(^2\\) change in geographic range. A natural solution to this problem is to standardize our predictors so that they are all on the same scale (having the same standard deviation). You might have heard of this as z-scores or something else – we will not be using this language. To standardize our data, Gelman and Hill recommend scaling or predictors by dividing by 2 standard deviations so that “a 1-unit change in the rescaled predictor corresponds to a change from 1 standard deviation blow the mean, to 1 standard deviation above.” But why 2 standard deviations and not 1? By dividing by 2 standard deviations we gain comparability between our continuous covariates and the binary predictors. For example, consider a simple binary variable \\(x\\) where 0 and 1 occur with equal probability. The standard deviation of this variable is then \\(\\sqrt{0.5 \\cdot 0.5} = 0.5\\), which means the standardized variable, \\((x - \\mu_{x}) / (2 \\sigma_{x})\\), takes on the values \\(\\pm 0.5\\), and its coefficient reflects comparisons between \\(x = 0\\) and \\(x = 1\\). In contrast, dividing by 1 standard deviation means that the scaled variable takes on values \\(\\pm 1\\) and its coefficients correspond to half the difference between the two possible values of \\(x\\). Gelman and Hill also state that in complicated regression models with lots of predictors, we can leave our binary inputs as is and just transform our continuous predictors. In that case, the 2 standard deviation rule gives rough comparability in the coeffcieints. I generally just center and rescale my continuous covariates. Our goal should be able to have meaningful interpretations for as many parameters as possible, especially regression coefficients. To ensure this, we should center and scale all continuous predictor variables before analysis. If these transformations aren’t done, than we can have difficulty interpreting our models intercept and can incorrectly and unfairly compare the estimates of our regression coefficients. pantheria_fit2 &lt;- pantheria %&gt;% drop_na(density_log, mass_log, trophic_level) %&gt;% mutate(mass_log_stan = (mass_log - mean(mass_log)) / (2 * sd(mass_log)), trophic_level = fct_infreq(trophic_level)) m_2 &lt;- pantheria_fit2 %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_stan + trophic_level), prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 5), class = b), prior(normal(-1, 5), class = b, coef = mass_log_stan), prior(cauchy(0, 5), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) print(m_2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: density_log ~ 1 + mass_log_stan + trophic_level ## Data: . (Number of observations: 746) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept 3.15 0.10 2.95 3.35 3486 ## mass_log_stan -5.00 0.14 -5.28 -4.73 4164 ## trophic_levelherbivore 1.29 0.15 0.99 1.59 3460 ## trophic_levelcarnivore -1.32 0.21 -1.73 -0.93 4052 ## Rhat ## Intercept 1.00 ## mass_log_stan 1.00 ## trophic_levelherbivore 1.00 ## trophic_levelcarnivore 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.80 0.05 1.71 1.90 4443 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now that we’ve fit our model with standardized data, we can directly compare the regression coefficients. With multiple predictors we might want to know which the relative importance of the predictors or which predictor has the largest effect size. In general, comparing effect sizes just means calculating the probability that the absolute value of the posterior distribution of one coefficient is greater than another. Remember that in math, magnitude means absolute value. So when you compare effect sizes, you are comparing the relative magnitudes of the regression coefficients. For categorical predictors with more than 2 states, we might also want to know which categories are expected to have a greater expected value of \\(y\\) than the other categories. The way parameters are estimated, our regression coefficients only describe the difference in expected \\(y\\) between that state and the intercept/default state, but it does not describe the difference in expected \\(y\\) between that state and any other non-default state. In our example, this means that the carnivora regression coefficient only describes the difference in expected population density between omnivores and carnivores, but not carnivores and herbivore. Luckily, it is very easy to calculate the expected values of \\(y\\) for each of categorical predictor’s states – then all state combinations can be compared directly. Here is a demonstration. m_2 %&gt;% spread_draws(b_Intercept, b_trophic_levelherbivore, b_trophic_levelcarnivore) %&gt;% transmute(omni = b_Intercept, herb = b_Intercept + b_trophic_levelherbivore, carn = b_Intercept + b_trophic_levelcarnivore) %&gt;% gather(key = trophic_state, value = value) %&gt;% ggplot() + geom_density(mapping = aes(x = value, group = trophic_state, fill = trophic_state)) + scale_fill_viridis(discrete = TRUE) The differences in expected \\(y\\) between three states are extremely obvious, but you could imagine calculating the probability that one state is greater than another. Here is the trivial example. m_2 %&gt;% spread_draws(b_Intercept, b_trophic_levelherbivore, b_trophic_levelcarnivore) %&gt;% transmute(omni = b_Intercept, herb = b_Intercept + b_trophic_levelherbivore, carn = b_Intercept + b_trophic_levelcarnivore) %&gt;% dplyr::summarize(omni_gr_herb = sum(omni &gt; herb) / length(omni), carn_gr_herb = sum(carn &gt; herb) / length(omni), carn_gr_omni = sum(carn &gt; omni) / length(omni)) %&gt;% gather(key = comparison, value = probability) ## # A tibble: 3 x 2 ## comparison probability ## &lt;chr&gt; &lt;dbl&gt; ## 1 omni_gr_herb 0 ## 2 carn_gr_herb 0 ## 3 carn_gr_omni 0 5.7 Checking model fit So we’ve been able to fit our model and have (more) interpretable parameters. Our work with this model, however, is not done. As with our previous lessons, we now need to get an idea for how well our model represents our data – can we “trust” inferences based on its parameter estimates? As I’ve stated before, there is no single best way of evaluating model fit. Let’s look at a lot of different aspects of model fit and make conclusions based on this host of evidence. First, we check if the linear part of our model (i.e. the definition of \\(\\mu_{i}\\)) appears the capture mean population density. This means plotting the fitted draws against our data. pantheria_fit2 %&gt;% add_fitted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_stan, y = density_log)) + geom_line(mapping = aes(y = .value, group = .draw), alpha = 1 / 20, colour = &#39;blue&#39;) + geom_point(data = pantheria_fit2, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL The linear predictor appears to cut through the “meat” of our data, though with so few carnivore observations it is hard to tell the quality of fit to those observations from this plot. But the fitted draws are only the linear part of our model, and not our full posterior predictive distribution which includes our estimate of the scale of our data (i.e. \\(\\sigma\\)). Like before, we can generate the posterior predictive distribution of population density for the whole range of mass values and trophic levels. This is technically a type counterfactual plot because we are estimating values for unobserved values of body mass. However, we will overlay our data points over the posterior predictive distribution to visually compare how closely our estimates resemble the observed distribution of values. pantheria_fit2 %&gt;% data_grid(mass_log_stan = seq_range(mass_log_stan, n = 1000), trophic_level) %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = mass_log_stan, y = density_log)) + stat_lineribbon(mapping = aes(y = .prediction), .width = c(0.9, 0.5, 0.1), size = 0) + geom_point(data = pantheria_fit2, size = 2) + scale_fill_brewer() + facet_wrap(~ trophic_level) + NULL Our data appears to sit over our posterior predictive distribution, with the concentration of points corresponding to the tighter credible intervals. However, this plot does not resolve our difficulty with evaluating our models fit to the carnivore observations. We can also compare the density of our observations to those of draws from our posterior predictive distribution, something we have done before. This involves simulating multiple (e.g. 100) datasets from the posterior predictive distribution and visually comparing these distributions to our observed. Because our previous plots weren’t able to help us resolve the quality of our model’s fit to our carnivore observations, let’s also break down our data by trophic level. pantheria_fit2 %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% ggplot(aes(x = .prediction, group = .draw)) + geom_line(stat = &#39;density&#39;, alpha = 0.1, colour = &#39;blue&#39;) + geom_line(stat = &#39;density&#39;, data = pantheria_fit2, mapping = aes(x = density_log, group = NULL), colour = &#39;black&#39;, size = 1.5) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(paste(&#39;Population density &#39;, log(n / km^2))), title = &#39;Population density, actual versus predicted&#39;) + facet_wrap(~ trophic_level) + NULL Let’s also look our model’s ability to reconstruct specific point estimates. This means comparing point estimates from multiple (e.g. 100) posterior predictive datasets to those estimates from observed data. We’ve previously focused on the median population density for the trophic groups, but let’s also take a look at the standard deviation of population density for those groups – perhaps there is heterogeneity in the spread of population density between the groups that we’ve yet to consider. pan_trophic_summary &lt;- pantheria_fit2 %&gt;% group_by(trophic_level) %&gt;% dplyr::summarize(median = median(density_log), sd = sd(density_log)) %&gt;% gather(key, value, median, sd) pantheria_summary_ppc &lt;- pantheria_fit2 %&gt;% add_predicted_draws(model = m_2, n = 100) %&gt;% ungroup() %&gt;% group_by(.draw, trophic_level) %&gt;% dplyr::summarize(median = median(.prediction), sd = sd(.prediction)) %&gt;% ungroup() %&gt;% gather(key, value, median, sd) pantheria_summary_ppc %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &#39;blue&#39;) + geom_vline(data = pan_trophic_summary, mapping = aes(xintercept = value), size = 2) + facet_grid(key ~ trophic_level, scales = &#39;free&#39;, switch = &#39;y&#39;) + labs(x = &#39;Median population density&#39;, y = &#39;&#39;) + NULL ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The current model is doing a much better job at capturing the median population densities compared to our previous model. We also notice some evidence of heterogeneity in standard deviation between our groups – while our model does a good job at approximating the variation in population density of herbivores and omnivores, it fails to approximate that of carnivores. How do we resolve this situation? Do we need to deal with this in order to answer our scientific questions? Are we concerned with the spread of our data just as much as its central tendency? 5.8 Aside: Matrix Notation Regression models are frequently written in a compact form like \\[ \\mu_{i} = \\alpha + \\sum^{n}_{j = 1} \\beta_{j} x_{ji} \\] where \\(j\\) indexes each of the \\(n\\) predictor variables. This statement can be unpacked as \\[ \\mu_{i} = \\alpha + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\ldots + \\beta_{n} x_{ni}. \\] Both of these forms can be read as \\(\\mu\\) is modeled as the sum of an intercept and an additive combination of the products of parameters and predictors. This statement can be expressed more succinctly using matrix notation as \\[ \\mu_{i} = X_{i} \\beta, \\] or even more succinctly as \\[ \\mu = X \\beta \\] where \\(\\mu\\) is a vector of means, is a vector of regression coefficients, and X is a design matrix. A design matrix has as mnay rows as the data, and as many columns as there are predictors plot one. This additional column is filled with 1s. These 1s are multiplied by the intercept and so return the unmodified intercept. Here is a quick visual on vector-matrix multiplication to help refresh your memory on how this all works to fit the vector \\(\\mu\\): You might have realized that we’ve actually interacted with design matrices before when we were exploring how dummy coding is handled – the column of all 1s called Intercept is eactly what I’ve described here. For example, we can peak at the design matrix from the regression model we developed in this lesson. (pantheria_fit2 %&gt;% model_matrix(density_log ~ 1 + mass_log_stan + trophic_level)) ## # A tibble: 746 x 4 ## `(Intercept)` mass_log_stan trophic_levelherbivore trophic_levelcarnivo… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.967 1 0 ## 2 1 0.333 0 0 ## 3 1 0.321 0 0 ## 4 1 0.357 0 1 ## 5 1 0.517 0 1 ## 6 1 1.05 1 0 ## 7 1 1.01 1 0 ## 8 1 -0.235 0 0 ## 9 1 -0.282 0 0 ## 10 1 -0.274 0 0 ## # … with 736 more rows Each row of the design matrix is multiplied by a vector of regression coefficients (which includes the intercept term), which are summed to give the estimate of \\(\\mu\\). 5.9 Summary This lesson covered linear models with more than one predictor variable. We continued our development of a regression model to explain variation in population density in mammals, using data from the PanTHERIA database. We returned to categorical variables and how they can be translated for use in regression models. Our newest model included body mass and trophic level as predictors of population density. We found that this model is an obvious improvement over the single predictor model from the previous less as more adequately describes the differences in population density assocaited with differences in body mass and torphic level. While our model is not a prefect descriptor of the data, it appears adequate for addressing questions relating to differences expected population density but not necessarily adequate for predicting the range of plausible population densities for carnivorous mammals. "],
["logistic-regression.html", "6 Logistic regression 6.1 Objectives 6.2 Introduction 6.3 Foram coiling", " 6 Logistic regression 6.1 Objectives Introduce logistic regression Interpret regression coefficients from logistic regression Assess model adequacy library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, readxl, arm) theme_set(theme_bw()) 6.2 Introduction Not all response variables we might be interested in are continuous variables between \\(-\\infty\\) and \\(\\infty\\). In some cases our variable of interest might be binary (0/1). For example, we might be interested in if a snail shell coils to the left or right, or if a species survives from one time to the next. The globe tossing exercise from an earlier lesson is an example of this type of data. But instead of just asking for the probability of an outcome, we’ll now lean how to incorporate predictor variables into that type of model. Logistic regression is the standard way to model a binary response variable. We will be modeling the response variable, \\(y\\), as following a Bernoulli distribution. The Bernoulli distribution has a single parameter, \\(\\theta\\), which is the probability of a “positive” outcome i.e. a 1 and not a 0. The Bernoulli distribution is related to the Binomial distribution introduced with the globe tossing exercise, but describes the probability of individual events as opposed to the probability of those “positive” events in aggregate. Just because the type of the response variable has changed doesn’t mean we have to completely reinvent the wheel. While we aren’t going to use the Gaussian distribution to model our outcome like in linear regression, we’re still going to use an additive series of covariates multiplied by regression coefficients. But we have one huge complication: the Bernoulli distribution’s only parameter \\(\\theta\\) is defined from 0 to 1 while a linear model \\(X \\beta\\) is defined from \\(-\\infty\\) to \\(\\infty\\). We need a tool (i.e. mathematical function) to make these definitions talk to each other. The logit, or log-odds, function does this exactly. The logit function is defined as the logarithm of the odds of some probability \\(p\\): \\[ \\text{logit}(p) = \\text{log} \\frac{p}{1 - p}. \\] In the case of logistic regression, we would substitute the \\(\\theta\\) parameter from the Binomial which gives us an expression to map out linear model to: \\[ \\text{logit}(\\theta) = \\text{log} \\frac{\\theta}{1 - \\theta} = X \\beta. \\] Sometimes you will see the previous statement written using the inverse logit function, which is actually called the logitic function, which is defined \\[ \\theta = \\text{logit}^{-1} (X \\beta) = \\frac{\\exp^{X \\beta}}{1 + \\exp^{X \\beta}}. \\] The logistic function is where logistic regression gets its name. Let’s take a look at how the logistic transformation maps values from \\(-\\infty\\) to \\(\\infty\\) onto the (0, 1) space. df &lt;- tibble(x = seq(from = -10, to = 10, by = .1)) %&gt;% mutate(y = arm::invlogit(x)) # inverse logit df %&gt;% ggplot() + geom_line(aes(x = x, y = y)) This plot of the logistic function is very revealing. When x has a magnitude of 3 , y is either close to its maxima or minima. This behaviour means that the approximate slope of the logistic function for values of x between -3 and 3 is much larger than the approximate slope of the logistic function for values of x with a magnitude of 3+. We will explore the importance of this behaviour in a latter part of this lesson. Mapping functions, like the logit and logistic functions, let lot us model a parameter by a linear model, is frequently referred to as a link function. Now that we’ve defined a way to model the \\(\\theta\\) parameter of the Bernoulli distribution as a linear function, we can start building a logistic regression and explore how to interpret our model’s regression coefficients. 6.3 Foram coiling For this scion, we will develop a logistic regression model to describe the probability of a foram being dextrally coiling as a function of multiple measures of that foram’s size and shape. To get to that point, we will first explore the foram measurement data. Once we have a handle on the data, we’ll develop and write out a Bayesian logistic regression model and then fit that model using brms. Finally, as always, we’ll then explore the adequacy of our model at describing out data. The data we will be focusing on is from a paper by Pearson and Ezard that explored changes to morphology associated with speciation. In the original study, the morpholgical measures were analyzed as time series in an effort to characterize their evolution. For this lesson, we are instead tackling a much simpler question: do measures of foram size and shape predict if an individual is dextrally coiled or not? Are differently coiled forams simple mirror images or is there something more going on? We’re going to be ignoring the time structure of the data for now. We’ll cover time series models in a later lesson. For now, we are just interested how size and shape measures predict coiling with the assumption that this relationship is constant over time. Let’s bring the data into the memory and start playing around with it. The Peason and Ezard paper describes a few derived measures, like compression ratio, that we can quickly recalculate. # read in excel data directly with function from readxl (foram &lt;- read_xlsx(path = here::here(&#39;data&#39;, &#39;pearson_ezard&#39;, &#39;turborotalia_data.xlsx&#39;)) %&gt;% janitor::clean_names() %&gt;% # useful derived measures mutate(compression = diameter / axis, chamber_aspect = chamber_height / chamber_width, aperture_aspect = aperture_height / aperture_width, dextral = as.factor(dextral))) ## # A tibble: 10,200 x 22 ## core section interval depth age id baseline diameter axis radius ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3H 1 18.5-18… 18.5 34.7 1 389. 597. 381. 252. ## 2 3H 1 18.5-18… 18.5 34.7 2 388. 571. 364. 244. ## 3 3H 1 18.5-18… 18.5 34.7 3 387. 570. 355. 242. ## 4 3H 1 18.5-18… 18.5 34.7 4 367. 513. 341. 244. ## 5 3H 1 18.5-18… 18.5 34.7 5 335. 554. 341. 292. ## 6 3H 1 18.5-18… 18.5 34.7 6 319. 518. 308. 218. ## 7 3H 1 18.5-18… 18.5 34.7 7 293. 526. 259. 207. ## 8 3H 1 18.5-18… 18.5 34.7 8 445. 703. 463. 319. ## 9 3H 1 18.5-18… 18.5 34.7 9 387. 567. 351. 228. ## 10 3H 1 18.5-18… 18.5 34.7 10 290. 496. 286. 229. ## # … with 10,190 more rows, and 12 more variables: aperture_width &lt;dbl&gt;, ## # aperture_height &lt;dbl&gt;, chamber_width &lt;dbl&gt;, chamber_height &lt;dbl&gt;, ## # umbilical_angle &lt;dbl&gt;, periphery &lt;dbl&gt;, area &lt;dbl&gt;, ## # chamber_number &lt;dbl&gt;, dextral &lt;fct&gt;, compression &lt;dbl&gt;, ## # chamber_aspect &lt;dbl&gt;, aperture_aspect &lt;dbl&gt; A common occurrence with measurement data is that many of the measures are derived from each other. The obvious examples here are the aspect ratios – these are directly calculated from 2 other measures. So which measures do we include in our analyses? I’m going to use the original paper as a guide and stick with compression, area, umbilical angle, aperture aspect ratio, chamber aspect ratio, and the number of chambers in the final whorl. foram %&gt;% dplyr::select(dextral, compression, area, umbilical_angle, aperture_aspect, chamber_aspect, chamber_number) %&gt;% gather(key = key, value = value, -dextral) %&gt;% ggplot(aes(x = value, fill = dextral)) + geom_histogram() + facet_wrap(~ key, scales = &#39;free&#39;) + scale_fill_viridis(discrete = TRUE) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.3.1 Writing out a model Now that we’ve defined our response variable (dextral) and isolated our predictors of interest, we can now start describing out model to predict if a specimen will be dextrally or sinistrally coiled. Let’s define the coiling of each observation as \\(y_{i}\\) where \\(i\\), where \\(i = 1, 2, ..., N\\) and \\(N\\) is the total number of observations. We can then define \\(X\\) as our design matrix which is an \\(N\\) by \\(D + 1\\) matrix, where \\(D\\) is the number of covariates. The additional column in \\(X\\) is entirely 1s, so that it corresponds to the intercept term of the regression. Given these definitions, we can write out most of our logistic regression model. \\[ \\begin{align} y_{i} &amp;\\sim \\text{Bernoulli}(\\theta) \\\\ \\text{logit}(\\theta) &amp;= X_{i} \\beta \\\\ \\end{align} \\] What key component(s) of this model are missing? When thinking about how to define the priors for our logistic regression model, let’s return to our graph of the logistic function foram_ready &lt;- foram %&gt;% dplyr::select(dextral, compression, area, umbilical_angle, aperture_aspect, chamber_aspect, chamber_number) %&gt;% mutate_at(.vars = vars(-dextral), # mean center, 2*sd standardize .funs = list(rescale = ~ arm::rescale(.))) %&gt;% dplyr::select(dextral, ends_with(&#39;rescale&#39;)) m_1 &lt;- foram_ready %&gt;% brm(data = ., family = bernoulli(), formula = bf(dextral ~ .), prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) ## Compiling the C++ model ## Start sampling "],
["poisson-regression-and-others-glms.html", "7 Poisson regression and others GLMs 7.1 Outline 7.2 Poisson distribution", " 7 Poisson regression and others GLMs 7.1 Outline library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot) theme_set(theme_bw()) 7.2 Poisson distribution #df &lt;- tibble(x = seq(from = 0, to = 20, by = 1)) %&gt;% # mutate(lambda_01 = dpois(x, lambda = 0.1)) # #df %&gt;% # gather(key = &#39;key&#39;, value = &#39;value&#39;, -x) %&gt;% # separate(key, c(&#39;type&#39;, &#39;scale&#39;)) %&gt;% # mutate(scale = factor(scale, levels = sort(order(scale)))) %&gt;% # ggplot(aes(x = x, y = value, colour = scale)) + # geom_line(size = 2) + # scale_y_continuous(NULL, breaks = NULL) + # scale_colour_viridis(discrete = TRUE) + # labs(colour = &#39;Scale&#39;) + # NULL "],
["varying-intercepts.html", "8 Varying-intercept(s) 8.1 Objectives", " 8 Varying-intercept(s) 8.1 Objectives Introduce multilevel structures Introduce varying intercept models library(pacman) p_load(tidyverse, here, janitor, purrr, viridis, brms, tidybayes, bayesplot, modelr, forcats) theme_set(theme_bw()) Motivation Simpson’s Paradox Dummy coding versus varying-intercepts. varying intercepts reframe problem. now the intercept is “universal” and the codes are their divergence from the mean. In both cases each group gets its “own intercept” but how that value is calculated differs. Also, in varying intercept models we can put a prior on the variance between the groups. ARM: with grouped data, a regression that includes indicators for groups is called a varying-intercept model because it can be interepreted as a model with a different intercept for each group. \\[ \\begin{align} y_{i} &amp;= \\text{Normal}(\\alpha_{j[i]} + \\beta x_{i}, \\sigma) \\end{align} \\] Clustered data pantheria &lt;- read_tsv(here(&#39;data&#39;, &#39;PanTHERIA_1-0_WR05_Aug2008.txt&#39;), na = &#39;-999.00&#39;) %&gt;% clean_names() %&gt;% rename(order = msw05_order, family = msw05_family, genus = msw05_genus, species = msw05_species, binomial = msw05_binomial) %&gt;% mutate(mass_log = log(x5_1_adult_body_mass_g), range_group_log = log(x22_1_home_range_km2), range_indiv_log = log(x22_2_home_range_indiv_km2), density_log = log(x21_1_population_density_n_km2), activity_cycle = case_when(x1_1_activity_cycle == 1 ~ &#39;nocturnal&#39;, x1_1_activity_cycle == 2 ~ &#39;mixed&#39;, x1_1_activity_cycle == 3 ~ &#39;diurnal&#39;), trophic_level = case_when(x6_2_trophic_level == 1 ~ &#39;herbivore&#39;, x6_2_trophic_level == 2 ~ &#39;omnivore&#39;, x6_2_trophic_level == 3 ~ &#39;carnivore&#39;)) %&gt;% drop_na(density_log, mass_log, trophic_level, order, family, genus) %&gt;% mutate(mass_log_stan = (mass_log - mean(mass_log)) / (2 * sd(mass_log)), trophic_level = fct_infreq(trophic_level)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## MSW05_Order = col_character(), ## MSW05_Family = col_character(), ## MSW05_Genus = col_character(), ## MSW05_Species = col_character(), ## MSW05_Binomial = col_character(), ## References = col_character() ## ) ## See spec(...) for full column specifications. m_1 &lt;- pantheria %&gt;% brm(data = ., family = gaussian(), formula = bf(density_log ~ 1 + mass_log_stan + trophic_level + (1 | order)), prior = c(prior(normal(0, 10), class = &#39;Intercept&#39;), prior(normal(0, 5), class = &#39;b&#39;), prior(normal(-1, 5), class = &#39;b&#39;, coef = &#39;mass_log_stan&#39;), prior(cauchy(0, 5), class = &#39;sigma&#39;), prior(cauchy(0, 1), class = &#39;sd&#39;, group = &#39;order&#39;)), iter = 2000, warmup = 1000, chains = 4, cores = 4, refresh = 0) Repeated measurements "],
["varying-slopes.html", "9 Varying slope(s)", " 9 Varying slope(s) "],
["varying-slopes-and-intercepts.html", "10 Varying slopes and intercepts", " 10 Varying slopes and intercepts "]
]
