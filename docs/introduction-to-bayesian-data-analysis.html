<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2 Introduction to Bayesian data analysis | Analytical Paleobiology</title>
  <meta name="description" content="An informal course on analytical paleobiology">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2 Introduction to Bayesian data analysis | Analytical Paleobiology" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An informal course on analytical paleobiology" />
  <meta name="github-repo" content="psmits/paleo_book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Introduction to Bayesian data analysis | Analytical Paleobiology" />
  <meta name="twitter:site" content="@PeterDSmits" />
  <meta name="twitter:description" content="An informal course on analytical paleobiology" />
  

<meta name="author" content="Peter D Smits">


<meta name="date" content="2019-04-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="managing-and-processing-data-from-the-paleobiology-database.html">
<link rel="next" href="reg-intro.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="...">Short-Course on Analytical Paleobiology</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html"><i class="fa fa-check"></i><b>1</b> Managing and Processing Data From the Paleobiology Database</a><ul>
<li class="chapter" data-level="1.1" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#objectives"><i class="fa fa-check"></i><b>1.1</b> Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#project-reading"><i class="fa fa-check"></i><b>1.2</b> Reading</a></li>
<li class="chapter" data-level="1.3" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#introduction"><i class="fa fa-check"></i><b>1.3</b> Introduction</a></li>
<li class="chapter" data-level="1.4" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#getting-data"><i class="fa fa-check"></i><b>1.4</b> Getting data</a></li>
<li class="chapter" data-level="1.5" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#processing-data"><i class="fa fa-check"></i><b>1.5</b> Processing data</a></li>
<li class="chapter" data-level="1.6" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#binning-observations"><i class="fa fa-check"></i><b>1.6</b> Binning observations</a></li>
<li class="chapter" data-level="1.7" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#sharing-data"><i class="fa fa-check"></i><b>1.7</b> Sharing data</a></li>
<li class="chapter" data-level="1.8" data-path="managing-and-processing-data-from-the-paleobiology-database.html"><a href="managing-and-processing-data-from-the-paleobiology-database.html#summary"><i class="fa fa-check"></i><b>1.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#objectives-1"><i class="fa fa-check"></i><b>2.1</b> Objectives</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#reading"><i class="fa fa-check"></i><b>2.2</b> Reading</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#learning-from-data"><i class="fa fa-check"></i><b>2.3</b> Learning from data</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#counting-and-plausibility"><i class="fa fa-check"></i><b>2.3.1</b> Counting and plausibility</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#globe-example"><i class="fa fa-check"></i><b>2.4</b> Building a model</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#a-data-story"><i class="fa fa-check"></i><b>2.4.1</b> A data story</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#bayesian-updating"><i class="fa fa-check"></i><b>2.4.2</b> Bayesian updating</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#evaluate"><i class="fa fa-check"></i><b>2.4.3</b> Evaluate</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#terms-and-theory"><i class="fa fa-check"></i><b>2.5</b> Terms and theory</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#bayes-theorem"><i class="fa fa-check"></i><b>2.6</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#but-how-does-it-work"><i class="fa fa-check"></i><b>2.7</b> But how does it <em>work</em>?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#grid-approximation"><i class="fa fa-check"></i><b>2.7.1</b> Grid approximation</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>2.7.2</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="2.7.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#aside-on-interpreting-probabilities"><i class="fa fa-check"></i><b>2.7.3</b> Aside on Interpreting Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#working-with-samples"><i class="fa fa-check"></i><b>2.8</b> Working with samples</a><ul>
<li class="chapter" data-level="2.8.1" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#intervals-of-defined-boundaries"><i class="fa fa-check"></i><b>2.8.1</b> Intervals of defined boundaries</a></li>
<li class="chapter" data-level="2.8.2" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#intervals-of-defined-mass"><i class="fa fa-check"></i><b>2.8.2</b> Intervals of defined mass</a></li>
<li class="chapter" data-level="2.8.3" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#point-estimates"><i class="fa fa-check"></i><b>2.8.3</b> Point estimates</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="introduction-to-bayesian-data-analysis.html"><a href="introduction-to-bayesian-data-analysis.html#summary-1"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="reg-intro.html"><a href="reg-intro.html"><i class="fa fa-check"></i><b>3</b> Introduction to linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="reg-intro.html"><a href="reg-intro.html#objectives-2"><i class="fa fa-check"></i><b>3.1</b> Objectives</a></li>
<li class="chapter" data-level="3.2" data-path="reg-intro.html"><a href="reg-intro.html#reading-1"><i class="fa fa-check"></i><b>3.2</b> Reading</a></li>
<li class="chapter" data-level="3.3" data-path="reg-intro.html"><a href="reg-intro.html#linear-regression"><i class="fa fa-check"></i><b>3.3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="reg-intro.html"><a href="reg-intro.html#talking-about-models"><i class="fa fa-check"></i><b>3.3.1</b> Talking about models</a></li>
<li class="chapter" data-level="3.3.2" data-path="reg-intro.html"><a href="reg-intro.html#growing-a-regression-model"><i class="fa fa-check"></i><b>3.3.2</b> Growing a regression model</a></li>
<li class="chapter" data-level="3.3.3" data-path="reg-intro.html"><a href="reg-intro.html#sampling-from-the-model"><i class="fa fa-check"></i><b>3.3.3</b> Sampling from the model</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="reg-intro.html"><a href="reg-intro.html#adding-a-predictor-to-the-mix"><i class="fa fa-check"></i><b>3.4</b> Adding a predictor to the mix</a><ul>
<li class="chapter" data-level="3.4.1" data-path="reg-intro.html"><a href="reg-intro.html#aside-dummy-coding"><i class="fa fa-check"></i><b>3.4.1</b> Aside: Dummy coding</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="reg-intro.html"><a href="reg-intro.html#interpreting-the-model-fit"><i class="fa fa-check"></i><b>3.5</b> Interpreting the model fit</a><ul>
<li class="chapter" data-level="3.5.1" data-path="reg-intro.html"><a href="reg-intro.html#linear-predictor"><i class="fa fa-check"></i><b>3.5.1</b> Linear predictor</a></li>
<li class="chapter" data-level="3.5.2" data-path="reg-intro.html"><a href="reg-intro.html#posterior-prediction"><i class="fa fa-check"></i><b>3.5.2</b> Posterior prediction</a></li>
<li class="chapter" data-level="3.5.3" data-path="reg-intro.html"><a href="reg-intro.html#posterior-predictive-tests"><i class="fa fa-check"></i><b>3.5.3</b> Posterior predictive tests</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="reg-intro.html"><a href="reg-intro.html#summary-2"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="reg-continue.html"><a href="reg-continue.html"><i class="fa fa-check"></i><b>4</b> Continuing with regression with continuous predictors</a><ul>
<li class="chapter" data-level="4.1" data-path="reg-continue.html"><a href="reg-continue.html#objectives-3"><i class="fa fa-check"></i><b>4.1</b> Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="reg-continue.html"><a href="reg-continue.html#reading-2"><i class="fa fa-check"></i><b>4.2</b> Reading</a></li>
<li class="chapter" data-level="4.3" data-path="reg-continue.html"><a href="reg-continue.html#our-first-example"><i class="fa fa-check"></i><b>4.3</b> Our first example</a></li>
<li class="chapter" data-level="4.4" data-path="reg-continue.html"><a href="reg-continue.html#a-single-continuous-predictor"><i class="fa fa-check"></i><b>4.4</b> A single continuous predictor</a><ul>
<li class="chapter" data-level="4.4.1" data-path="reg-continue.html"><a href="reg-continue.html#centering"><i class="fa fa-check"></i><b>4.4.1</b> Aside: Centering</a></li>
<li class="chapter" data-level="4.4.2" data-path="reg-continue.html"><a href="reg-continue.html#continue-ppc"><i class="fa fa-check"></i><b>4.4.2</b> Checking model fit</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="reg-continue.html"><a href="reg-continue.html#summary-3"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Multiple predictors in linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#objectives-4"><i class="fa fa-check"></i><b>5.1</b> Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#reading-3"><i class="fa fa-check"></i><b>5.2</b> Reading</a></li>
<li class="chapter" data-level="5.3" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#more-than-one-predictor"><i class="fa fa-check"></i><b>5.3</b> More than one predictor</a><ul>
<li class="chapter" data-level="5.3.1" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#dummy"><i class="fa fa-check"></i><b>5.3.1</b> Categorical predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#defining-our-model"><i class="fa fa-check"></i><b>5.4</b> Defining our model</a></li>
<li class="chapter" data-level="5.5" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#fitting-model-in-brms"><i class="fa fa-check"></i><b>5.5</b> Fitting model in <code>brms</code></a></li>
<li class="chapter" data-level="5.6" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#aside-standardizing"><i class="fa fa-check"></i><b>5.6</b> Aside: Standardizing</a></li>
<li class="chapter" data-level="5.7" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#multi-ppc"><i class="fa fa-check"></i><b>5.7</b> Checking model fit</a></li>
<li class="chapter" data-level="5.8" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#matrix-notation"><i class="fa fa-check"></i><b>5.8</b> Aside: Matrix Notation</a></li>
<li class="chapter" data-level="5.9" data-path="multiple-predictors-in-linear-regression.html"><a href="multiple-predictors-in-linear-regression.html#summary-4"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="interactions.html"><a href="interactions.html"><i class="fa fa-check"></i><b>6</b> Interactions</a><ul>
<li class="chapter" data-level="6.1" data-path="interactions.html"><a href="interactions.html#objectives-5"><i class="fa fa-check"></i><b>6.1</b> Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="interactions.html"><a href="interactions.html#reading-4"><i class="fa fa-check"></i><b>6.2</b> Reading</a></li>
<li class="chapter" data-level="6.3" data-path="interactions.html"><a href="interactions.html#introduction-1"><i class="fa fa-check"></i><b>6.3</b> Introduction</a></li>
<li class="chapter" data-level="6.4" data-path="interactions.html"><a href="interactions.html#inter-initial"><i class="fa fa-check"></i><b>6.4</b> Data and inital model</a></li>
<li class="chapter" data-level="6.5" data-path="interactions.html"><a href="interactions.html#how-to-specify-an-interaction"><i class="fa fa-check"></i><b>6.5</b> How to specify an interaction</a><ul>
<li class="chapter" data-level="6.5.1" data-path="interactions.html"><a href="interactions.html#symmetry-of-interactions"><i class="fa fa-check"></i><b>6.5.1</b> Symmetry of interactions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="interactions.html"><a href="interactions.html#fitting-a-model-with-an-interaction"><i class="fa fa-check"></i><b>6.6</b> Fitting a model with an interaction</a></li>
<li class="chapter" data-level="6.7" data-path="interactions.html"><a href="interactions.html#interpreting-our-model"><i class="fa fa-check"></i><b>6.7</b> Interpreting our model</a></li>
<li class="chapter" data-level="6.8" data-path="interactions.html"><a href="interactions.html#have-we-improved-on-our-previous-model"><i class="fa fa-check"></i><b>6.8</b> Have we improved on our previous model?</a></li>
<li class="chapter" data-level="6.9" data-path="interactions.html"><a href="interactions.html#introducing-root-mean-square-error"><i class="fa fa-check"></i><b>6.9</b> Introducing root mean square error</a></li>
<li class="chapter" data-level="6.10" data-path="interactions.html"><a href="interactions.html#continuous-x-continuous"><i class="fa fa-check"></i><b>6.10</b> continuous X continuous</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic regression</a><ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#objectives-6"><i class="fa fa-check"></i><b>7.1</b> Objectives</a></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#reading-5"><i class="fa fa-check"></i><b>7.2</b> Reading</a></li>
<li class="chapter" data-level="7.3" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-2"><i class="fa fa-check"></i><b>7.3</b> Introduction</a></li>
<li class="chapter" data-level="7.4" data-path="logistic-regression.html"><a href="logistic-regression.html#foram-coiling"><i class="fa fa-check"></i><b>7.4</b> Foram coiling</a></li>
<li class="chapter" data-level="7.5" data-path="logistic-regression.html"><a href="logistic-regression.html#writing-out-a-model"><i class="fa fa-check"></i><b>7.5</b> Writing out a model</a><ul>
<li class="chapter" data-level="7.5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-logistic-regression-coefficients"><i class="fa fa-check"></i><b>7.5.1</b> Interpreting logistic regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="logistic-regression.html"><a href="logistic-regression.html#priors-for-our-model"><i class="fa fa-check"></i><b>7.6</b> Priors for our model</a></li>
<li class="chapter" data-level="7.7" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-our-model"><i class="fa fa-check"></i><b>7.7</b> Fitting our model</a></li>
<li class="chapter" data-level="7.8" data-path="logistic-regression.html"><a href="logistic-regression.html#checking-model-adequacy"><i class="fa fa-check"></i><b>7.8</b> Checking model adequacy</a></li>
<li class="chapter" data-level="7.9" data-path="logistic-regression.html"><a href="logistic-regression.html#summary-5"><i class="fa fa-check"></i><b>7.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html"><i class="fa fa-check"></i><b>8</b> Poisson regression and others GLMs</a><ul>
<li class="chapter" data-level="8.1" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html#outline"><i class="fa fa-check"></i><b>8.1</b> Outline</a></li>
<li class="chapter" data-level="8.2" data-path="poisson-regression-and-others-glms.html"><a href="poisson-regression-and-others-glms.html#poisson-distribution"><i class="fa fa-check"></i><b>8.2</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="varying-intercepts.html"><a href="varying-intercepts.html"><i class="fa fa-check"></i><b>9</b> Varying-intercept(s)</a><ul>
<li class="chapter" data-level="9.1" data-path="varying-intercepts.html"><a href="varying-intercepts.html#objectives-7"><i class="fa fa-check"></i><b>9.1</b> Objectives</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="varying-slopes.html"><a href="varying-slopes.html"><i class="fa fa-check"></i><b>10</b> Varying slope(s)</a></li>
<li class="chapter" data-level="11" data-path="varying-slopes-and-intercepts.html"><a href="varying-slopes-and-intercepts.html"><i class="fa fa-check"></i><b>11</b> Varying slopes and intercepts</a></li>
<li class="chapter" data-level="12" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>12</b> Time Series</a><ul>
<li class="chapter" data-level="12.1" data-path="time-series.html"><a href="time-series.html#objectives-8"><i class="fa fa-check"></i><b>12.1</b> Objectives</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown">
Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Analytical Paleobiology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-bayesian-data-analysis" class="section level1">
<h1><span class="header-section-number">2</span> Introduction to Bayesian data analysis</h1>
<blockquote>
<p>Statistical inference is concerned with drawing conclusions, from numerical data, about quantities that are not observed. – Gelman et al. <strong>BDA3</strong>: 4.</p>
</blockquote>
<blockquote>
<p>Bayesian data analysis is just a logical procedure for processing information. – McElreath <strong>Statistical Rethinking</strong>: 12.</p>
</blockquote>
<div id="objectives-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Objectives</h2>
<ul>
<li>Learn what a (Bayesian) model actually means.</li>
<li>Introduce the logic behind Bayesian updating.</li>
<li>Dip our toes into numerical methods using <strong>brms</strong>.</li>
<li>Cover the basics of summarizing a posterior distribution using <strong>tidybayes</strong>.</li>
</ul>
</div>
<div id="reading" class="section level2">
<h2><span class="header-section-number">2.2</span> Reading</h2>
<p>The following materials are recommended prereadings before starting this tutorial.</p>
<ul>
<li>Chapter 1 “The Golem of Prague” from <a href="https://xcelab.net/rm/statistical-rethinking/"><strong>Statistical Rethinking</strong> by Richard McElreath</a>.</li>
<li>Chapter 2 “Small Worlds and Large Worlds” from <a href="https://xcelab.net/rm/statistical-rethinking/"><strong>Statistical Rethinking</strong> by Richard McElreath</a>.</li>
<li>OPTIONAL Chapter 3 “Sampling the Imaginary” from <a href="https://xcelab.net/rm/statistical-rethinking/"><strong>Statistical Rethinking</strong> by Richard McElreath</a>.</li>
</ul>
<p>Most of this section is based on material presented in <strong>Statistical Rethinking</strong> and code material from <a href="https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/">Solomon Kurz’s rewrite of <strong>Statistical Rethinking</strong></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pacman)

<span class="kw">p_load</span>(tidyverse, brms, tidybayes)

<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</code></pre></div>
</div>
<div id="learning-from-data" class="section level2">
<h2><span class="header-section-number">2.3</span> Learning from data</h2>
<p>Bayesian inference is a fancy way of counting and comparing possibilities. As we collect and analyze our data, we learn which possibilities are more plausible than others. The logical strategy is “When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible.”</p>
<div id="counting-and-plausibility" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Counting and plausibility</h3>
<p>We’re going to use a simple example from <strong>Statistical Rethinking</strong> to start our thinking about Bayesian analysis.</p>
<p><strong>Setup</strong>: We’ve got a bag with four marbles. Each marble can be either blue (B) or white (W). We don’t know the composition of the bag. Our goal is to figure out which of possible configuration is most plausible, given whatever evidence we learn about the bag. A sequence of three marbles have pulled from the bag, one at a time, returned to the bag, and then the bag is shaken before drawing another. We draw the sequence [B W B].</p>
<p>How many ways are there to produce this draw? Try first this possible bag composition: [B W W W].</p>
<p>Here is the full table of ways to produce [B W B] given each possible bag composition.</p>
<table>
<thead>
<tr class="header">
<th>conjecture</th>
<th>ways to produce [B W B]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[W W W W]</td>
<td>0 x 4 x 0 = 0</td>
</tr>
<tr class="even">
<td>[B W W W]</td>
<td>1 x 3 x 1 = 3</td>
</tr>
<tr class="odd">
<td>[B B W W]</td>
<td>2 x 2 x 2 = 8</td>
</tr>
<tr class="even">
<td>[B B B W]</td>
<td>3 x 1 x 3 = 9</td>
</tr>
<tr class="odd">
<td>[B B B B]</td>
<td>4 x 0 x 4 = 0</td>
</tr>
</tbody>
</table>
<p>What happens when we draw another marble from the bag? We update the counts! How do we update the counts? We multiply the prior counts by the new count, with the old counts are acting as our prior counts. Here is an example how we would update our counts if we were to draw an additional [B].</p>
<table>
<thead>
<tr class="header">
<th>conjecture</th>
<th>ways to produce [B]</th>
<th>previous counts</th>
<th>new count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[W W W W]</td>
<td>0</td>
<td>0</td>
<td>0 x 0 = 0</td>
</tr>
<tr class="even">
<td>[B W W W]</td>
<td>1</td>
<td>3</td>
<td>3 x 1 = 3</td>
</tr>
<tr class="odd">
<td>[B B W W]</td>
<td>2</td>
<td>8</td>
<td>8 x 2 = 16</td>
</tr>
<tr class="even">
<td>[B B B W]</td>
<td>3</td>
<td>9</td>
<td>9 x 3 = 27</td>
</tr>
<tr class="odd">
<td>[B B B B]</td>
<td>4</td>
<td>0</td>
<td>0 x 4 = 0</td>
</tr>
</tbody>
</table>
<p>Logically, what we’ve just done can be expressed as</p>
<center>
plausibility of [B W W W] after seeing [B W B] <span class="math inline">\(\propto\)</span> ways [B W W W] can produce [B W B] X prior plausibility of [B W W W].
</center>
<p>But these are just counts and plausibilities; we want probabilities! First, to make explaining ourselves simpler, let’s define <span class="math inline">\(p\)</span> as the proportion of blue marbles in the bag and <span class="math inline">\(D_{new}\)</span> as our data. We can now rewrite the previous statement as:</p>
<center>
plausibility of <span class="math inline">\(p\)</span> after <span class="math inline">\(D_{new}\)</span> <span class="math inline">\(\propto\)</span> ways <span class="math inline">\(p\)</span> can produce <span class="math inline">\(D_{new}\)</span> X prior plausibility of <span class="math inline">\(p\)</span>.
</center>
<p>We want to standardize the measure of plausibility so that the sum of the plausibilities for all conjectures sums to 1. To standardize, we add up all the products, one for each <span class="math inline">\(p\)</span>, then divide each product by the sum of the products.</p>
<table>
<thead>
<tr class="header">
<th>possible combinations</th>
<th><span class="math inline">\(p\)</span></th>
<th>ways to produce data</th>
<th>plausibility</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[W W W W]</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>[B W W W]</td>
<td>0.25</td>
<td>3</td>
<td>0.15</td>
</tr>
<tr class="odd">
<td>[B B W W]</td>
<td>0.5</td>
<td>8</td>
<td>0.40</td>
</tr>
<tr class="even">
<td>[B B B W]</td>
<td>0.75</td>
<td>9</td>
<td>0.45</td>
</tr>
<tr class="odd">
<td>[B B B B]</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>This process is equivalent to</p>
<center>
plausibility of <span class="math inline">\(p\)</span> after <span class="math inline">\(D_{new}\)</span> = (ways <span class="math inline">\(p\)</span> can produce <span class="math inline">\(D_{new}\)</span> X prior plausibility <span class="math inline">\(p\)</span>) / sum of products
</center>
<p>Each part of the calculations we’ve done so far correspond directly to quantities in applied probability theory.</p>
<ul>
<li>The conjectured proportion of blue marbles, <span class="math inline">\(p\)</span>, is usually called a <em>parameter</em>.</li>
<li>The relative number of ways that <span class="math inline">\(p\)</span> can produce the data is usually called the <em>likelihood</em>.</li>
<li>The prior plausibility of a specific <span class="math inline">\(p\)</span> is called the <em>prior probability</em>.</li>
<li>The updated plausibility of any specific <span class="math inline">\(p\)</span> is called the <em>posterior probability</em>.</li>
</ul>
</div>
</div>
<div id="globe-example" class="section level2">
<h2><span class="header-section-number">2.4</span> Building a model</h2>
<p>Bayesian inference is made easier by working with probabilities instead of counts, but this makes everything look a lot harder. Let’s explore this with a new problem.</p>
<p><strong>Setup</strong>: We’ve a globe with land and water sections. We want to know how much of the globe is water. Our strategy is to throw the globe up, catch it, and note the surface under the right index finger. Let’s do this 9 times.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">toss =</span> <span class="kw">c</span>(<span class="st">&quot;w&quot;</span>, <span class="st">&quot;l&quot;</span>, <span class="st">&quot;w&quot;</span>, <span class="st">&quot;w&quot;</span>, <span class="st">&quot;w&quot;</span>, <span class="st">&quot;l&quot;</span>, <span class="st">&quot;w&quot;</span>, <span class="st">&quot;l&quot;</span>, <span class="st">&quot;w&quot;</span>))

<span class="co"># then rephrase in terms of trials and count of water</span>
(d &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">n_trials  =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>,
        <span class="dt">n_success =</span> <span class="kw">cumsum</span>(toss <span class="op">==</span><span class="st"> &quot;w&quot;</span>)))</code></pre></div>
<pre><code>## # A tibble: 9 x 3
##   toss  n_trials n_success
##   &lt;chr&gt;    &lt;int&gt;     &lt;int&gt;
## 1 w            1         1
## 2 l            2         1
## 3 w            3         2
## 4 w            4         3
## 5 w            5         4
## 6 l            6         4
## 7 w            7         5
## 8 l            8         5
## 9 w            9         6</code></pre>
<p>To get the logic machinery working, we need to make some assumptions. These assumptions constitute our model. An ideal design loop for developing a Bayesian model has three steps:</p>
<ol style="list-style-type: decimal">
<li><strong>Data story</strong>
<ul>
<li>Describe aspects of the underlying reality and the sampling process.</li>
<li>Translate this description into a formal probability model.</li>
<li>Acts as a framework for interpretation, but still just a story.</li>
<li>Helps with realizing additional questions that must be answered as hypotheses are frequently vague.</li>
</ul></li>
<li><strong>Updating</strong>
<ul>
<li>Bayesian models begin with a set of plausibilities assigned to each possibility (<em>Prior</em>).</li>
<li>Update those plausibilities based on the data to give <em>posterior</em> plausibility.</li>
</ul></li>
<li><strong>Evaluate</strong>
<ul>
<li>Certainty is no guarantee that the model is good or accurate.</li>
<li>Supervise and critique your model!</li>
<li>Check model’s adequacy for some purpose, or in light of stuff we don’t know.</li>
</ul></li>
</ol>
<div id="a-data-story" class="section level3">
<h3><span class="header-section-number">2.4.1</span> A data story</h3>
<p>How did the data come to be? A data story is a description of the aspects of the reality underlying our data, including the sampling process. This story should be sufficient to specifying an algorithm to simulate new data. Write out the data story for the globe tossing activity.</p>
</div>
<div id="bayesian-updating" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Bayesian updating</h3>
<p>A Bayesian model begins with one set of plausibilities assigned to each possible result: the prior plausibilities. These values are updated in light of data to produce our posterior plausibilities. This process is called <em>Bayesian updating</em>.</p>
<p>Here is an illustration of Bayesian updating applied to our globe tossing experiment. The posterior generated at each observation becomes the prior for the subsequent observation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sequence_length &lt;-<span class="st"> </span><span class="dv">50</span>                  <span class="co"># how many points to calculate prob for</span>

d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">expand</span>(n_trials, <span class="co"># for each value of ...</span>
         <span class="dt">p_water =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> sequence_length)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(d, <span class="dt">by =</span> <span class="st">&quot;n_trials&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(p_water) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># lag is the *previous* value</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lagged_n_success =</span> <span class="kw">lag</span>(n_success, <span class="dt">k =</span> <span class="dv">1</span>), 
         <span class="dt">lagged_n_trials  =</span> <span class="kw">lag</span>(n_trials,  <span class="dt">k =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># if first time, flat prior. </span>
<span class="st">  </span><span class="co"># otherwise use previous posterior as new prior.</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior =</span> <span class="kw">ifelse</span>(n_trials <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, 
                        <span class="fl">.5</span>,
                        <span class="kw">dbinom</span>(<span class="dt">x =</span> lagged_n_success, 
                               <span class="dt">size =</span> lagged_n_trials, 
                               <span class="dt">prob =</span> p_water)),
         <span class="dt">strip =</span> <span class="kw">str_c</span>(<span class="st">&quot;n = &quot;</span>, n_trials), <span class="co"># which draw is it?</span>
         <span class="dt">likelihood =</span> <span class="kw">dbinom</span>(<span class="dt">x =</span> n_success, 
                             <span class="dt">size =</span> n_trials, 
                             <span class="dt">prob =</span> p_water)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># calculate likelihood for current draw</span>
<span class="st">  </span><span class="co"># normalize the prior and the likelihood, making them probabilities </span>
<span class="st">  </span><span class="kw">group_by</span>(n_trials) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior =</span> prior <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(prior),
         <span class="dt">likelihood =</span> likelihood <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(likelihood)) <span class="op">%&gt;%</span><span class="st">   </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> p_water)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> prior), <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> likelihood)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;proportion water&quot;</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">.5</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;plausibility&quot;</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>strip, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/globe_updating-1.png" width="672" /></p>
<p><strong>Side note on sample size:</strong> Bayesian estimates are valid and interpretable at any sample size. This fact is very much in contrast to the folk-wisdom around a minimum number of samples needed that you’ll hear in non-Bayesian contexts. In non-Bayesian contests, statistical inference is justified by behaviour at large samples sizes, called asymptotic behaviour. The reason Bayesian estimates are always valid and interpretable comes down to the prior. If the prior is bad, the resulting posterior could be misleading. Regardless, all estimates in either context are based on assumptions – the assumptions made during Bayesian analysis are just more obvious and capable of being directly interrogated.</p>
</div>
<div id="evaluate" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Evaluate</h3>
<p>Our model is learning from a “small world” of data. If there are important differences between the model and reality, then there is no guarantee of “large world” performance.</p>
<p>Certainty is no guarantee that the model is good, either. As the amount of data increases, our model will become increasingly sure of the proportion of water. This sureness happens even if the model is seriously misleading because our estimates are conditional on our model. What is happening is that, given a specific model, we can be sure that plausible values are within a narrow range.</p>
<p>It is important that you supervise and critique your model, and not just assume it worked or is correct because your code did not return an error. Anything that is not included in our model might not affect our inference directly, but might affect it indirectly because of that unmodeled dependence. For example, we currently are assuming that the order the data was collected in doesn’t matter (<em>exchangeable</em>), but what if the order of the observations actually did matter? Check the model’s inferences in light of aspects of the data that you know but the model doesn’t know about. This part of data analysis is an inherently creative endeavor that is up to you (the analyst) and your scientific community. Robots can’t do this step for you.</p>
<p>The goal when evaluating your model is not to test the truth of the model’s assumptions. Our model’s assumptions can never be exactly right as they are not the true data generating process. Failure to prove that our model false is a failure of our imagination, not a success of our model. Additionally, a model doesn’t have to be true in order to produce precise and useful inference. Models are information processing machines, and there are parts of information that cannot be easily represented by framing our problem in terms of the truth of our assumptions.</p>
<p>Instead, our objective should be to test the model’s adequacy for some <em>purpose</em>. What are we trying to learn? This means asking and answering more questions than those originally used to construct our model. Think about what you already know as a domain expert and compare this knowledge to your model’s estimates/predictions; if there is a conflict you should update your model (likelihood <em>and/or</em> prior) to better reflect your domain knowledge. It is hard to give general advice on model evaluation as there are lots of different contexts for evaluating the adequacy of a model – prediction, comprehension, measurement, and persuasion. These are inherently <em>scientific questions</em>, not statistical questions. As I said earlier, robot’s can’t do this step for you.</p>
</div>
</div>
<div id="terms-and-theory" class="section level2">
<h2><span class="header-section-number">2.5</span> Terms and theory</h2>
<ul>
<li><strong>Common notation</strong>
<ul>
<li><span class="math inline">\(y\)</span> observed data.</li>
<li><span class="math inline">\(\tilde{y}\)</span> unobserved data.</li>
<li><span class="math inline">\(X\)</span> explanatory variables, covariates, etc.</li>
<li><span class="math inline">\(\Theta\)</span> parameters of interest.</li>
<li><span class="math inline">\(p(\cdot|\cdot)\)</span> conditional probability distribution.</li>
<li><span class="math inline">\(p(\cdot)\)</span> marginal probability distribution.</li>
<li><span class="math inline">\(p(\cdot,\cdot)\)</span> joint probability distribution.</li>
<li><span class="math inline">\(Pr(\cdot)\)</span> probability of an event.</li>
</ul></li>
<li><strong>Likelihood</strong>
<ul>
<li>Specifies the plausibility of data mathematically.</li>
<li>Maps each conjecture onto the relative number of ways the data could occur, given that possibility.</li>
<li>Sometimes written <span class="math inline">\(p(y | \Theta)\)</span> or <span class="math inline">\(L(\Theta | y)\)</span>.</li>
</ul></li>
<li><strong>Parameters</strong>
<ul>
<li>Adjustable inputs.</li>
<li>1+ quantities we want to know about.</li>
<li>Represent the different conjectures for causes or explanations of the data</li>
<li>Difference between data and parameters is fuzzy and exploitable in Bayesian analysis –&gt; advanced topic</li>
</ul></li>
<li><strong>Prior</strong>
<ul>
<li>Every parameter you are trying to estimate must be provided a prior.</li>
<li>The “initial conditions” of the plausibility of each possibility.
<ul>
<li>Which parameters values do we think are more plausible than others?</li>
<li>Constrain parameters to reasonable ranges.</li>
<li>Express any knowledge we have about that parameter before any data is observed.</li>
</ul></li>
<li>An engineering assumption; helps us learn from our data.</li>
<li>Regularizing or weakly informative priors are conservative in that they tend to guard against inferring strong associations between variables –&gt; advanced topics.</li>
<li>Sometimes written <span class="math inline">\(p(\Theta)\)</span>.</li>
</ul></li>
<li><strong>Posterior</strong>
<ul>
<li>Logical consequence of likelihood, the set of parameters to estimate, and priors for each parameter.</li>
<li>The relative plausibility of different parameter values, conditional on the data.</li>
<li>Sometimes written <span class="math inline">\(p(\Theta | y)\)</span>.</li>
</ul></li>
</ul>
</div>
<div id="bayes-theorem" class="section level2">
<h2><span class="header-section-number">2.6</span> Bayes’ Theorem</h2>
<p>The logic defining the posterior distribution is called Bayes’ Theorem. The theorem itself is an intuitive result from probability theory.</p>
<p>First, describe the model and data as a joint probability. <span class="math display">\[
\begin{align}
p(y, \Theta) &amp;= p(\Theta | y) p(y) \\
p(y, \Theta) &amp;= p(y | \Theta) p(\Theta) \\
\end{align}
\]</span></p>
<p>Then set equal to each other and solve for <span class="math inline">\(p(\Theta | y)\)</span>: <span class="math display">\[
\begin{align}
p(\Theta | y) p(y) &amp;= p(y | \Theta) p(\Theta) \\
p(\Theta | y) &amp;= \frac{p(y | \Theta) p(\Theta)}{p(y)} \\
\end{align}
\]</span></p>
<p>Et voilà, Bayes’ Theorem. The probability of any particular value of <span class="math inline">\(\Theta\)</span>, given the data, is equal to the product of the likelihood and the prior, divided by <span class="math inline">\(p(y)\)</span>.</p>
<p>“But what’s <span class="math inline">\(p(y)\)</span>?” you ask. The term <span class="math inline">\(p(y)\)</span> is a confusing one – it can be called the “average likelihood,” “evidence,” or “probability of the data.” The average likelihood here means it is averaged over the prior and its job is to standardize the posterior so it integrates to 1. <span class="math inline">\(p(y)\)</span> is expressed mathematically as:</p>
<p><span class="math display">\[
p(y) = E(p(y | \Theta)) = \int p(y | \Theta) p(\Theta) d\Theta
\]</span></p>
<p><span class="math inline">\(E(\cdot)\)</span> means to take the expectation, a (weighted) average. Notice also that <span class="math inline">\(p(y)\)</span> is a type of marginal probability distribution; the process of integrating out a term (<span class="math inline">\(\Theta\)</span>) is called marginalization – we are averaging <span class="math inline">\(y\)</span> over all values of <span class="math inline">\(\Theta\)</span>. Remember also that an integral is like an average over a continuous distribution of values.</p>
</div>
<div id="but-how-does-it-work" class="section level2">
<h2><span class="header-section-number">2.7</span> But how does it <em>work</em>?</h2>
<p>Our model has three parts: likelihood, parameters, and the prior. These values get put into a “motor” that gives us a posterior distribution. The motor goes through the process of conditioning the prior on the data.</p>
<p>Turns out that knowing all the rules doesn’t necessarily help us with the calculations. For most interesting models we will ever consider, the necessary integrals in the Bayesian conditioning machinery have no closed form and can’t be calculated no matter how talented you are. Each new parameter effectively adds a new level to the integral used to calculate <span class="math inline">\(p(y)\)</span>. Instead, we need to rely on numerical techniques to approximate the mathematics defined in Bayes’ theorem.</p>
<p>All the numerical techniques we use produce only <em>samples</em> from the posterior distribution, not the distribution itself. Luckily, samples from the distribution are easier to work with than the actual distribution – this way we don’t have to do integrals.</p>
<div id="grid-approximation" class="section level3">
<h3><span class="header-section-number">2.7.1</span> Grid approximation</h3>
<p>One of the simplest conditioning techniques is grid approximation. While most parameters are continuous, we can get a decent approximation of them by considering only a finite grid of parameter values. For any particular value <span class="math inline">\(p&#39;\)</span>, compute the posterior probability by multiplying the prior probability of <span class="math inline">\(p&#39;\)</span> by the likelihood of <span class="math inline">\(p&#39;\)</span>.</p>
<p>Grid approximation is a teaching tool that forces you to really understand the nature of Bayesian updating. You will probably never use it in your actual work mostly because grid approximation scales very poorly as the number of parameters increases.</p>
<p>How to do grid approximation:</p>
<ol style="list-style-type: decimal">
<li>Define a grid (range of values to look at).</li>
<li>Calculate the value of the prior at each parameter value of the grid.</li>
<li>Compute the likelihood at each parameter value.</li>
<li>Compute the unstandardized posterior at each parameter, but multiplying the prior by the likelihood.</li>
<li>Standardize the posterior by dividing each value by the sum of all unstandardized values.</li>
</ol>
<p>The number of points you evaluate on the grid determines the precision of your estimates. More points, finer grained posterior.</p>
<p>For example, here’s the posterior probability of the percent of water on the globe given our data evaluated at 5 points and 20 points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">p_grid =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">5</span>), <span class="co"># define grid</span>
       <span class="dt">prior =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st">                  </span><span class="co"># define prior</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">likelihood =</span> <span class="kw">dbinom</span>(<span class="dv">6</span>, <span class="dt">size =</span> <span class="dv">9</span>, <span class="dt">prob =</span> p_grid), <span class="co"># compute likelihood at each value in grid</span>
         <span class="dt">unstd_posterior =</span> likelihood <span class="op">*</span><span class="st"> </span>prior, <span class="co"># computer product of likelihood and prior</span>
         <span class="dt">posterior =</span> unstd_posterior <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(unstd_posterior)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># standardize the posterior, so it sums to 1</span>
<span class="st">  </span><span class="co"># make a plot</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> p_grid, <span class="dt">y =</span> posterior)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;5 points&quot;</span>,
         <span class="dt">x =</span> <span class="st">&quot;probability of water&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;posterior probability&quot;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/prob_water_5-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tibble</span>(<span class="dt">p_grid =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">20</span>), <span class="co"># define grid</span>
       <span class="dt">prior =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st">                  </span><span class="co"># define prior</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">likelihood =</span> <span class="kw">dbinom</span>(<span class="dv">6</span>, <span class="dt">size =</span> <span class="dv">9</span>, <span class="dt">prob =</span> p_grid), <span class="co"># compute likelihood at each value in grid</span>
         <span class="dt">unstd_posterior =</span> likelihood <span class="op">*</span><span class="st"> </span>prior, <span class="co"># compute product of likelihood and prior</span>
         <span class="dt">posterior =</span> unstd_posterior <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(unstd_posterior)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># standardize the posterior, so it sums to 1</span>
<span class="st">  </span><span class="co"># make a plot</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> p_grid, <span class="dt">y =</span> posterior)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;20 points&quot;</span>,
         <span class="dt">x =</span> <span class="st">&quot;probability of water&quot;</span>,
         <span class="dt">y =</span> <span class="st">&quot;posterior probability&quot;</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/prob_water_20-1.png" width="672" /></p>
</div>
<div id="markov-chain-monte-carlo" class="section level3">
<h3><span class="header-section-number">2.7.2</span> Markov chain Monte Carlo</h3>
<p>For many models, grid approximation or quadratic approximation just aren’t good enough. Grid approximation takes too long as your model gets bigger. Quadratic approximation chokes on complex models. Instead, we end up having to use a technique like Markov chain Monte Carlo (MCMC).</p>
<p>Unlike grid approximation, where we computed the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, where the frequencies of those values correspond to the posterior plausibilities.</p>
<p>Let’s do a quick example where we fit this model using <strong>brms</strong>. This package acts as an interface with <a href="http://mc-stan.org/">Stan probabilistic programming language</a> which implements Hamiltonian Monte Carlo sampling, a fancy type of MCMC. The function <code>brm()</code> is the workhorse of the <strong>brms</strong> package, and builds and compiles a Stan model as defined in R. If you’ve used functions like <code>lm()</code> or <code>glm()</code>, the syntax should look familiar to you.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># this can take a bit as the model compiles</span>
globe_brms &lt;-
<span class="st">  </span><span class="kw">brm</span>(<span class="dt">data =</span> <span class="kw">list</span>(<span class="dt">w =</span> <span class="dv">24</span>),             <span class="co"># generate data</span>
      <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;identity&quot;</span>), <span class="co"># define likelihood distribution</span>
      <span class="dt">formula =</span> w <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(<span class="dv">36</span>) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,    <span class="co"># define parameter</span>
      <span class="dt">prior =</span> <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> Intercept), <span class="co"># give prior to parameter</span>
      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.95</span>), <span class="co"># control sampling behavior --&gt; advanced topic</span>
      <span class="dt">refresh =</span> <span class="dv">0</span>,                     <span class="co"># silences a bunch of text</span>
      <span class="dt">iter =</span> <span class="dv">2000</span>,                     <span class="co"># how many draws from posterior? (default)</span>
      <span class="dt">warmup =</span> <span class="dv">1000</span>)                   <span class="co"># how many draws till we start recording (default = 1/2 iter)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(globe_brms)</code></pre></div>
<pre><code>##  Family: binomial 
##   Links: mu = identity 
## Formula: w | trials(36) ~ 1 
##    Data: list(w = 24) (Number of observations: 1) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept     0.66      0.08     0.50     0.79       1214 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># look at the posterior distribution of proportion water</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> b_Intercept)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">stat =</span> <span class="st">&#39;density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&#39;proportion water&#39;</span>,
                     <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>)</code></pre></div>
<p><img src="paleo_book_files/figure-html/print_globe-1.png" width="672" /></p>
</div>
<div id="aside-on-interpreting-probabilities" class="section level3">
<h3><span class="header-section-number">2.7.3</span> Aside on Interpreting Probabilities</h3>
<p>From <a href="mathwithbaddrawings.com/2015/09/23/what-does-probability-mean-in-your-profession/">Math with Bad Drawings</a>.</p>
<div class="figure">
<img src="figures/prob_actual.jpg" title="Actual Meaning" />

</div>
<div class="figure">
<img src="figures/prob_starwars.jpg" title="Star Wars" />

</div>
</div>
</div>
<div id="working-with-samples" class="section level2">
<h2><span class="header-section-number">2.8</span> Working with samples</h2>
<p>In applied Bayesian analysis we rarely work directly with the integrals required by Bayes’ theorem. Most numerical techniques we use, including MCMC methods, produce a set of samples that are individual draws from the posterior distribution. These samples transform a problem in calculus to a problem in data summary. It is easier to count the number of samples within an interval then calculate the integral for that interval.</p>
<p>This section also serves as a brief introduction to summarizing posterior samples using <strong>tidybayes</strong>, which we will continue using in our next lesson.</p>
<p>Once our model produces a posterior distribution, the model’s work is done. It is now <em>our</em> job to summarize and interpret that posterior. Common questions we might want to ask include:</p>
<ul>
<li>How much posterior probability lies below some parameter value?</li>
<li>How much posterior probability lies between two parameter values?</li>
<li>Which parameter value marks the lower 5% of the posterior probability?</li>
<li>Which range of parameter values contains 90% of the posterior probability?</li>
<li>Which parameter value has the highest posterior probability?</li>
</ul>
<div id="intervals-of-defined-boundaries" class="section level3">
<h3><span class="header-section-number">2.8.1</span> Intervals of defined boundaries</h3>
<p>What is the posterior probability that the proportion of water is less than 50%? Count the number of observations that are less than 0.5, then divide by the number of samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">globe_brms <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">p_val =</span> <span class="kw">sum</span>(b_Intercept <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span>) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(b_Intercept))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##    p_val
##    &lt;dbl&gt;
## 1 0.0278</code></pre>
<p>What if we want to know the posterior probability that the proportion of water is between 0.5 and 0.75?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">globe_brms <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">p_val =</span> <span class="kw">sum</span>(b_Intercept <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">&amp;</span><span class="st"> </span>b_Intercept <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.75</span>) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(b_Intercept))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   p_val
##   &lt;dbl&gt;
## 1 0.862</code></pre>
</div>
<div id="intervals-of-defined-mass" class="section level3">
<h3><span class="header-section-number">2.8.2</span> Intervals of defined mass</h3>
<p>An interval of defined mass report two parameter values that contain between them the specified amount of posterior probability, a probability mass. You probably have heard of confidence intervals – an interval of posterior probability is called a <em>credible interval</em>, though the distinction between the terms isn’t terribly important.</p>
<p>There are two kinds of intervals of defined mass: percentile (or quantile) interval, and highest posterior density interval.</p>
<div id="percentile-interval-pi" class="section level4">
<h4><span class="header-section-number">2.8.2.1</span> Percentile interval (PI)</h4>
<p>For example, you may want to know the boundaries of the lower 80% posterior interval. This interval has to start at 0, but where does it stop? How do we calculate the 80th percentile?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">q80 =</span> <span class="kw">quantile</span>(b_Intercept, <span class="fl">0.8</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 1
##     q80
##   &lt;dbl&gt;
## 1 0.723</code></pre>
<p>What about the middle 80% interval? There are lots of ways to get this information, so here are two examples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># one way</span>
(qu &lt;-<span class="st"> </span>globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">q10 =</span> <span class="kw">quantile</span>(b_Intercept, <span class="fl">0.1</span>),
                   <span class="dt">q90 =</span> <span class="kw">quantile</span>(b_Intercept, <span class="fl">0.9</span>)))</code></pre></div>
<pre><code>## # A tibble: 1 x 2
##     q10   q90
##   &lt;dbl&gt; &lt;dbl&gt;
## 1 0.555 0.754</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># another way</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">quantile</span>(<span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>))         </code></pre></div>
<pre><code>##       10%       90% 
## 0.5549006 0.7542126</code></pre>
<p>We can also plot this interval on the distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot the distribution</span>
p1 &lt;-<span class="st"> </span>globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> b_Intercept)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">stat =</span> <span class="st">&#39;density&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))

<span class="co"># get back the density line calculation</span>
p1_df &lt;-<span class="st"> </span><span class="kw">ggplot_build</span>(p1)<span class="op">$</span>data[[<span class="dv">1</span>]]    <span class="co"># this is messy</span>

<span class="co"># shade area under the distribution</span>
p1 <span class="op">+</span><span class="st"> </span><span class="kw">geom_area</span>(<span class="dt">data =</span> <span class="kw">subset</span>(p1_df, x <span class="op">&gt;</span><span class="st"> </span>qu<span class="op">$</span>q10 <span class="op">&amp;</span><span class="st"> </span>x <span class="op">&lt;</span><span class="st"> </span>qu<span class="op">$</span>q90),
               <span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>y),
               <span class="dt">fill =</span> <span class="st">&quot;black&quot;</span>, 
               <span class="dt">color =</span> <span class="ot">NA</span>) </code></pre></div>
<p><img src="paleo_book_files/figure-html/shade_interval-1.png" width="672" /></p>
</div>
<div id="highest-posterior-density-interval" class="section level4">
<h4><span class="header-section-number">2.8.2.2</span> Highest posterior density interval</h4>
<p>An HPDI is defined as the narrowest interval containing the specified probability mass. There are an infinite of posterior intervals with the same mass, but what if you want that interval that best represents the parameter values most consistent with the data AND you want the densest of these intervals.</p>
<p>Here an example of a 50% HPDI</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">hdi</span>(<span class="dt">.width =</span> <span class="fl">0.5</span>)</code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,] 0.6137909 0.7158464</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compare to 50% PI</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">pull</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">qi</span>(<span class="dt">.width =</span> <span class="fl">0.5</span>)</code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,] 0.6082956 0.7113937</code></pre>
<p>Why are the PI and HPDI not equal?</p>
<p>If the choice of interval makes a big difference in your summary, then you probably shouldn’t be using intervals to summarize the posterior! The entire distribution of samples is our actual estimate, these summaries are just there to help digest this wealth of information. Do not only work with simplified versions of our posterior estimates!</p>
</div>
</div>
<div id="point-estimates" class="section level3">
<h3><span class="header-section-number">2.8.3</span> Point estimates</h3>
<p>Sometimes we only want a single point from the distribution. Given an entire posterior distribution, what value should we report? Mechanically, this task is simple – pick a summary (e.g. mean) and go. Conceptually, however, this task is actually quite complex. The Bayesian parameter estimate is the entire posterior distribution, and not just a single number. In most cases, it is unnecessary to choose a single point estimate. It is always better to report <em>more</em> than necessary about the posterior distribution than not enough.</p>
<p>The three most common point estimates are the mean, median, and mode – you’re probably already familiar with all three of them. A principled way of choosing among these three estimates is considering as products of different <em>loss functions</em>. Loss functions are an advanced topic we will not cover today but I encourage you to read up on; here’s a good <a href="http://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/">blog entry by John Myles White</a> on the subject.</p>
<p>The mode represents the parameter value with the highest posterior probability, or the <em>maximum a posteriori</em> estimate (MAP). In frequentist contexts, the maximum likelihood estimate is the equivalent to mode of the likelihood function. Calculating the mode of a distribution is an optimization problem and isn’t always easy. Luckily, <strong>tidybayes</strong> has a function which gets us a mode (and other information) from the posterior samples.</p>
<p>Here is a code snippet that gives a mode and a middle 95% percentile interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mode_qi</span>()</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   b_Intercept .lower .upper .width .point .interval
##         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1       0.662  0.497  0.794   0.95 mode   qi</code></pre>
<p>We could also report a mean or median. Here are some ways to do get these estimates along with some kind of interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mean with 95% PI</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mean_qi</span>()</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   b_Intercept .lower .upper .width .point .interval
##         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1       0.658  0.497  0.794   0.95 mean   qi</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mean with 95% HPDI</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mean_hdci</span>()</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   b_Intercept .lower .upper .width .point .interval
##         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1       0.658  0.499  0.795   0.95 mean   hdci</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># median with 95% PI</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">median_qi</span>()</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   b_Intercept .lower .upper .width .point .interval
##         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1       0.661  0.497  0.794   0.95 median qi</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># median with 95% HPDI</span>
globe_brms <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spread_draws</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(b_Intercept) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">median_hdci</span>()</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   b_Intercept .lower .upper .width .point .interval
##         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1       0.661  0.499  0.795   0.95 median hdci</code></pre>
<p>Usually it is better to communicate as much about the posterior distribution as you can.</p>
</div>
</div>
<div id="summary-1" class="section level2">
<h2><span class="header-section-number">2.9</span> Summary</h2>
<p>So far we’ve introduced the conceptual mechanism driving Bayesian data analysis. This framework emphasizes the posterior probability distribution, which is the logical compromise between our previous information and whatever new information we’ve gained (given our model). Posterior probabilities state the relative plausibility of each conjectured possibility that could have produced the data, given our model. These plausibilities are updated in light of observations, a process known as Bayesian updating.</p>
<p>We’ve defined the components of a Bayesian model: a likelihood, one or more parameters, and a prior for every parameter. The likelihood defines the plausibility of the data, given a fixed value for the parameters. The prior provides the initial plausibility of each parameter value, before accounting for the data. These components, when processed through Bayes’ Theorem, yield the posterior distribution.</p>
<p>Many of the actual calculations necessary to yield the posterior distribution have no closed-form solution and must instead be approximated using numerical methods. We covered grid approximation as a gentle introduction to sampling. We also fit our basic model using <strong>brms</strong>, which uses Stan’s HMC engine to condition our estimates on the data.</p>
<p>Given the posterior samples from our model fit, we also covered basic methods for summarizing a posterior distribution such as intervals and point estimates.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="managing-and-processing-data-from-the-paleobiology-database.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reg-intro.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": [["analytical_paleobiology.pdf", "PDF"], ["analytical_paleobiology.epub", "EPUB"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
